{
  "papers": {
    "2505.00586v2": {
      "arxiv_id": "2505.00586v2",
      "title": "ParkDiffusion: Heterogeneous Multi-Agent Multi-Modal Trajectory Prediction for Automated Parking using Diffusion Models",
      "authors": [
        "Jiarong Wei",
        "Niclas Vödisch",
        "Anna Rehr",
        "Christian Feist",
        "Abhinav Valada"
      ],
      "summary": "Automated parking is a critical feature of Advanced Driver Assistance Systems (ADAS), where accurate trajectory prediction is essential to bridge perception and planning modules. Despite its significance, research in this domain remains relatively limited, with most existing studies concentrating on single-modal trajectory prediction of vehicles. In this work, we propose ParkDiffusion, a novel approach that predicts the trajectories of both vehicles and pedestrians in automated parking scenarios. ParkDiffusion employs diffusion models to capture the inherent uncertainty and multi-modality of future trajectories, incorporating several key innovations. First, we propose a dual map encoder that processes soft semantic cues and hard geometric constraints using a two-step cross-attention mechanism. Second, we introduce an adaptive agent type embedding module, which dynamically conditions the prediction process on the distinct characteristics of vehicles and pedestrians. Third, to ensure kinematic feasibility, our model outputs control signals that are subsequently used within a kinematic framework to generate physically feasible trajectories. We evaluate ParkDiffusion on the Dragon Lake Parking (DLP) dataset and the Intersections Drone (inD) dataset. Our work establishes a new baseline for heterogeneous trajectory prediction in parking scenarios, outperforming existing methods by a considerable margin.",
      "published_utc": "2025-05-01T15:16:59Z",
      "updated_utc": "2025-08-13T15:18:31Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.00586v2",
      "abs_url": "http://arxiv.org/abs/2505.00586v2"
    },
    "2504.17371v3": {
      "arxiv_id": "2504.17371v3",
      "title": "Highly Accurate and Diverse Traffic Data: The DeepScenario Open 3D Dataset",
      "authors": [
        "Oussema Dhaouadi",
        "Johannes Meier",
        "Luca Wahl",
        "Jacques Kaiser",
        "Luca Scalerandi",
        "Nick Wandelburg",
        "Zhuolun Zhou",
        "Nijanthan Berinpanathan",
        "Holger Banzhaf",
        "Daniel Cremers"
      ],
      "summary": "Accurate 3D trajectory data is crucial for advancing autonomous driving. Yet, traditional datasets are usually captured by fixed sensors mounted on a car and are susceptible to occlusion. Additionally, such an approach can precisely reconstruct the dynamic environment in the close vicinity of the measurement vehicle only, while neglecting objects that are further away. In this paper, we introduce the DeepScenario Open 3D Dataset (DSC3D), a high-quality, occlusion-free dataset of 6 degrees of freedom bounding box trajectories acquired through a novel monocular camera drone tracking pipeline. Our dataset includes more than 175,000 trajectories of 14 types of traffic participants and significantly exceeds existing datasets in terms of diversity and scale, containing many unprecedented scenarios such as complex vehicle-pedestrian interaction on highly populated urban streets and comprehensive parking maneuvers from entry to exit. DSC3D dataset was captured in five various locations in Europe and the United States and include: a parking lot, a crowded inner-city, a steep urban intersection, a federal highway, and a suburban intersection. Our 3D trajectory dataset aims to enhance autonomous driving systems by providing detailed environmental 3D representations, which could lead to improved obstacle interactions and safety. We demonstrate its utility across multiple applications including motion prediction, motion planning, scenario mining, and generative reactive traffic agents. Our interactive online visualization platform and the complete dataset are publicly available at https://app.deepscenario.com, facilitating research in motion prediction, behavior modeling, and safety validation.",
      "published_utc": "2025-04-24T08:43:48Z",
      "updated_utc": "2025-08-22T13:21:59Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.17371v3",
      "abs_url": "http://arxiv.org/abs/2504.17371v3"
    },
    "2405.16439v3": {
      "arxiv_id": "2405.16439v3",
      "title": "Multi-Agent Inverse Reinforcement Learning in Real World Unstructured Pedestrian Crowds",
      "authors": [
        "Rohan Chandra",
        "Haresh Karnan",
        "Negar Mehr",
        "Peter Stone",
        "Joydeep Biswas"
      ],
      "summary": "Social robot navigation in crowded public spaces such as university campuses, restaurants, grocery stores, and hospitals, is an increasingly important area of research. One of the core strategies for achieving this goal is to understand humans' intent--underlying psychological factors that govern their motion--by learning their reward functions, typically via inverse reinforcement learning (IRL). Despite significant progress in IRL, learning reward functions of multiple agents simultaneously in dense unstructured pedestrian crowds has remained intractable due to the nature of the tightly coupled social interactions that occur in these scenarios \\textit{e.g.} passing, intersections, swerving, weaving, etc. In this paper, we present a new multi-agent maximum entropy inverse reinforcement learning algorithm for real world unstructured pedestrian crowds. Key to our approach is a simple, but effective, mathematical trick which we name the so-called tractability-rationality trade-off trick that achieves tractability at the cost of a slight reduction in accuracy. We compare our approach to the classical single-agent MaxEnt IRL as well as state-of-the-art trajectory prediction methods on several datasets including the ETH, UCY, SCAND, JRDB, and a new dataset, called Speedway, collected at a busy intersection on a University campus focusing on dense, complex agent interactions. Our key findings show that, on the dense Speedway dataset, our approach ranks 1st among top 7 baselines with >2X improvement over single-agent IRL, and is competitive with state-of-the-art large transformer-based encoder-decoder models on sparser datasets such as ETH/UCY (ranks 3rd among top 7 baselines).",
      "published_utc": "2024-05-26T05:48:21Z",
      "updated_utc": "2025-03-26T21:19:58Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.16439v3",
      "abs_url": "http://arxiv.org/abs/2405.16439v3"
    },
    "2403.00353v1": {
      "arxiv_id": "2403.00353v1",
      "title": "MS-Net: A Multi-Path Sparse Model for Motion Prediction in Multi-Scenes",
      "authors": [
        "Xiaqiang Tang",
        "Weigao Sun",
        "Siyuan Hu",
        "Yiyang Sun",
        "Yafeng Guo"
      ],
      "summary": "The multi-modality and stochastic characteristics of human behavior make motion prediction a highly challenging task, which is critical for autonomous driving. While deep learning approaches have demonstrated their great potential in this area, it still remains unsolved to establish a connection between multiple driving scenes (e.g., merging, roundabout, intersection) and the design of deep learning models. Current learning-based methods typically use one unified model to predict trajectories in different scenarios, which may result in sub-optimal results for one individual scene. To address this issue, we propose Multi-Scenes Network (aka. MS-Net), which is a multi-path sparse model trained by an evolutionary process. MS-Net selectively activates a subset of its parameters during the inference stage to produce prediction results for each scene. In the training stage, the motion prediction task under differentiated scenes is abstracted as a multi-task learning problem, an evolutionary algorithm is designed to encourage the network search of the optimal parameters for each scene while sharing common knowledge between different scenes. Our experiment results show that with substantially reduced parameters, MS-Net outperforms existing state-of-the-art methods on well-established pedestrian motion prediction datasets, e.g., ETH and UCY, and ranks the 2nd place on the INTERACTION challenge.",
      "published_utc": "2024-03-01T08:32:12Z",
      "updated_utc": "2024-03-01T08:32:12Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.00353v1",
      "abs_url": "http://arxiv.org/abs/2403.00353v1"
    },
    "2306.01075v1": {
      "arxiv_id": "2306.01075v1",
      "title": "Pedestrian Crossing Action Recognition and Trajectory Prediction with 3D Human Keypoints",
      "authors": [
        "Jiachen Li",
        "Xinwei Shi",
        "Feiyu Chen",
        "Jonathan Stroud",
        "Zhishuai Zhang",
        "Tian Lan",
        "Junhua Mao",
        "Jeonhyung Kang",
        "Khaled S. Refaat",
        "Weilong Yang",
        "Eugene Ie",
        "Congcong Li"
      ],
      "summary": "Accurate understanding and prediction of human behaviors are critical prerequisites for autonomous vehicles, especially in highly dynamic and interactive scenarios such as intersections in dense urban areas. In this work, we aim at identifying crossing pedestrians and predicting their future trajectories. To achieve these goals, we not only need the context information of road geometry and other traffic participants but also need fine-grained information of the human pose, motion and activity, which can be inferred from human keypoints. In this paper, we propose a novel multi-task learning framework for pedestrian crossing action recognition and trajectory prediction, which utilizes 3D human keypoints extracted from raw sensor data to capture rich information on human pose and activity. Moreover, we propose to apply two auxiliary tasks and contrastive learning to enable auxiliary supervisions to improve the learned keypoints representation, which further enhances the performance of major tasks. We validate our approach on a large-scale in-house dataset, as well as a public benchmark dataset, and show that our approach achieves state-of-the-art performance on a wide range of evaluation metrics. The effectiveness of each model component is validated in a detailed ablation study.",
      "published_utc": "2023-06-01T18:27:48Z",
      "updated_utc": "2023-06-01T18:27:48Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.01075v1",
      "abs_url": "http://arxiv.org/abs/2306.01075v1"
    },
    "2210.10254v2": {
      "arxiv_id": "2210.10254v2",
      "title": "Safe Planning in Dynamic Environments using Conformal Prediction",
      "authors": [
        "Lars Lindemann",
        "Matthew Cleaveland",
        "Gihyun Shim",
        "George J. Pappas"
      ],
      "summary": "We propose a framework for planning in unknown dynamic environments with probabilistic safety guarantees using conformal prediction. Particularly, we design a model predictive controller (MPC) that uses i) trajectory predictions of the dynamic environment, and ii) prediction regions quantifying the uncertainty of the predictions. To obtain prediction regions, we use conformal prediction, a statistical tool for uncertainty quantification, that requires availability of offline trajectory data - a reasonable assumption in many applications such as autonomous driving. The prediction regions are valid, i.e., they hold with a user-defined probability, so that the MPC is provably safe. We illustrate the results in the self-driving car simulator CARLA at a pedestrian-filled intersection. The strength of our approach is compatibility with state of the art trajectory predictors, e.g., RNNs and LSTMs, while making no assumptions on the underlying trajectory-generating distribution. To the best of our knowledge, these are the first results that provide valid safety guarantees in such a setting.",
      "published_utc": "2022-10-19T02:25:33Z",
      "updated_utc": "2023-06-08T04:46:57Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.10254v2",
      "abs_url": "http://arxiv.org/abs/2210.10254v2"
    },
    "2209.02297v1": {
      "arxiv_id": "2209.02297v1",
      "title": "SIND: A Drone Dataset at Signalized Intersection in China",
      "authors": [
        "Yanchao Xu",
        "Wenbo Shao",
        "Jun Li",
        "Kai Yang",
        "Weida Wang",
        "Hua Huang",
        "Chen Lv",
        "Hong Wang"
      ],
      "summary": "Intersection is one of the most challenging scenarios for autonomous driving tasks. Due to the complexity and stochasticity, essential applications (e.g., behavior modeling, motion prediction, safety validation, etc.) at intersections rely heavily on data-driven techniques. Thus, there is an intense demand for trajectory datasets of traffic participants (TPs) in intersections. Currently, most intersections in urban areas are equipped with traffic lights. However, there is not yet a large-scale, high-quality, publicly available trajectory dataset for signalized intersections. Therefore, in this paper, a typical two-phase signalized intersection is selected in Tianjin, China. Besides, a pipeline is designed to construct a Signalized INtersection Dataset (SIND), which contains 7 hours of recording including over 13,000 TPs with 7 types. Then, the behaviors of traffic light violations in SIND are recorded. Furthermore, the SIND is also compared with other similar works. The features of the SIND can be summarized as follows: 1) SIND provides more comprehensive information, including traffic light states, motion parameters, High Definition (HD) map, etc. 2) The category of TPs is diverse and characteristic, where the proportion of vulnerable road users (VRUs) is up to 62.6% 3) Multiple traffic light violations of non-motor vehicles are shown. We believe that SIND would be an effective supplement to existing datasets and can promote related research on autonomous driving.The dataset is available online via: https://github.com/SOTIF-AVLab/SinD",
      "published_utc": "2022-09-06T08:49:44Z",
      "updated_utc": "2022-09-06T08:49:44Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.GL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.02297v1",
      "abs_url": "http://arxiv.org/abs/2209.02297v1"
    },
    "2106.00559v2": {
      "arxiv_id": "2106.00559v2",
      "title": "Predicting Vehicles Trajectories in Urban Scenarios with Transformer Networks and Augmented Information",
      "authors": [
        "A. Quintanar",
        "D. Fernández-Llorca",
        "I. Parra",
        "R. Izquierdo",
        "M. A. Sotelo"
      ],
      "summary": "Understanding the behavior of road users is of vital importance for the development of trajectory prediction systems. In this context, the latest advances have focused on recurrent structures, establishing the social interaction between the agents involved in the scene. More recently, simpler structures have also been introduced for predicting pedestrian trajectories, based on Transformer Networks, and using positional information. They allow the individual modelling of each agent's trajectory separately without any complex interaction terms. Our model exploits these simple structures by adding augmented data (position and heading), and adapting their use to the problem of vehicle trajectory prediction in urban scenarios in prediction horizons up to 5 seconds. In addition, a cross-performance analysis is performed between different types of scenarios, including highways, intersections and roundabouts, using recent datasets (inD, rounD, highD and INTERACTION). Our model achieves state-of-the-art results and proves to be flexible and adaptable to different types of urban contexts.",
      "published_utc": "2021-06-01T15:18:55Z",
      "updated_utc": "2021-06-07T23:38:30Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2106.00559v2",
      "abs_url": "http://arxiv.org/abs/2106.00559v2"
    },
    "2008.08294v2": {
      "arxiv_id": "2008.08294v2",
      "title": "TNT: Target-driveN Trajectory Prediction",
      "authors": [
        "Hang Zhao",
        "Jiyang Gao",
        "Tian Lan",
        "Chen Sun",
        "Benjamin Sapp",
        "Balakrishnan Varadarajan",
        "Yue Shen",
        "Yi Shen",
        "Yuning Chai",
        "Cordelia Schmid",
        "Congcong Li",
        "Dragomir Anguelov"
      ],
      "summary": "Predicting the future behavior of moving agents is essential for real world applications. It is challenging as the intent of the agent and the corresponding behavior is unknown and intrinsically multimodal. Our key insight is that for prediction within a moderate time horizon, the future modes can be effectively captured by a set of target states. This leads to our target-driven trajectory prediction (TNT) framework. TNT has three stages which are trained end-to-end. It first predicts an agent's potential target states $T$ steps into the future, by encoding its interactions with the environment and the other agents. TNT then generates trajectory state sequences conditioned on targets. A final stage estimates trajectory likelihoods and a final compact set of trajectory predictions is selected. This is in contrast to previous work which models agent intents as latent variables, and relies on test-time sampling to generate diverse trajectories. We benchmark TNT on trajectory prediction of vehicles and pedestrians, where we outperform state-of-the-art on Argoverse Forecasting, INTERACTION, Stanford Drone and an in-house Pedestrian-at-Intersection dataset.",
      "published_utc": "2020-08-19T06:52:46Z",
      "updated_utc": "2020-08-21T07:33:10Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2008.08294v2",
      "abs_url": "http://arxiv.org/abs/2008.08294v2"
    },
    "2003.09996v1": {
      "arxiv_id": "2003.09996v1",
      "title": "Analysis and Prediction of Pedestrian Crosswalk Behavior during Automated Vehicle Interactions",
      "authors": [
        "Suresh Kumaar Jayaraman",
        "Dawn M. Tilbury",
        "X. Jessie Yang",
        "Anuj K. Pradhan",
        "Lionel P. Robert"
      ],
      "summary": "For safe navigation around pedestrians, automated vehicles (AVs) need to plan their motion by accurately predicting pedestrians trajectories over long time horizons. Current approaches to AV motion planning around crosswalks predict only for short time horizons (1-2 s) and are based on data from pedestrian interactions with human-driven vehicles (HDVs). In this paper, we develop a hybrid systems model that uses pedestrians gap acceptance behavior and constant velocity dynamics for long-term pedestrian trajectory prediction when interacting with AVs. Results demonstrate the applicability of the model for long-term (> 5 s) pedestrian trajectory prediction at crosswalks. Further we compared measures of pedestrian crossing behaviors in the immersive virtual environment (when interacting with AVs) to that in the real world (results of published studies of pedestrians interacting with HDVs), and found similarities between the two. These similarities demonstrate the applicability of the hybrid model of AV interactions developed from an immersive virtual environment (IVE) for real-world scenarios for both AVs and HDVs.",
      "published_utc": "2020-03-22T21:28:39Z",
      "updated_utc": "2020-03-22T21:28:39Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CY",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2003.09996v1",
      "abs_url": "http://arxiv.org/abs/2003.09996v1"
    },
    "1911.09476v1": {
      "arxiv_id": "1911.09476v1",
      "title": "Incremental Learning of Motion Primitives for Pedestrian Trajectory Prediction at Intersections",
      "authors": [
        "Golnaz Habibi",
        "Nikita Japuria",
        "Jonathan P. How"
      ],
      "summary": "This paper presents a novel incremental learning algorithm for pedestrian motion prediction, with the ability to improve the learned model over time when data is incrementally available. In this setup, trajectories are modeled as simple segments called motion primitives. Transitions between motion primitives are modeled as Gaussian Processes. When new data is available, the motion primitives learned from the new data are compared with the previous ones by measuring the inner product of the motion primitive vectors. Similar motion primitives and transitions are fused and novel motion primitives are added to capture newly observed behaviors. The proposed approach is tested and compared with other baselines in intersection scenarios where the data is incrementally available either from a single intersection or from multiple intersections with different geometries. In both cases, our method incrementally learns motion patterns and outperforms the offline learning approach in terms of prediction errors. The results also show that the model size in our algorithm grows at a much lower rate than standard incremental learning, where newly learned motion primitives and transitions are simply accumulated over time.",
      "published_utc": "2019-11-21T14:06:18Z",
      "updated_utc": "2019-11-21T14:06:18Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/1911.09476v1",
      "abs_url": "http://arxiv.org/abs/1911.09476v1"
    },
    "1910.03088v1": {
      "arxiv_id": "1910.03088v1",
      "title": "INTERACTION Dataset: An INTERnational, Adversarial and Cooperative moTION Dataset in Interactive Driving Scenarios with Semantic Maps",
      "authors": [
        "Wei Zhan",
        "Liting Sun",
        "Di Wang",
        "Haojie Shi",
        "Aubrey Clausse",
        "Maximilian Naumann",
        "Julius Kummerle",
        "Hendrik Konigshof",
        "Christoph Stiller",
        "Arnaud de La Fortelle",
        "Masayoshi Tomizuka"
      ],
      "summary": "Behavior-related research areas such as motion prediction/planning, representation/imitation learning, behavior modeling/generation, and algorithm testing, require support from high-quality motion datasets containing interactive driving scenarios with different driving cultures. In this paper, we present an INTERnational, Adversarial and Cooperative moTION dataset (INTERACTION dataset) in interactive driving scenarios with semantic maps. Five features of the dataset are highlighted. 1) The interactive driving scenarios are diverse, including urban/highway/ramp merging and lane changes, roundabouts with yield/stop signs, signalized intersections, intersections with one/two/all-way stops, etc. 2) Motion data from different countries and different continents are collected so that driving preferences and styles in different cultures are naturally included. 3) The driving behavior is highly interactive and complex with adversarial and cooperative motions of various traffic participants. Highly complex behavior such as negotiations, aggressive/irrational decisions and traffic rule violations are densely contained in the dataset, while regular behavior can also be found from cautious car-following, stop, left/right/U-turn to rational lane-change and cycling and pedestrian crossing, etc. 4) The levels of criticality span wide, from regular safe operations to dangerous, near-collision maneuvers. Real collision, although relatively slight, is also included. 5) Maps with complete semantic information are provided with physical layers, reference lines, lanelet connections and traffic rules. The data is recorded from drones and traffic cameras. Statistics of the dataset in terms of number of entities and interaction density are also provided, along with some utilization examples in a variety of behavior-related research areas. The dataset can be downloaded via https://interaction-dataset.com.",
      "published_utc": "2019-09-30T17:26:51Z",
      "updated_utc": "2019-09-30T17:26:51Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.LG",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/1910.03088v1",
      "abs_url": "http://arxiv.org/abs/1910.03088v1"
    },
    "1906.00486v4": {
      "arxiv_id": "1906.00486v4",
      "title": "Impact of Traffic Lights on Trajectory Forecasting of Human-driven Vehicles Near Signalized Intersections",
      "authors": [
        "Geunseob Oh",
        "Huei Peng"
      ],
      "summary": "Forecasting trajectories of human-driven vehicles is a crucial problem in autonomous driving. Trajectory forecasting in the urban area is particularly hard due to complex interactions with cars and pedestrians, and traffic lights (TLs). Unlike the former that has been widely studied, the impact of TLs on the trajectory prediction has been rarely discussed. In this work, we first identify the less studied, perhaps overlooked impact of TLs. Second, we present a novel resolution that is mindful of the impact, inspired by the fact that human drives differently depending on signal phase (green, yellow, red) and timing (elapsed time). Central to the proposed approach is Human Policy Models which model how drivers react to various states of TLs by mapping a sequence of states of vehicles and TLs to a subsequent action (acceleration) of the vehicle. We then combine the Human Policy Models with a known transition function (system dynamics) to conduct a sequential prediction; thus our approach is viewed as Behavior Cloning. One novelty of our approach is the use of vehicle-to-infrastructure communications to obtain the future states of TLs. We demonstrate the impact of TL and the proposed approach using an ablation study for longitudinal trajectory forecasting tasks on real-world driving data recorded near a signalized intersection. Finally, we propose probabilistic (generative) Human Policy Models which provide probabilistic contexts and capture competing policies, e.g., pass or stop in the yellow-light dilemma zone.",
      "published_utc": "2019-06-02T21:06:25Z",
      "updated_utc": "2020-04-26T23:39:54Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/1906.00486v4",
      "abs_url": "http://arxiv.org/abs/1906.00486v4"
    },
    "1806.09453v1": {
      "arxiv_id": "1806.09453v1",
      "title": "Context-Aware Pedestrian Motion Prediction In Urban Intersections",
      "authors": [
        "Golnaz Habibi",
        "Nikita Jaipuria",
        "Jonathan P. How"
      ],
      "summary": "This paper presents a novel context-based approach for pedestrian motion prediction in crowded, urban intersections, with the additional flexibility of prediction in similar, but new, environments. Previously, Chen et. al. combined Markovian-based and clustering-based approaches to learn motion primitives in a grid-based world and subsequently predict pedestrian trajectories by modeling the transition between learned primitives as a Gaussian Process (GP). This work extends that prior approach by incorporating semantic features from the environment (relative distance to curbside and status of pedestrian traffic lights) in the GP formulation for more accurate predictions of pedestrian trajectories over the same timescale. We evaluate the new approach on real-world data collected using one of the vehicles in the MIT Mobility On Demand fleet. The results show 12.5% improvement in prediction accuracy and a 2.65 times reduction in Area Under the Curve (AUC), which is used as a metric to quantify the span of predicted set of trajectories, such that a lower AUC corresponds to a higher level of confidence in the future direction of pedestrian motion.",
      "published_utc": "2018-06-25T13:45:57Z",
      "updated_utc": "2018-06-25T13:45:57Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.RO",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/1806.09453v1",
      "abs_url": "http://arxiv.org/abs/1806.09453v1"
    },
    "1806.09444v1": {
      "arxiv_id": "1806.09444v1",
      "title": "A Transferable Pedestrian Motion Prediction Model for Intersections with Different Geometries",
      "authors": [
        "Nikita Jaipuria",
        "Golnaz Habibi",
        "Jonathan P. How"
      ],
      "summary": "This paper presents a novel framework for accurate pedestrian intent prediction at intersections. Given some prior knowledge of the curbside geometry, the presented framework can accurately predict pedestrian trajectories, even in new intersections that it has not been trained on. This is achieved by making use of the contravariant components of trajectories in the curbside coordinate system, which ensures that the transformation of trajectories across intersections is affine, regardless of the curbside geometry. Our method is based on the Augmented Semi Nonnegative Sparse Coding (ASNSC) formulation and we use that as a baseline to show improvement in prediction performance on real pedestrian datasets collected at two intersections in Cambridge, with distinctly different curbside and crosswalk geometries. We demonstrate a 7.2% improvement in prediction accuracy in the case of same train and test intersections. Furthermore, we show a comparable prediction performance of TASNSC when trained and tested in different intersections with the baseline, trained and tested on the same intersection.",
      "published_utc": "2018-06-25T13:19:45Z",
      "updated_utc": "2018-06-25T13:19:45Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/1806.09444v1",
      "abs_url": "http://arxiv.org/abs/1806.09444v1"
    },
    "1804.00495v2": {
      "arxiv_id": "1804.00495v2",
      "title": "Transferable Pedestrian Motion Prediction Models at Intersections",
      "authors": [
        "Macheng Shen",
        "Golnaz Habibi",
        "Jonathan P. How"
      ],
      "summary": "One desirable capability of autonomous cars is to accurately predict the pedestrian motion near intersections for safe and efficient trajectory planning. We are interested in developing transfer learning algorithms that can be trained on the pedestrian trajectories collected at one intersection and yet still provide accurate predictions of the trajectories at another, previously unseen intersection. We first discussed the feature selection for transferable pedestrian motion models in general. Following this discussion, we developed one transferable pedestrian motion prediction algorithm based on Inverse Reinforcement Learning (IRL) that infers pedestrian intentions and predicts future trajectories based on observed trajectory. We evaluated our algorithm on a dataset collected at two intersections, trained at one intersection and tested at the other intersection. We used the accuracy of augmented semi-nonnegative sparse coding (ASNSC), trained and tested at the same intersection as a baseline. The result shows that the proposed algorithm improves the baseline accuracy by 40% in the non-transfer task, and 16% in the transfer task.",
      "published_utc": "2018-03-15T23:58:19Z",
      "updated_utc": "2019-09-18T23:51:54Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/1804.00495v2",
      "abs_url": "http://arxiv.org/abs/1804.00495v2"
    },
    "1803.03577v1": {
      "arxiv_id": "1803.03577v1",
      "title": "Intentions of Vulnerable Road Users - Detection and Forecasting by Means of Machine Learning",
      "authors": [
        "Michael Goldhammer",
        "Sebastian Köhler",
        "Stefan Zernetsch",
        "Konrad Doll",
        "Bernhard Sick",
        "Klaus Dietmayer"
      ],
      "summary": "Avoiding collisions with vulnerable road users (VRUs) using sensor-based early recognition of critical situations is one of the manifold opportunities provided by the current development in the field of intelligent vehicles. As especially pedestrians and cyclists are very agile and have a variety of movement options, modeling their behavior in traffic scenes is a challenging task. In this article we propose movement models based on machine learning methods, in particular artificial neural networks, in order to classify the current motion state and to predict the future trajectory of VRUs. Both model types are also combined to enable the application of specifically trained motion predictors based on a continuously updated pseudo probabilistic state classification. Furthermore, the architecture is used to evaluate motion-specific physical models for starting and stopping and video-based pedestrian motion classification. A comprehensive dataset consisting of 1068 pedestrian and 494 cyclist scenes acquired at an urban intersection is used for optimization, training, and evaluation of the different models. The results show substantial higher classification rates and the ability to earlier recognize motion state changes with the machine learning approaches compared to interacting multiple model (IMM) Kalman Filtering. The trajectory prediction quality is also improved for all kinds of test scenes, especially when starting and stopping motions are included. Here, 37\\% and 41\\% lower position errors were achieved on average, respectively.",
      "published_utc": "2018-03-09T15:49:07Z",
      "updated_utc": "2018-03-09T15:49:07Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/1803.03577v1",
      "abs_url": "http://arxiv.org/abs/1803.03577v1"
    },
    "2512.02777v1": {
      "arxiv_id": "2512.02777v1",
      "title": "CogDrive: Cognition-Driven Multimodal Prediction-Planning Fusion for Safe Autonomy",
      "authors": [
        "Heye Huang",
        "Yibin Yang",
        "Mingfeng Fan",
        "Haoran Wang",
        "Xiaocong Zhao",
        "Jianqiang Wang"
      ],
      "summary": "Safe autonomous driving in mixed traffic requires a unified understanding of multimodal interactions and dynamic planning under uncertainty. Existing learning based approaches struggle to capture rare but safety critical behaviors, while rule based systems often lack adaptability in complex interactions. To address these limitations, CogDrive introduces a cognition driven multimodal prediction and planning framework that integrates explicit modal reasoning with safety aware trajectory optimization. The prediction module adopts cognitive representations of interaction modes based on topological motion semantics and nearest neighbor relational encoding. With a differentiable modal loss and multimodal Gaussian decoding, CogDrive learns sparse and unbalanced interaction behaviors and improves long horizon trajectory prediction. The planning module incorporates an emergency response concept and optimizes safety stabilized trajectories, where short term consistent branches ensure safety during replanning cycles and long term branches support smooth and collision free motion under low probability switching modes. Experiments on Argoverse2 and INTERACTION datasets show that CogDrive achieves strong performance in trajectory accuracy and miss rate, while closed loop simulations confirm adaptive behavior in merge and intersection scenarios. By combining cognitive multimodal prediction with safety oriented planning, CogDrive offers an interpretable and reliable paradigm for safe autonomy in complex traffic.",
      "published_utc": "2025-12-02T13:53:18Z",
      "updated_utc": "2025-12-02T13:53:18Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.MA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2512.02777v1",
      "abs_url": "http://arxiv.org/abs/2512.02777v1"
    },
    "2506.12474v1": {
      "arxiv_id": "2506.12474v1",
      "title": "Generalizable Trajectory Prediction via Inverse Reinforcement Learning with Mamba-Graph Architecture",
      "authors": [
        "Wenyun Li",
        "Wenjie Huang",
        "Zejian Deng",
        "Chen Sun"
      ],
      "summary": "Accurate driving behavior modeling is fundamental to safe and efficient trajectory prediction, yet remains challenging in complex traffic scenarios. This paper presents a novel Inverse Reinforcement Learning (IRL) framework that captures human-like decision-making by inferring diverse reward functions, enabling robust cross-scenario adaptability. The learned reward function is utilized to maximize the likelihood of output by the encoder-decoder architecture that combines Mamba blocks for efficient long-sequence dependency modeling with graph attention networks to encode spatial interactions among traffic agents. Comprehensive evaluations on urban intersections and roundabouts demonstrate that the proposed method not only outperforms various popular approaches in prediction accuracy but also achieves 2 times higher generalization performance to unseen scenarios compared to other IRL-based method.",
      "published_utc": "2025-06-14T12:18:19Z",
      "updated_utc": "2025-06-14T12:18:19Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.12474v1",
      "abs_url": "http://arxiv.org/abs/2506.12474v1"
    },
    "2504.15541v1": {
      "arxiv_id": "2504.15541v1",
      "title": "RiskNet: Interaction-Aware Risk Forecasting for Autonomous Driving in Long-Tail Scenarios",
      "authors": [
        "Qichao Liu",
        "Heye Huang",
        "Shiyue Zhao",
        "Lei Shi",
        "Soyoung Ahn",
        "Xiaopeng Li"
      ],
      "summary": "Ensuring the safety of autonomous vehicles (AVs) in long-tail scenarios remains a critical challenge, particularly under high uncertainty and complex multi-agent interactions. To address this, we propose RiskNet, an interaction-aware risk forecasting framework, which integrates deterministic risk modeling with probabilistic behavior prediction for comprehensive risk assessment. At its core, RiskNet employs a field-theoretic model that captures interactions among ego vehicle, surrounding agents, and infrastructure via interaction fields and force. This model supports multidimensional risk evaluation across diverse scenarios (highways, intersections, and roundabouts), and shows robustness under high-risk and long-tail settings. To capture the behavioral uncertainty, we incorporate a graph neural network (GNN)-based trajectory prediction module, which learns multi-modal future motion distributions. Coupled with the deterministic risk field, it enables dynamic, probabilistic risk inference across time, enabling proactive safety assessment under uncertainty. Evaluations on the highD, inD, and rounD datasets, spanning lane changes, turns, and complex merges, demonstrate that our method significantly outperforms traditional approaches (e.g., TTC, THW, RSS, NC Field) in terms of accuracy, responsiveness, and directional sensitivity, while maintaining strong generalization across scenarios. This framework supports real-time, scenario-adaptive risk forecasting and demonstrates strong generalization across uncertain driving environments. It offers a unified foundation for safety-critical decision-making in long-tail scenarios.",
      "published_utc": "2025-04-22T02:36:54Z",
      "updated_utc": "2025-04-22T02:36:54Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.15541v1",
      "abs_url": "http://arxiv.org/abs/2504.15541v1"
    },
    "2504.13111v3": {
      "arxiv_id": "2504.13111v3",
      "title": "Uncertainty-Aware Trajectory Prediction via Rule-Regularized Heteroscedastic Deep Classification",
      "authors": [
        "Kumar Manas",
        "Christian Schlauch",
        "Adrian Paschke",
        "Christian Wirth",
        "Nadja Klein"
      ],
      "summary": "Deep learning-based trajectory prediction models have demonstrated promising capabilities in capturing complex interactions. However, their out-of-distribution generalization remains a significant challenge, particularly due to unbalanced data and a lack of enough data and diversity to ensure robustness and calibration. To address this, we propose SHIFT (Spectral Heteroscedastic Informed Forecasting for Trajectories), a novel framework that uniquely combines well-calibrated uncertainty modeling with informative priors derived through automated rule extraction. SHIFT reformulates trajectory prediction as a classification task and employs heteroscedastic spectral-normalized Gaussian processes to effectively disentangle epistemic and aleatoric uncertainties. We learn informative priors from training labels, which are automatically generated from natural language driving rules, such as stop rules and drivability constraints, using a retrieval-augmented generation framework powered by a large language model. Extensive evaluations over the nuScenes dataset, including challenging low-data and cross-location scenarios, demonstrate that SHIFT outperforms state-of-the-art methods, achieving substantial gains in uncertainty calibration and displacement metrics. In particular, our model excels in complex scenarios, such as intersections, where uncertainty is inherently higher. Project page: https://kumarmanas.github.io/SHIFT/.",
      "published_utc": "2025-04-17T17:24:50Z",
      "updated_utc": "2025-08-28T12:33:23Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.13111v3",
      "abs_url": "http://arxiv.org/abs/2504.13111v3"
    },
    "2501.13461v2": {
      "arxiv_id": "2501.13461v2",
      "title": "Knowledge-Informed Multi-Agent Trajectory Prediction at Signalized Intersections for Infrastructure-to-Everything",
      "authors": [
        "Huilin Yin",
        "Yangwenhui Xu",
        "Jiaxiang Li",
        "Hao Zhang",
        "Gerhard Rigoll"
      ],
      "summary": "Multi-agent trajectory prediction at signalized intersections is crucial for developing efficient intelligent transportation systems and safe autonomous driving systems. Due to the complexity of intersection scenarios and the limitations of single-vehicle perception, the performance of vehicle-centric prediction methods has reached a plateau. In this paper, we introduce an Infrastructure-to-Everything (I2X) collaborative prediction scheme. In this scheme, roadside units (RSUs) independently forecast the future trajectories of all vehicles and transmit these predictions unidirectionally to subscribing vehicles. Building on this scheme, we propose I2XTraj, a dedicated infrastructure-based trajectory prediction model. I2XTraj leverages real-time traffic signal states, prior maneuver strategy knowledge, and multi-agent interactions to generate accurate, joint multi-modal trajectory prediction. First, a continuous signal-informed mechanism is proposed to adaptively process real-time traffic signals to guide trajectory proposal generation under varied intersection configurations. Second, a driving strategy awareness mechanism estimates the joint distribution of maneuver strategies by integrating spatial priors of intersection areas with dynamic vehicle states, enabling coverage of the full set of feasible maneuvers. Third, a spatial-temporal-mode attention network models multi-agent interactions to refine and adjust joint trajectory outputs.Finally, I2XTraj is evaluated on two real-world datasets of signalized intersections, the V2X-Seq and the SinD drone dataset. In both single-infrastructure and online collaborative scenarios, our model outperforms state-of-the-art methods by over 30\\% on V2X-Seq and 15\\% on SinD, demonstrating strong generalizability and robustness.",
      "published_utc": "2025-01-23T08:23:45Z",
      "updated_utc": "2025-05-17T09:18:02Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.MA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.13461v2",
      "abs_url": "http://arxiv.org/abs/2501.13461v2"
    },
    "2405.02145v1": {
      "arxiv_id": "2405.02145v1",
      "title": "Characterized Diffusion and Spatial-Temporal Interaction Network for Trajectory Prediction in Autonomous Driving",
      "authors": [
        "Haicheng Liao",
        "Xuelin Li",
        "Yongkang Li",
        "Hanlin Kong",
        "Chengyue Wang",
        "Bonan Wang",
        "Yanchen Guan",
        "KaHou Tam",
        "Zhenning Li",
        "Chengzhong Xu"
      ],
      "summary": "Trajectory prediction is a cornerstone in autonomous driving (AD), playing a critical role in enabling vehicles to navigate safely and efficiently in dynamic environments. To address this task, this paper presents a novel trajectory prediction model tailored for accuracy in the face of heterogeneous and uncertain traffic scenarios. At the heart of this model lies the Characterized Diffusion Module, an innovative module designed to simulate traffic scenarios with inherent uncertainty. This module enriches the predictive process by infusing it with detailed semantic information, thereby enhancing trajectory prediction accuracy. Complementing this, our Spatio-Temporal (ST) Interaction Module captures the nuanced effects of traffic scenarios on vehicle dynamics across both spatial and temporal dimensions with remarkable effectiveness. Demonstrated through exhaustive evaluations, our model sets a new standard in trajectory prediction, achieving state-of-the-art (SOTA) results on the Next Generation Simulation (NGSIM), Highway Drone (HighD), and Macao Connected Autonomous Driving (MoCAD) datasets across both short and extended temporal spans. This performance underscores the model's unparalleled adaptability and efficacy in navigating complex traffic scenarios, including highways, urban streets, and intersections.",
      "published_utc": "2024-05-03T14:51:50Z",
      "updated_utc": "2024-05-03T14:51:50Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.02145v1",
      "abs_url": "http://arxiv.org/abs/2405.02145v1"
    },
    "2404.11946v1": {
      "arxiv_id": "2404.11946v1",
      "title": "S4TP: Social-Suitable and Safety-Sensitive Trajectory Planning for Autonomous Vehicles",
      "authors": [
        "Xiao Wang",
        "Ke Tang",
        "Xingyuan Dai",
        "Jintao Xu",
        "Quancheng Du",
        "Rui Ai",
        "Yuxiao Wang",
        "Weihao Gu"
      ],
      "summary": "In public roads, autonomous vehicles (AVs) face the challenge of frequent interactions with human-driven vehicles (HDVs), which render uncertain driving behavior due to varying social characteristics among humans. To effectively assess the risks prevailing in the vicinity of AVs in social interactive traffic scenarios and achieve safe autonomous driving, this article proposes a social-suitable and safety-sensitive trajectory planning (S4TP) framework. Specifically, S4TP integrates the Social-Aware Trajectory Prediction (SATP) and Social-Aware Driving Risk Field (SADRF) modules. SATP utilizes Transformers to effectively encode the driving scene and incorporates an AV's planned trajectory during the prediction decoding process. SADRF assesses the expected surrounding risk degrees during AVs-HDVs interactions, each with different social characteristics, visualized as two-dimensional heat maps centered on the AV. SADRF models the driving intentions of the surrounding HDVs and predicts trajectories based on the representation of vehicular interactions. S4TP employs an optimization-based approach for motion planning, utilizing the predicted HDVs'trajectories as input. With the integration of SADRF, S4TP executes real-time online optimization of the planned trajectory of AV within lowrisk regions, thus improving the safety and the interpretability of the planned trajectory. We have conducted comprehensive tests of the proposed method using the SMARTS simulator. Experimental results in complex social scenarios, such as unprotected left turn intersections, merging, cruising, and overtaking, validate the superiority of our proposed S4TP in terms of safety and rationality. S4TP achieves a pass rate of 100% across all scenarios, surpassing the current state-of-the-art methods Fanta of 98.25% and Predictive-Decision of 94.75%.",
      "published_utc": "2024-04-18T06:58:02Z",
      "updated_utc": "2024-04-18T06:58:02Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.11946v1",
      "abs_url": "http://arxiv.org/abs/2404.11946v1"
    },
    "2404.11181v2": {
      "arxiv_id": "2404.11181v2",
      "title": "KI-GAN: Knowledge-Informed Generative Adversarial Networks for Enhanced Multi-Vehicle Trajectory Forecasting at Signalized Intersections",
      "authors": [
        "Chuheng Wei",
        "Guoyuan Wu",
        "Matthew J. Barth",
        "Amr Abdelraouf",
        "Rohit Gupta",
        "Kyungtae Han"
      ],
      "summary": "Reliable prediction of vehicle trajectories at signalized intersections is crucial to urban traffic management and autonomous driving systems. However, it presents unique challenges, due to the complex roadway layout at intersections, involvement of traffic signal controls, and interactions among different types of road users. To address these issues, we present in this paper a novel model called Knowledge-Informed Generative Adversarial Network (KI-GAN), which integrates both traffic signal information and multi-vehicle interactions to predict vehicle trajectories accurately. Additionally, we propose a specialized attention pooling method that accounts for vehicle orientation and proximity at intersections. Based on the SinD dataset, our KI-GAN model is able to achieve an Average Displacement Error (ADE) of 0.05 and a Final Displacement Error (FDE) of 0.12 for a 6-second observation and 6-second prediction cycle. When the prediction window is extended to 9 seconds, the ADE and FDE values are further reduced to 0.11 and 0.26, respectively. These results demonstrate the effectiveness of the proposed KI-GAN model in vehicle trajectory prediction under complex scenarios at signalized intersections, which represents a significant advancement in the target field.",
      "published_utc": "2024-04-17T08:53:59Z",
      "updated_utc": "2024-04-19T14:28:00Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.11181v2",
      "abs_url": "http://arxiv.org/abs/2404.11181v2"
    },
    "2312.05144v1": {
      "arxiv_id": "2312.05144v1",
      "title": "Kraken: enabling joint trajectory prediction by utilizing Mode Transformer and Greedy Mode Processing",
      "authors": [
        "Daniil S. Antonenko",
        "Stepan Konev",
        "Yuriy Biktairov",
        "Boris Yangel"
      ],
      "summary": "Accurate and reliable motion prediction is essential for safe urban autonomy. The most prominent motion prediction approaches are based on modeling the distribution of possible future trajectories of each actor in autonomous system's vicinity. These \"independent\" marginal predictions might be accurate enough to properly describe casual driving situations where the prediction target is not likely to interact with other actors. They are, however, inadequate for modeling interactive situations where the actors' future trajectories are likely to intersect. To mitigate this issue we propose Kraken -- a real-time trajectory prediction model capable of approximating pairwise interactions between the actors as well as producing accurate marginal predictions. Kraken relies on a simple Greedy Mode Processing technique allowing it to convert a factorized prediction for a pair of agents into a physically-plausible joint prediction. It also utilizes the Mode Transformer module to increase the diversity of predicted trajectories and make the joint prediction more informative. We evaluate Kraken on Waymo Motion Prediction challenge where it held the first place in the Interaction leaderboard and the second place in the Motion leaderboard in October 2021.",
      "published_utc": "2023-12-08T16:24:05Z",
      "updated_utc": "2023-12-08T16:24:05Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.05144v1",
      "abs_url": "http://arxiv.org/abs/2312.05144v1"
    },
    "2309.13893v3": {
      "arxiv_id": "2309.13893v3",
      "title": "Scene Informer: Anchor-based Occlusion Inference and Trajectory Prediction in Partially Observable Environments",
      "authors": [
        "Bernard Lange",
        "Jiachen Li",
        "Mykel J. Kochenderfer"
      ],
      "summary": "Navigating complex and dynamic environments requires autonomous vehicles (AVs) to reason about both visible and occluded regions. This involves predicting the future motion of observed agents, inferring occluded ones, and modeling their interactions based on vectorized scene representations of the partially observable environment. However, prior work on occlusion inference and trajectory prediction have developed in isolation, with the former based on simplified rasterized methods and the latter assuming full environment observability. We introduce the Scene Informer, a unified approach for predicting both observed agent trajectories and inferring occlusions in a partially observable setting. It uses a transformer to aggregate various input modalities and facilitate selective queries on occlusions that might intersect with the AV's planned path. The framework estimates occupancy probabilities and likely trajectories for occlusions, as well as forecast motion for observed agents. We explore common observability assumptions in both domains and their performance impact. Our approach outperforms existing methods in both occupancy prediction and trajectory prediction in partially observable setting on the Waymo Open Motion Dataset.",
      "published_utc": "2023-09-25T06:16:09Z",
      "updated_utc": "2024-03-09T00:40:08Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.13893v3",
      "abs_url": "http://arxiv.org/abs/2309.13893v3"
    },
    "2211.10226v1": {
      "arxiv_id": "2211.10226v1",
      "title": "Leveraging Multi-stream Information Fusion for Trajectory Prediction in Low-illumination Scenarios: A Multi-channel Graph Convolutional Approach",
      "authors": [
        "Hailong Gong",
        "Zirui Li",
        "Chao Lu",
        "Guodong Du",
        "Jianwei Gong"
      ],
      "summary": "Trajectory prediction is a fundamental problem and challenge for autonomous vehicles. Early works mainly focused on designing complicated architectures for deep-learning-based prediction models in normal-illumination environments, which fail in dealing with low-light conditions. This paper proposes a novel approach for trajectory prediction in low-illumination scenarios by leveraging multi-stream information fusion, which flexibly integrates image, optical flow, and object trajectory information. The image channel employs Convolutional Neural Network (CNN) and Long Short-term Memory (LSTM) networks to extract temporal information from the camera. The optical flow channel is applied to capture the pattern of relative motion between adjacent camera frames and modelled by Spatial-Temporal Graph Convolutional Network (ST-GCN). The trajectory channel is used to recognize high-level interactions between vehicles. Finally, information from all the three channels is effectively fused in the prediction module to generate future trajectories of surrounding vehicles in low-illumination conditions. The proposed multi-channel graph convolutional approach is validated on HEV-I and newly generated Dark-HEV-I, egocentric vision datasets that primarily focus on urban intersection scenarios. The results demonstrate that our method outperforms the baselines, in standard and low-illumination scenarios. Additionally, our approach is generic and applicable to scenarios with different types of perception data. The source code of the proposed approach is available at https://github.com/TommyGong08/MSIF}{https://github.com/TommyGong08/MSIF.",
      "published_utc": "2022-11-18T13:25:15Z",
      "updated_utc": "2022-11-18T13:25:15Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.10226v1",
      "abs_url": "http://arxiv.org/abs/2211.10226v1"
    },
    "2207.10398v1": {
      "arxiv_id": "2207.10398v1",
      "title": "D2-TPred: Discontinuous Dependency for Trajectory Prediction under Traffic Lights",
      "authors": [
        "Yuzhen Zhang",
        "Wentong Wang",
        "Weizhi Guo",
        "Pei Lv",
        "Mingliang Xu",
        "Wei Chen",
        "Dinesh Manocha"
      ],
      "summary": "A profound understanding of inter-agent relationships and motion behaviors is important to achieve high-quality planning when navigating in complex scenarios, especially at urban traffic intersections. We present a trajectory prediction approach with respect to traffic lights, D2-TPred, which uses a spatial dynamic interaction graph (SDG) and a behavior dependency graph (BDG) to handle the problem of discontinuous dependency in the spatial-temporal space. Specifically, the SDG is used to capture spatial interactions by reconstructing sub-graphs for different agents with dynamic and changeable characteristics during each frame. The BDG is used to infer motion tendency by modeling the implicit dependency of the current state on priors behaviors, especially the discontinuous motions corresponding to acceleration, deceleration, or turning direction. Moreover, we present a new dataset for vehicle trajectory prediction under traffic lights called VTP-TL. Our experimental results show that our model achieves more than {20.45% and 20.78% }improvement in terms of ADE and FDE, respectively, on VTP-TL as compared to other trajectory prediction algorithms. The dataset and code are available at: https://github.com/VTP-TL/D2-TPred.",
      "published_utc": "2022-07-21T10:19:07Z",
      "updated_utc": "2022-07-21T10:19:07Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.10398v1",
      "abs_url": "http://arxiv.org/abs/2207.10398v1"
    },
    "2202.05140v2": {
      "arxiv_id": "2202.05140v2",
      "title": "Transferable and Adaptable Driving Behavior Prediction",
      "authors": [
        "Letian Wang",
        "Yeping Hu",
        "Liting Sun",
        "Wei Zhan",
        "Masayoshi Tomizuka",
        "Changliu Liu"
      ],
      "summary": "While autonomous vehicles still struggle to solve challenging situations during on-road driving, humans have long mastered the essence of driving with efficient, transferable, and adaptable driving capability. By mimicking humans' cognition model and semantic understanding during driving, we propose HATN, a hierarchical framework to generate high-quality, transferable, and adaptable predictions for driving behaviors in multi-agent dense-traffic environments. Our hierarchical method consists of a high-level intention identification policy and a low-level trajectory generation policy. We introduce a novel semantic sub-task definition and generic state representation for each sub-task. With these techniques, the hierarchical framework is transferable across different driving scenarios. Besides, our model is able to capture variations of driving behaviors among individuals and scenarios by an online adaptation module. We demonstrate our algorithms in the task of trajectory prediction for real traffic data at intersections and roundabouts from the INTERACTION dataset. Through extensive numerical studies, it is evident that our method significantly outperformed other methods in terms of prediction accuracy, transferability, and adaptability. Pushing the state-of-the-art performance by a considerable margin, we also provide a cognitive view of understanding the driving behavior behind such improvement. We highlight that in the future, more research attention and effort are deserved for transferability and adaptability. It is not only due to the promising performance elevation of prediction and planning algorithms, but more fundamentally, they are crucial for the scalable and general deployment of autonomous vehicles.",
      "published_utc": "2022-02-10T16:46:24Z",
      "updated_utc": "2022-02-13T12:45:09Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2202.05140v2",
      "abs_url": "http://arxiv.org/abs/2202.05140v2"
    },
    "2111.00788v3": {
      "arxiv_id": "2111.00788v3",
      "title": "Hierarchical Adaptable and Transferable Networks (HATN) for Driving Behavior Prediction",
      "authors": [
        "Letian Wang",
        "Yeping Hu",
        "Liting Sun",
        "Wei Zhan",
        "Masayoshi Tomizuka",
        "Changliu Liu"
      ],
      "summary": "When autonomous vehicles still struggle to solve challenging situations during on-road driving, humans have long mastered the essence of driving with efficient transferable and adaptable driving capability. By mimicking humans' cognition model and semantic understanding during driving, we present HATN, a hierarchical framework to generate high-quality driving behaviors in multi-agent dense-traffic environments. Our method hierarchically consists of a high-level intention identification and low-level action generation policy. With the semantic sub-task definition and generic state representation, the hierarchical framework is transferable across different driving scenarios. Besides, our model is also able to capture variations of driving behaviors among individuals and scenarios by an online adaptation module. We demonstrate our algorithms in the task of trajectory prediction for real traffic data at intersections and roundabouts, where we conducted extensive studies of the proposed method and demonstrated how our method outperformed other methods in terms of prediction accuracy and transferability.",
      "published_utc": "2021-11-01T09:37:32Z",
      "updated_utc": "2021-12-11T17:05:17Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2111.00788v3",
      "abs_url": "http://arxiv.org/abs/2111.00788v3"
    },
    "2011.12406v2": {
      "arxiv_id": "2011.12406v2",
      "title": "Prediction-Based Reachability for Collision Avoidance in Autonomous Driving",
      "authors": [
        "Anjian Li",
        "Liting Sun",
        "Wei Zhan",
        "Masayoshi Tomizuka",
        "Mo Chen"
      ],
      "summary": "Safety is an important topic in autonomous driving since any collision may cause serious injury to people and damage to property. Hamilton-Jacobi (HJ) Reachability is a formal method that verifies safety in multi-agent interaction and provides a safety controller for collision avoidance. However, due to the worst-case assumption on the cars future behaviours, reachability might result in too much conservatism such that the normal operation of the vehicle is badly hindered. In this paper, we leverage the power of trajectory prediction and propose a prediction-based reachability framework to compute safety controllers. Instead of always assuming the worst case, we cluster the car's behaviors into multiple driving modes, e.g. left turn or right turn. Under each mode, a reachability-based safety controller is designed based on a less conservative action set. For online implementation, we first utilize the trajectory prediction and our proposed mode classifier to predict the possible modes, and then deploy the corresponding safety controller. Through simulations in a T-intersection and an 8-way roundabout, we demonstrate that our prediction-based reachability method largely avoids collision between two interacting cars and reduces the conservatism that the safety controller brings to the car's original operation.",
      "published_utc": "2020-11-24T21:33:01Z",
      "updated_utc": "2021-05-21T07:53:51Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2011.12406v2",
      "abs_url": "http://arxiv.org/abs/2011.12406v2"
    },
    "2010.16267v3": {
      "arxiv_id": "2010.16267v3",
      "title": "Exploring Dynamic Context for Multi-path Trajectory Prediction",
      "authors": [
        "Hao Cheng",
        "Wentong Liao",
        "Xuejiao Tang",
        "Michael Ying Yang",
        "Monika Sester",
        "Bodo Rosenhahn"
      ],
      "summary": "To accurately predict future positions of different agents in traffic scenarios is crucial for safely deploying intelligent autonomous systems in the real-world environment. However, it remains a challenge due to the behavior of a target agent being affected by other agents dynamically and there being more than one socially possible paths the agent could take. In this paper, we propose a novel framework, named Dynamic Context Encoder Network (DCENet). In our framework, first, the spatial context between agents is explored by using self-attention architectures. Then, the two-stream encoders are trained to learn temporal context between steps by taking the respective observed trajectories and the extracted dynamic spatial context as input. The spatial-temporal context is encoded into a latent space using a Conditional Variational Auto-Encoder (CVAE) module. Finally, a set of future trajectories for each agent is predicted conditioned on the learned spatial-temporal context by sampling from the latent space, repeatedly. DCENet is evaluated on one of the most popular challenging benchmarks for trajectory forecasting Trajnet and reports a new state-of-the-art performance. It also demonstrates superior performance evaluated on the benchmark inD for mixed traffic at intersections. A series of ablation studies is conducted to validate the effectiveness of each proposed module. Our code is available at https://github.com/wtliao/DCENet.",
      "published_utc": "2020-10-30T13:39:20Z",
      "updated_utc": "2021-03-24T10:28:47Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.MA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2010.16267v3",
      "abs_url": "http://arxiv.org/abs/2010.16267v3"
    },
    "2005.08307v2": {
      "arxiv_id": "2005.08307v2",
      "title": "AC-VRNN: Attentive Conditional-VRNN for Multi-Future Trajectory Prediction",
      "authors": [
        "Alessia Bertugli",
        "Simone Calderara",
        "Pasquale Coscia",
        "Lamberto Ballan",
        "Rita Cucchiara"
      ],
      "summary": "Anticipating human motion in crowded scenarios is essential for developing intelligent transportation systems, social-aware robots and advanced video surveillance applications. A key component of this task is represented by the inherently multi-modal nature of human paths which makes socially acceptable multiple futures when human interactions are involved. To this end, we propose a generative architecture for multi-future trajectory predictions based on Conditional Variational Recurrent Neural Networks (C-VRNNs). Conditioning mainly relies on prior belief maps, representing most likely moving directions and forcing the model to consider past observed dynamics in generating future positions. Human interactions are modeled with a graph-based attention mechanism enabling an online attentive hidden state refinement of the recurrent estimation. To corroborate our model, we perform extensive experiments on publicly-available datasets (e.g., ETH/UCY, Stanford Drone Dataset, STATS SportVU NBA, Intersection Drone Dataset and TrajNet++) and demonstrate its effectiveness in crowded scenes compared to several state-of-the-art methods.",
      "published_utc": "2020-05-17T17:21:23Z",
      "updated_utc": "2021-07-08T08:23:15Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2005.08307v2",
      "abs_url": "http://arxiv.org/abs/2005.08307v2"
    },
    "2002.01965v1": {
      "arxiv_id": "2002.01965v1",
      "title": "Learning Probabilistic Intersection Traffic Models for Trajectory Prediction",
      "authors": [
        "Andrew Patterson",
        "Aditya Gahlawat",
        "Naira Hovakimyan"
      ],
      "summary": "Autonomous agents must be able to safely interact with other vehicles to integrate into urban environments. The safety of these agents is dependent on their ability to predict collisions with other vehicles' future trajectories for replanning and collision avoidance. The information needed to predict collisions can be learned from previously observed vehicle trajectories in a specific environment, generating a traffic model. The learned traffic model can then be incorporated as prior knowledge into any trajectory estimation method being used in this environment. This work presents a Gaussian process based probabilistic traffic model that is used to quantify vehicle behaviors in an intersection. The Gaussian process model provides estimates for the average vehicle trajectory, while also capturing the variance between the different paths a vehicle may take in the intersection. The method is demonstrated on a set of time-series position trajectories. These trajectories are reconstructed by removing object recognition errors and missed frames that may occur due to data source processing. To create the intersection traffic model, the reconstructed trajectories are clustered based on their source and destination lanes. For each cluster, a Gaussian process model is created to capture the average behavior and the variance of the cluster. To show the applicability of the Gaussian model, the test trajectories are classified with only partial observations. Performance is quantified by the number of observations required to correctly classify the vehicle trajectory. Both the intersection traffic modeling computations and the classification procedure are timed. These times are presented as results and demonstrate that the model can be constructed in a reasonable amount of time and the classification procedure can be used for online applications.",
      "published_utc": "2020-02-05T19:22:26Z",
      "updated_utc": "2020-02-05T19:22:26Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2002.01965v1",
      "abs_url": "http://arxiv.org/abs/2002.01965v1"
    },
    "1911.03801v1": {
      "arxiv_id": "1911.03801v1",
      "title": "Human Driver Behavior Prediction based on UrbanFlow",
      "authors": [
        "Zhiqian Qiao",
        "Jing Zhao",
        "Zachariah Tyree",
        "Priyantha Mudalige",
        "Jeff Schneider",
        "John M. Dolan"
      ],
      "summary": "How autonomous vehicles and human drivers share public transportation systems is an important problem, as fully automatic transportation environments are still a long way off. Understanding human drivers' behavior can be beneficial for autonomous vehicle decision making and planning, especially when the autonomous vehicle is surrounded by human drivers who have various driving behaviors and patterns of interaction with other vehicles. In this paper, we propose an LSTM-based trajectory prediction method for human drivers which can help the autonomous vehicle make better decisions, especially in urban intersection scenarios. Meanwhile, in order to collect human drivers' driving behavior data in the urban scenario, we describe a system called UrbanFlow which includes the whole procedure from raw bird's-eye view data collection via drone to the final processed trajectories. The system is mainly intended for urban scenarios but can be extended to be used for any traffic scenarios.",
      "published_utc": "2019-11-09T23:25:41Z",
      "updated_utc": "2019-11-09T23:25:41Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/1911.03801v1",
      "abs_url": "http://arxiv.org/abs/1911.03801v1"
    },
    "1909.00792v1": {
      "arxiv_id": "1909.00792v1",
      "title": "Conditional Vehicle Trajectories Prediction in CARLA Urban Environment",
      "authors": [
        "Thibault Buhet",
        "Emilie Wirbel",
        "Xavier Perrotton"
      ],
      "summary": "Imitation learning is becoming more and more successful for autonomous driving. End-to-end (raw signal to command) performs well on relatively simple tasks (lane keeping and navigation). Mid-to-mid (environment abstraction to mid-level trajectory representation) or direct perception (raw signal to performance) approaches strive to handle more complex, real life environment and tasks (e.g. complex intersection). In this work, we show that complex urban situations can be handled with raw signal input and mid-level representation. We build a hybrid end-to-mid approach predicting trajectories for neighbor vehicles and for the ego vehicle with a conditional navigation goal. We propose an original architecture inspired from social pooling LSTM taking low and mid level data as input and producing trajectories as polynomials of time. We introduce a label augmentation mechanism to get the level of generalization that is required to control a vehicle. The performance is evaluated on CARLA 0.8 benchmark, showing significant improvements over previously published state of the art.",
      "published_utc": "2019-09-02T16:41:24Z",
      "updated_utc": "2019-09-02T16:41:24Z",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/1909.00792v1",
      "abs_url": "http://arxiv.org/abs/1909.00792v1"
    },
    "1808.06887v5": {
      "arxiv_id": "1808.06887v5",
      "title": "Multimodal Interaction-aware Motion Prediction for Autonomous Street Crossing",
      "authors": [
        "Noha Radwan",
        "Wolfram Burgard",
        "Abhinav Valada"
      ],
      "summary": "For mobile robots navigating on sidewalks, it is essential to be able to safely cross street intersections. Most existing approaches rely on the recognition of the traffic light signal to make an informed crossing decision. Although these approaches have been crucial enablers for urban navigation, the capabilities of robots employing such approaches are still limited to navigating only on streets containing signalized intersections. In this paper, we address this challenge and propose a multimodal convolutional neural network framework to predict the safety of a street intersection for crossing. Our architecture consists of two subnetworks; an interaction-aware trajectory estimation stream IA-TCNN, that predicts the future states of all observed traffic participants in the scene, and a traffic light recognition stream AtteNet. Our IA-TCNN utilizes dilated causal convolutions to model the behavior of the observable dynamic agents in the scene without explicitly assigning priorities to the interactions among them. While AtteNet utilizes Squeeze-Excitation blocks to learn a content-aware mechanism for selecting the relevant features from the data, thereby improving the noise robustness. Learned representations from the traffic light recognition stream are fused with the estimated trajectories from the motion prediction stream to learn the crossing decision. Furthermore, we extend our previously introduced Freiburg Street Crossing dataset with sequences captured at different types of intersections, demonstrating complex interactions among the traffic participants. Extensive experimental evaluations on public benchmark datasets and our proposed dataset demonstrate that our network achieves state-of-the-art performance for each of the subtasks, as well as for the crossing safety prediction.",
      "published_utc": "2018-08-21T13:18:27Z",
      "updated_utc": "2020-08-03T16:41:58Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/1808.06887v5",
      "abs_url": "http://arxiv.org/abs/1808.06887v5"
    },
    "1807.09995v1": {
      "arxiv_id": "1807.09995v1",
      "title": "Naturalistic Driver Intention and Path Prediction using Recurrent Neural Networks",
      "authors": [
        "Alex Zyner",
        "Stewart Worrall",
        "Eduardo Nebot"
      ],
      "summary": "Understanding the intentions of drivers at intersections is a critical component for autonomous vehicles. Urban intersections that do not have traffic signals are a common epicentre of highly variable vehicle movement and interactions. We present a method for predicting driver intent at urban intersections through multi-modal trajectory prediction with uncertainty. Our method is based on recurrent neural networks combined with a mixture density network output layer. To consolidate the multi-modal nature of the output probability distribution, we introduce a clustering algorithm that extracts the set of possible paths that exist in the prediction output, and ranks them according to likelihood. To verify the method's performance and generalizability, we present a real-world dataset that consists of over 23,000 vehicles traversing five different intersections, collected using a vehicle mounted Lidar based tracking system. An array of metrics is used to demonstrate the performance of the model against several baselines.",
      "published_utc": "2018-07-26T07:57:13Z",
      "updated_utc": "2018-07-26T07:57:13Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/1807.09995v1",
      "abs_url": "http://arxiv.org/abs/1807.09995v1"
    },
    "1705.02445v1": {
      "arxiv_id": "1705.02445v1",
      "title": "On human motion prediction using recurrent neural networks",
      "authors": [
        "Julieta Martinez",
        "Michael J. Black",
        "Javier Romero"
      ],
      "summary": "Human motion modelling is a classical problem at the intersection of graphics and computer vision, with applications spanning human-computer interaction, motion synthesis, and motion prediction for virtual and augmented reality. Following the success of deep learning methods in several computer vision tasks, recent work has focused on using deep recurrent neural networks (RNNs) to model human motion, with the goal of learning time-dependent representations that perform tasks such as short-term motion prediction and long-term human motion synthesis. We examine recent work, with a focus on the evaluation methodologies commonly used in the literature, and show that, surprisingly, state-of-the-art performance can be achieved by a simple baseline that does not attempt to model motion at all. We investigate this result, and analyze recent RNN methods by looking at the architectures, loss functions, and training procedures used in state-of-the-art approaches. We propose three changes to the standard RNN models typically used for human motion, which result in a simple and scalable RNN architecture that obtains state-of-the-art performance on human motion prediction.",
      "published_utc": "2017-05-06T05:08:05Z",
      "updated_utc": "2017-05-06T05:08:05Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/1705.02445v1",
      "abs_url": "http://arxiv.org/abs/1705.02445v1"
    },
    "2505.09935v1": {
      "arxiv_id": "2505.09935v1",
      "title": "VRU-CIPI: Crossing Intention Prediction at Intersections for Improving Vulnerable Road Users Safety",
      "authors": [
        "Ahmed S. Abdelrahman",
        "Mohamed Abdel-Aty",
        "Quoc Dai Tran"
      ],
      "summary": "Understanding and predicting human behavior in-thewild, particularly at urban intersections, remains crucial for enhancing interaction safety between road users. Among the most critical behaviors are crossing intentions of Vulnerable Road Users (VRUs), where misinterpretation may result in dangerous conflicts with oncoming vehicles. In this work, we propose the VRU-CIPI framework with a sequential attention-based model designed to predict VRU crossing intentions at intersections. VRU-CIPI employs Gated Recurrent Unit (GRU) to capture temporal dynamics in VRU movements, combined with a multi-head Transformer self-attention mechanism to encode contextual and spatial dependencies critical for predicting crossing direction. Evaluated on UCF-VRU dataset, our proposed achieves state-of-the-art performance with an accuracy of 96.45% and achieving real-time inference speed reaching 33 frames per second. Furthermore, by integrating with Infrastructure-to-Vehicles (I2V) communication, our approach can proactively enhance intersection safety through timely activation of crossing signals and providing early warnings to connected vehicles, ensuring smoother and safer interactions for all road users.",
      "published_utc": "2025-05-15T03:40:29Z",
      "updated_utc": "2025-05-15T03:40:29Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.09935v1",
      "abs_url": "http://arxiv.org/abs/2505.09935v1"
    },
    "2502.15824v1": {
      "arxiv_id": "2502.15824v1",
      "title": "Getting SMARTER for Motion Planning in Autonomous Driving Systems",
      "authors": [
        "Montgomery Alban",
        "Ehsan Ahmadi",
        "Randy Goebel",
        "Amir Rasouli"
      ],
      "summary": "Motion planning is a fundamental problem in autonomous driving and perhaps the most challenging to comprehensively evaluate because of the associated risks and expenses of real-world deployment. Therefore, simulations play an important role in efficient development of planning algorithms. To be effective, simulations must be accurate and realistic, both in terms of dynamics and behavior modeling, and also highly customizable in order to accommodate a broad spectrum of research frameworks. In this paper, we introduce SMARTS 2.0, the second generation of our motion planning simulator which, in addition to being highly optimized for large-scale simulation, provides many new features, such as realistic map integration, vehicle-to-vehicle (V2V) communication, traffic and pedestrian simulation, and a broad variety of sensor models. Moreover, we present a novel benchmark suite for evaluating planning algorithms in various highly challenging scenarios, including interactive driving, such as turning at intersections, and adaptive driving, in which the task is to closely follow a lead vehicle without any explicit knowledge of its intention. Each scenario is characterized by a variety of traffic patterns and road structures. We further propose a series of common and task-specific metrics to effectively evaluate the performance of the planning algorithms. At the end, we evaluate common motion planning algorithms using the proposed benchmark and highlight the challenges the proposed scenarios impose. The new SMARTS 2.0 features and the benchmark are publicly available at github.com/huawei-noah/SMARTS.",
      "published_utc": "2025-02-20T03:51:49Z",
      "updated_utc": "2025-02-20T03:51:49Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.15824v1",
      "abs_url": "http://arxiv.org/abs/2502.15824v1"
    },
    "2311.16091v1": {
      "arxiv_id": "2311.16091v1",
      "title": "Interactive Autonomous Navigation with Internal State Inference and Interactivity Estimation",
      "authors": [
        "Jiachen Li",
        "David Isele",
        "Kanghoon Lee",
        "Jinkyoo Park",
        "Kikuo Fujimura",
        "Mykel J. Kochenderfer"
      ],
      "summary": "Deep reinforcement learning (DRL) provides a promising way for intelligent agents (e.g., autonomous vehicles) to learn to navigate complex scenarios. However, DRL with neural networks as function approximators is typically considered a black box with little explainability and often suffers from suboptimal performance, especially for autonomous navigation in highly interactive multi-agent environments. To address these issues, we propose three auxiliary tasks with spatio-temporal relational reasoning and integrate them into the standard DRL framework, which improves the decision making performance and provides explainable intermediate indicators. We propose to explicitly infer the internal states (i.e., traits and intentions) of surrounding agents (e.g., human drivers) as well as to predict their future trajectories in the situations with and without the ego agent through counterfactual reasoning. These auxiliary tasks provide additional supervision signals to infer the behavior patterns of other interactive agents. Multiple variants of framework integration strategies are compared. We also employ a spatio-temporal graph neural network to encode relations between dynamic entities, which enhances both internal state inference and decision making of the ego agent. Moreover, we propose an interactivity estimation mechanism based on the difference between predicted trajectories in these two situations, which indicates the degree of influence of the ego agent on other agents. To validate the proposed method, we design an intersection driving simulator based on the Intelligent Intersection Driver Model (IIDM) that simulates vehicles and pedestrians. Our approach achieves robust and state-of-the-art performance in terms of standard evaluation metrics and provides explainable intermediate indicators (i.e., internal states, and interactivity scores) for decision making.",
      "published_utc": "2023-11-27T18:57:42Z",
      "updated_utc": "2023-11-27T18:57:42Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.MA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.16091v1",
      "abs_url": "http://arxiv.org/abs/2311.16091v1"
    },
    "2201.04742v1": {
      "arxiv_id": "2201.04742v1",
      "title": "nuReality: A VR environment for research of pedestrian and autonomous vehicle interactions",
      "authors": [
        "Paul Schmitt",
        "Nicholas Britten",
        "JiHyun Jeong",
        "Amelia Coffey",
        "Kevin Clark",
        "Shweta Sunil Kothawade",
        "Elena Corina Grigore",
        "Adam Khaw",
        "Christopher Konopka",
        "Linh Pham",
        "Kim Ryan",
        "Christopher Schmitt",
        "Aryaman Pandya",
        "Emilio Frazzoli"
      ],
      "summary": "We present nuReality, a virtual reality 'VR' environment designed to test the efficacy of vehicular behaviors to communicate intent during interactions between autonomous vehicles 'AVs' and pedestrians at urban intersections. In this project we focus on expressive behaviors as a means for pedestrians to readily recognize the underlying intent of the AV's movements. VR is an ideal tool to use to test these situations as it can be immersive and place subjects into these potentially dangerous scenarios without risk. nuReality provides a novel and immersive virtual reality environment that includes numerous visual details (road and building texturing, parked cars, swaying tree limbs) as well as auditory details (birds chirping, cars honking in the distance, people talking). In these files we present the nuReality environment, its 10 unique vehicle behavior scenarios, and the Unreal Engine and Autodesk Maya source files for each scenario. The files are publicly released as open source at www.nuReality.org, to support the academic community studying the critical AV-pedestrian interaction.",
      "published_utc": "2022-01-12T23:54:09Z",
      "updated_utc": "2022-01-12T23:54:09Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2201.04742v1",
      "abs_url": "http://arxiv.org/abs/2201.04742v1"
    },
    "2010.05115v1": {
      "arxiv_id": "2010.05115v1",
      "title": "Autonomous Vehicle Visual Signals for Pedestrians: Experiments and Design Recommendations",
      "authors": [
        "Henry Chen",
        "Robin Cohen",
        "Kerstin Dautenhahn",
        "Edith Law",
        "Krzysztof Czarnecki"
      ],
      "summary": "Autonomous Vehicles (AV) will transform transportation, but also the interaction between vehicles and pedestrians. In the absence of a driver, it is not clear how an AV can communicate its intention to pedestrians. One option is to use visual signals. To advance their design, we conduct four human-participant experiments and evaluate six representative AV visual signals for visibility, intuitiveness, persuasiveness, and usability at pedestrian crossings. Based on the results, we distill twelve practical design recommendations for AV visual signals, with focus on signal pattern design and placement. Moreover, the paper advances the methodology for experimental evaluation of visual signals, including lab, closed-course, and public road tests using an autonomous vehicle. In addition, the paper also reports insights on pedestrian crosswalk behaviours and the impacts of pedestrian trust towards AVs on the behaviors. We hope that this work will constitute valuable input to the ongoing development of international standards for AV lamps, and thus help mature automated driving in general.",
      "published_utc": "2020-10-10T22:56:46Z",
      "updated_utc": "2020-10-10T22:56:46Z",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.MA",
        "cs.RO",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2010.05115v1",
      "abs_url": "http://arxiv.org/abs/2010.05115v1"
    },
    "2003.09998v1": {
      "arxiv_id": "2003.09998v1",
      "title": "Efficient Behavior-aware Control of Automated Vehicles at Crosswalks using Minimal Information Pedestrian Prediction Model",
      "authors": [
        "Suresh Kumaar Jayaraman",
        "Lionel P. Robert",
        "Xi Jessie Yang",
        "Anuj K. Pradhan",
        "Dawn M. Tilbury"
      ],
      "summary": "For automated vehicles (AVs) to reliably navigate through crosswalks, they need to understand pedestrians crossing behaviors. Simple and reliable pedestrian behavior models aid in real-time AV control by allowing the AVs to predict future pedestrian behaviors. In this paper, we present a Behavior aware Model Predictive Controller (B-MPC) for AVs that incorporates long-term predictions of pedestrian crossing behavior using a previously developed pedestrian crossing model. The model incorporates pedestrians gap acceptance behavior and utilizes minimal pedestrian information, namely their position and speed, to predict pedestrians crossing behaviors. The BMPC controller is validated through simulations and compared to a rule-based controller. By incorporating predictions of pedestrian behavior, the B-MPC controller is able to efficiently plan for longer horizons and handle a wider range of pedestrian interaction scenarios than the rule-based controller. Results demonstrate the applicability of the controller for safe and efficient navigation at crossing scenarios.",
      "published_utc": "2020-03-22T21:34:38Z",
      "updated_utc": "2020-03-22T21:34:38Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CY",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2003.09998v1",
      "abs_url": "http://arxiv.org/abs/2003.09998v1"
    },
    "1809.03705v3": {
      "arxiv_id": "1809.03705v3",
      "title": "Bio-LSTM: A Biomechanically Inspired Recurrent Neural Network for 3D Pedestrian Pose and Gait Prediction",
      "authors": [
        "Xiaoxiao Du",
        "Ram Vasudevan",
        "Matthew Johnson-Roberson"
      ],
      "summary": "In applications such as autonomous driving, it is important to understand, infer, and anticipate the intention and future behavior of pedestrians. This ability allows vehicles to avoid collisions and improve ride safety and quality. This paper proposes a biomechanically inspired recurrent neural network (Bio-LSTM) that can predict the location and 3D articulated body pose of pedestrians in a global coordinate frame, given 3D poses and locations estimated in prior frames with inaccuracy. The proposed network is able to predict poses and global locations for multiple pedestrians simultaneously, for pedestrians up to 45 meters from the cameras (urban intersection scale). The outputs of the proposed network are full-body 3D meshes represented in Skinned Multi-Person Linear (SMPL) model parameters. The proposed approach relies on a novel objective function that incorporates the periodicity of human walking (gait), the mirror symmetry of the human body, and the change of ground reaction forces in a human gait cycle. This paper presents prediction results on the PedX dataset, a large-scale, in-the-wild data set collected at real urban intersections with heavy pedestrian traffic. Results show that the proposed network can successfully learn the characteristics of pedestrian gait and produce accurate and consistent 3D pose predictions.",
      "published_utc": "2018-09-11T07:11:32Z",
      "updated_utc": "2019-09-13T15:28:14Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/1809.03705v3",
      "abs_url": "http://arxiv.org/abs/1809.03705v3"
    },
    "1803.02242v1": {
      "arxiv_id": "1803.02242v1",
      "title": "Early Start Intention Detection of Cyclists Using Motion History Images and a Deep Residual Network",
      "authors": [
        "Stefan Zernetsch",
        "Viktor Kress",
        "Bernhard Sick",
        "Konrad Doll"
      ],
      "summary": "In this article, we present a novel approach to detect starting motions of cyclists in real world traffic scenarios based on Motion History Images (MHIs). The method uses a deep Convolutional Neural Network (CNN) with a residual network architecture (ResNet), which is commonly used in image classification and detection tasks. By combining MHIs with a ResNet classifier and performing a frame by frame classification of the MHIs, we are able to detect starting motions in image sequences. The detection is performed using a wide angle stereo camera system at an urban intersection. We compare our algorithm to an existing method to detect movement transitions of pedestrians that uses MHIs in combination with a Histograms of Oriented Gradients (HOG) like descriptor and a Support Vector Machine (SVM), which we adapted to cyclists. To train and evaluate the methods a dataset containing MHIs of 394 cyclist starting motions was created. The results show that both methods can be used to detect starting motions of cyclists. Using the SVM approach, we were able to safely detect starting motions 0.506 s on average after the bicycle starts moving with an F1-score of 97.7%. The ResNet approach achieved an F1-score of 100% at an average detection time of 0.144 s. The ResNet approach outperformed the SVM approach in both robustness against false positive detections and detection time.",
      "published_utc": "2018-03-06T15:12:27Z",
      "updated_utc": "2018-03-06T15:12:27Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/1803.02242v1",
      "abs_url": "http://arxiv.org/abs/1803.02242v1"
    },
    "2511.09735v1": {
      "arxiv_id": "2511.09735v1",
      "title": "Social LSTM with Dynamic Occupancy Modeling for Realistic Pedestrian Trajectory Prediction",
      "authors": [
        "Ahmed Alia",
        "Mohcine Chraibi",
        "Armin Seyfried"
      ],
      "summary": "In dynamic and crowded environments, realistic pedestrian trajectory prediction remains a challenging task due to the complex nature of human motion and the mutual influences among individuals. Deep learning models have recently achieved promising results by implicitly learning such patterns from 2D trajectory data. However, most approaches treat pedestrians as point entities, ignoring the physical space that each person occupies. To address these limitations, this paper proposes a novel deep learning model that enhances the Social LSTM with a new Dynamic Occupied Space loss function. This loss function guides Social LSTM in learning to avoid realistic collisions without increasing displacement error across different crowd densities, ranging from low to high, in both homogeneous and heterogeneous density settings. Such a function achieves this by combining the average displacement error with a new collision penalty that is sensitive to scene density and individual spatial occupancy. For efficient training and evaluation, five datasets were generated from real pedestrian trajectories recorded during the Festival of Lights in Lyon 2022. Four datasets represent homogeneous crowd conditions -- low, medium, high, and very high density -- while the fifth corresponds to a heterogeneous density distribution. The experimental findings indicate that the proposed model not only lowers collision rates but also enhances displacement prediction accuracy in each dataset. Specifically, the model achieves up to a 31% reduction in the collision rate and reduces the average displacement error and the final displacement error by 5% and 6%, respectively, on average across all datasets compared to the baseline. Moreover, the proposed model consistently outperforms several state-of-the-art deep learning models across most test sets.",
      "published_utc": "2025-11-12T20:49:58Z",
      "updated_utc": "2025-11-12T20:49:58Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2511.09735v1",
      "abs_url": "http://arxiv.org/abs/2511.09735v1"
    },
    "2510.04365v1": {
      "arxiv_id": "2510.04365v1",
      "title": "Diffusion^2: Dual Diffusion Model with Uncertainty-Aware Adaptive Noise for Momentary Trajectory Prediction",
      "authors": [
        "Yuhao Luo",
        "Yuang Zhang",
        "Kehua Chen",
        "Xinyu Zheng",
        "Shucheng Zhang",
        "Sikai Chen",
        "Yinhai Wang"
      ],
      "summary": "Accurate pedestrian trajectory prediction is crucial for ensuring safety and efficiency in autonomous driving and human-robot interaction scenarios. Earlier studies primarily utilized sufficient observational data to predict future trajectories. However, in real-world scenarios, such as pedestrians suddenly emerging from blind spots, sufficient observational data is often unavailable (i.e. momentary trajectory), making accurate prediction challenging and increasing the risk of traffic accidents. Therefore, advancing research on pedestrian trajectory prediction under extreme scenarios is critical for enhancing traffic safety. In this work, we propose a novel framework termed Diffusion^2, tailored for momentary trajectory prediction. Diffusion^2 consists of two sequentially connected diffusion models: one for backward prediction, which generates unobserved historical trajectories, and the other for forward prediction, which forecasts future trajectories. Given that the generated unobserved historical trajectories may introduce additional noise, we propose a dual-head parameterization mechanism to estimate their aleatoric uncertainty and design a temporally adaptive noise module that dynamically modulates the noise scale in the forward diffusion process. Empirically, Diffusion^2 sets a new state-of-the-art in momentary trajectory prediction on ETH/UCY and Stanford Drone datasets.",
      "published_utc": "2025-10-05T21:19:33Z",
      "updated_utc": "2025-10-05T21:19:33Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2510.04365v1",
      "abs_url": "http://arxiv.org/abs/2510.04365v1"
    },
    "2510.03314v1": {
      "arxiv_id": "2510.03314v1",
      "title": "A Comprehensive Review on Artificial Intelligence Empowered Solutions for Enhancing Pedestrian and Cyclist Safety",
      "authors": [
        "Shucheng Zhang",
        "Yan Shi",
        "Bingzhang Wang",
        "Yuang Zhang",
        "Muhammad Monjurul Karim",
        "Kehua Chen",
        "Chenxi Liu",
        "Mehrdad Nasri",
        "Yinhai Wang"
      ],
      "summary": "Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and cyclists, remains a critical global challenge, as conventional infrastructure-based measures often prove inadequate in dynamic urban environments. Recent advances in artificial intelligence (AI), particularly in visual perception and reasoning, open new opportunities for proactive and context-aware VRU protection. However, existing surveys on AI applications for VRUs predominantly focus on detection, offering limited coverage of other vision-based tasks that are essential for comprehensive VRU understanding and protection. This paper presents a state-of-the-art review of recent progress in camera-based AI sensing systems for VRU safety, with an emphasis on developments from the past five years and emerging research trends. We systematically examine four core tasks, namely detection and classification, tracking and reidentification, trajectory prediction, and intent recognition and prediction, which together form the backbone of AI-empowered proactive solutions for VRU protection in intelligent transportation systems. To guide future research, we highlight four major open challenges from the perspectives of data, model, and deployment. By linking advances in visual AI with practical considerations for real-world implementation, this survey aims to provide a foundational reference for the development of next-generation sensing systems to enhance VRU safety.",
      "published_utc": "2025-09-30T23:50:55Z",
      "updated_utc": "2025-09-30T23:50:55Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2510.03314v1",
      "abs_url": "http://arxiv.org/abs/2510.03314v1"
    },
    "2509.15219v1": {
      "arxiv_id": "2509.15219v1",
      "title": "Out-of-Sight Trajectories: Tracking, Fusion, and Prediction",
      "authors": [
        "Haichao Zhang",
        "Yi Xu",
        "Yun Fu"
      ],
      "summary": "Trajectory prediction is a critical task in computer vision and autonomous systems, playing a key role in autonomous driving, robotics, surveillance, and virtual reality. Existing methods often rely on complete and noise-free observational data, overlooking the challenges associated with out-of-sight objects and the inherent noise in sensor data caused by limited camera coverage, obstructions, and the absence of ground truth for denoised trajectories. These limitations pose safety risks and hinder reliable prediction in real-world scenarios. In this extended work, we present advancements in Out-of-Sight Trajectory (OST), a novel task that predicts the noise-free visual trajectories of out-of-sight objects using noisy sensor data. Building on our previous research, we broaden the scope of Out-of-Sight Trajectory Prediction (OOSTraj) to include pedestrians and vehicles, extending its applicability to autonomous driving, robotics, surveillance, and virtual reality. Our enhanced Vision-Positioning Denoising Module leverages camera calibration to establish a vision-positioning mapping, addressing the lack of visual references, while effectively denoising noisy sensor data in an unsupervised manner. Through extensive evaluations on the Vi-Fi and JRDB datasets, our approach achieves state-of-the-art performance in both trajectory denoising and prediction, significantly surpassing previous baselines. Additionally, we introduce comparisons with traditional denoising methods, such as Kalman filtering, and adapt recent trajectory prediction models to our task, providing a comprehensive benchmark. This work represents the first initiative to integrate vision-positioning projection for denoising noisy sensor trajectories of out-of-sight agents, paving the way for future advances. The code and preprocessed datasets are available at github.com/Hai-chao-Zhang/OST",
      "published_utc": "2025-09-18T17:59:16Z",
      "updated_utc": "2025-09-18T17:59:16Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG",
        "cs.MA",
        "cs.MM",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2509.15219v1",
      "abs_url": "http://arxiv.org/abs/2509.15219v1"
    },
    "2509.10570v1": {
      "arxiv_id": "2509.10570v1",
      "title": "Large Foundation Models for Trajectory Prediction in Autonomous Driving: A Comprehensive Survey",
      "authors": [
        "Wei Dai",
        "Shengen Wu",
        "Wei Wu",
        "Zhenhao Wang",
        "Sisuo Lyu",
        "Haicheng Liao",
        "Limin Yu",
        "Weiping Ding",
        "Runwei Guan",
        "Yutao Yue"
      ],
      "summary": "Trajectory prediction serves as a critical functionality in autonomous driving, enabling the anticipation of future motion paths for traffic participants such as vehicles and pedestrians, which is essential for driving safety. Although conventional deep learning methods have improved accuracy, they remain hindered by inherent limitations, including lack of interpretability, heavy reliance on large-scale annotated data, and weak generalization in long-tail scenarios. The rise of Large Foundation Models (LFMs) is transforming the research paradigm of trajectory prediction. This survey offers a systematic review of recent advances in LFMs, particularly Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) for trajectory prediction. By integrating linguistic and scene semantics, LFMs facilitate interpretable contextual reasoning, significantly enhancing prediction safety and generalization in complex environments. The article highlights three core methodologies: trajectory-language mapping, multimodal fusion, and constraint-based reasoning. It covers prediction tasks for both vehicles and pedestrians, evaluation metrics, and dataset analyses. Key challenges such as computational latency, data scarcity, and real-world robustness are discussed, along with future research directions including low-latency inference, causality-aware modeling, and motion foundation models.",
      "published_utc": "2025-09-11T10:30:06Z",
      "updated_utc": "2025-09-11T10:30:06Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2509.10570v1",
      "abs_url": "http://arxiv.org/abs/2509.10570v1"
    },
    "2509.00624v1": {
      "arxiv_id": "2509.00624v1",
      "title": "Vehicle-in-Virtual-Environment (VVE) Method for Developing and Evaluating VRU Safety of Connected and Autonomous Driving with Focus on Bicyclist Safety",
      "authors": [
        "Haochong Chen",
        "Xincheng Cao",
        "Bilin Aksun-Guvenc",
        "Levent Guvenc"
      ],
      "summary": "Extensive research has already been conducted in the autonomous driving field to help vehicles navigate safely and efficiently. At the same time, plenty of current research on vulnerable road user (VRU) safety is performed which largely concentrates on perception, localization, or trajectory prediction of VRUs. However, existing research still exhibits several gaps, including the lack of a unified planning and collision avoidance system for autonomous vehicles, limited investigation into delay tolerant control strategies, and the absence of an efficient and standardized testing methodology. Ensuring VRU safety remains one of the most pressing challenges in autonomous driving, particularly in dynamic and unpredictable environments. In this two year project, we focused on applying the Vehicle in Virtual Environment (VVE) method to develop, evaluate, and demonstrate safety functions for Vulnerable Road Users (VRUs) using automated steering and braking of ADS. In this current second year project report, our primary focus was on enhancing the previous year results while also considering bicyclist safety.",
      "published_utc": "2025-08-30T22:43:14Z",
      "updated_utc": "2025-08-30T22:43:14Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2509.00624v1",
      "abs_url": "http://arxiv.org/abs/2509.00624v1"
    },
    "2508.14523v1": {
      "arxiv_id": "2508.14523v1",
      "title": "Great GATsBi: Hybrid, Multimodal, Trajectory Forecasting for Bicycles using Anticipation Mechanism",
      "authors": [
        "Kevin Riehl",
        "Shaimaa K. El-Baklish",
        "Anastasios Kouvelas",
        "Michail A. Makridis"
      ],
      "summary": "Accurate prediction of road user movement is increasingly required by many applications ranging from advanced driver assistance systems to autonomous driving, and especially crucial for road safety. Even though most traffic accident fatalities account to bicycles, they have received little attention, as previous work focused mainly on pedestrians and motorized vehicles. In this work, we present the Great GATsBi, a domain-knowledge-based, hybrid, multimodal trajectory prediction framework for bicycles. The model incorporates both physics-based modeling (inspired by motorized vehicles) and social-based modeling (inspired by pedestrian movements) to explicitly account for the dual nature of bicycle movement. The social interactions are modeled with a graph attention network, and include decayed historical, but also anticipated, future trajectory data of a bicycles neighborhood, following recent insights from psychological and social studies. The results indicate that the proposed ensemble of physics models -- performing well in the short-term predictions -- and social models -- performing well in the long-term predictions -- exceeds state-of-the-art performance. We also conducted a controlled mass-cycling experiment to demonstrate the framework's performance when forecasting bicycle trajectories and modeling social interactions with road users.",
      "published_utc": "2025-08-20T08:31:35Z",
      "updated_utc": "2025-08-20T08:31:35Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2508.14523v1",
      "abs_url": "http://arxiv.org/abs/2508.14523v1"
    },
    "2508.07079v1": {
      "arxiv_id": "2508.07079v1",
      "title": "Model Predictive Control for Crowd Navigation via Learning-Based Trajectory Prediction",
      "authors": [
        "Mohamed Parvez Aslam",
        "Bojan Derajic",
        "Mohamed-Khalil Bouzidi",
        "Sebastian Bernhard",
        "Jan Oliver Ringert"
      ],
      "summary": "Safe navigation in pedestrian-rich environments remains a key challenge for autonomous robots. This work evaluates the integration of a deep learning-based Social-Implicit (SI) pedestrian trajectory predictor within a Model Predictive Control (MPC) framework on the physical Continental Corriere robot. Tested across varied pedestrian densities, the SI-MPC system is compared to a traditional Constant Velocity (CV) model in both open-loop prediction and closed-loop navigation. Results show that SI improves trajectory prediction - reducing errors by up to 76% in low-density settings - and enhances safety and motion smoothness in crowded scenes. Moreover, real-world deployment reveals discrepancies between open-loop metrics and closed-loop performance, as the SI model yields broader, more cautious predictions. These findings emphasize the importance of system-level evaluation and highlight the SI-MPC framework's promise for safer, more adaptive navigation in dynamic, human-populated environments.",
      "published_utc": "2025-08-09T19:11:28Z",
      "updated_utc": "2025-08-09T19:11:28Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2508.07079v1",
      "abs_url": "http://arxiv.org/abs/2508.07079v1"
    },
    "2507.22742v1": {
      "arxiv_id": "2507.22742v1",
      "title": "Social-Pose: Enhancing Trajectory Prediction with Human Body Pose",
      "authors": [
        "Yang Gao",
        "Saeed Saadatnejad",
        "Alexandre Alahi"
      ],
      "summary": "Accurate human trajectory prediction is one of the most crucial tasks for autonomous driving, ensuring its safety. Yet, existing models often fail to fully leverage the visual cues that humans subconsciously communicate when navigating the space. In this work, we study the benefits of predicting human trajectories using human body poses instead of solely their Cartesian space locations in time. We propose `Social-pose', an attention-based pose encoder that effectively captures the poses of all humans in a scene and their social relations. Our method can be integrated into various trajectory prediction architectures. We have conducted extensive experiments on state-of-the-art models (based on LSTM, GAN, MLP, and Transformer), and showed improvements over all of them on synthetic (Joint Track Auto) and real (Human3.6M, Pedestrians and Cyclists in Road Traffic, and JRDB) datasets. We also explored the advantages of using 2D versus 3D poses, as well as the effect of noisy poses and the application of our pose-based predictor in robot navigation scenarios.",
      "published_utc": "2025-07-30T14:58:48Z",
      "updated_utc": "2025-07-30T14:58:48Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2507.22742v1",
      "abs_url": "http://arxiv.org/abs/2507.22742v1"
    },
    "2506.22111v1": {
      "arxiv_id": "2506.22111v1",
      "title": "Pedestrian Intention and Trajectory Prediction in Unstructured Traffic Using IDD-PeD",
      "authors": [
        "Ruthvik Bokkasam",
        "Shankar Gangisetty",
        "A. H. Abdul Hafez",
        "C. V. Jawahar"
      ],
      "summary": "With the rapid advancements in autonomous driving, accurately predicting pedestrian behavior has become essential for ensuring safety in complex and unpredictable traffic conditions. The growing interest in this challenge highlights the need for comprehensive datasets that capture unstructured environments, enabling the development of more robust prediction models to enhance pedestrian safety and vehicle navigation. In this paper, we introduce an Indian driving pedestrian dataset designed to address the complexities of modeling pedestrian behavior in unstructured environments, such as illumination changes, occlusion of pedestrians, unsignalized scene types and vehicle-pedestrian interactions. The dataset provides high-level and detailed low-level comprehensive annotations focused on pedestrians requiring the ego-vehicle's attention. Evaluation of the state-of-the-art intention prediction methods on our dataset shows a significant performance drop of up to $\\mathbf{15\\%}$, while trajectory prediction methods underperform with an increase of up to $\\mathbf{1208}$ MSE, defeating standard pedestrian datasets. Additionally, we present exhaustive quantitative and qualitative analysis of intention and trajectory baselines. We believe that our dataset will open new challenges for the pedestrian behavior research community to build robust models. Project Page: https://cvit.iiit.ac.in/research/projects/cvit-projects/iddped",
      "published_utc": "2025-06-27T10:41:18Z",
      "updated_utc": "2025-06-27T10:41:18Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.22111v1",
      "abs_url": "http://arxiv.org/abs/2506.22111v1"
    },
    "2506.14144v1": {
      "arxiv_id": "2506.14144v1",
      "title": "SceneAware: Scene-Constrained Pedestrian Trajectory Prediction with LLM-Guided Walkability",
      "authors": [
        "Juho Bai",
        "Inwook Shim"
      ],
      "summary": "Accurate prediction of pedestrian trajectories is essential for applications in robotics and surveillance systems. While existing approaches primarily focus on social interactions between pedestrians, they often overlook the rich environmental context that significantly shapes human movement patterns. In this paper, we propose SceneAware, a novel framework that explicitly incorporates scene understanding to enhance trajectory prediction accuracy. Our method leverages a Vision Transformer~(ViT) scene encoder to process environmental context from static scene images, while Multi-modal Large Language Models~(MLLMs) generate binary walkability masks that distinguish between accessible and restricted areas during training. We combine a Transformer-based trajectory encoder with the ViT-based scene encoder, capturing both temporal dynamics and spatial constraints. The framework integrates collision penalty mechanisms that discourage predicted trajectories from violating physical boundaries, ensuring physically plausible predictions. SceneAware is implemented in both deterministic and stochastic variants. Comprehensive experiments on the ETH/UCY benchmark datasets show that our approach outperforms state-of-the-art methods, with more than 50\\% improvement over previous models. Our analysis based on different trajectory categories shows that the model performs consistently well across various types of pedestrian movement. This highlights the importance of using explicit scene information and shows that our scene-aware approach is both effective and reliable in generating accurate and physically plausible predictions. Code is available at: https://github.com/juho127/SceneAware.",
      "published_utc": "2025-06-17T03:11:31Z",
      "updated_utc": "2025-06-17T03:11:31Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.14144v1",
      "abs_url": "http://arxiv.org/abs/2506.14144v1"
    },
    "2503.08016v1": {
      "arxiv_id": "2503.08016v1",
      "title": "SGNetPose+: Stepwise Goal-Driven Networks with Pose Information for Trajectory Prediction in Autonomous Driving",
      "authors": [
        "Akshat Ghiya",
        "Ali K. AlShami",
        "Jugal Kalita"
      ],
      "summary": "Predicting pedestrian trajectories is essential for autonomous driving systems, as it significantly enhances safety and supports informed decision-making. Accurate predictions enable the prevention of collisions, anticipation of crossing intent, and improved overall system efficiency. In this study, we present SGNetPose+, an enhancement of the SGNet architecture designed to integrate skeleton information or body segment angles with bounding boxes to predict pedestrian trajectories from video data to avoid hazards in autonomous driving. Skeleton information was extracted using a pose estimation model, and joint angles were computed based on the extracted joint data. We also apply temporal data augmentation by horizontally flipping video frames to increase the dataset size and improve performance. Our approach achieves state-of-the-art results on the JAAD and PIE datasets using pose data with the bounding boxes, outperforming the SGNet model. Code is available on Github: SGNetPose+.",
      "published_utc": "2025-03-11T03:45:51Z",
      "updated_utc": "2025-03-11T03:45:51Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.08016v1",
      "abs_url": "http://arxiv.org/abs/2503.08016v1"
    },
    "2501.02530v1": {
      "arxiv_id": "2501.02530v1",
      "title": "UDMC: Unified Decision-Making and Control Framework for Urban Autonomous Driving with Motion Prediction of Traffic Participants",
      "authors": [
        "Haichao Liu",
        "Kai Chen",
        "Yulin Li",
        "Zhenmin Huang",
        "Ming Liu",
        "Jun Ma"
      ],
      "summary": "Current autonomous driving systems often struggle to balance decision-making and motion control while ensuring safety and traffic rule compliance, especially in complex urban environments. Existing methods may fall short due to separate handling of these functionalities, leading to inefficiencies and safety compromises. To address these challenges, we introduce UDMC, an interpretable and unified Level 4 autonomous driving framework. UDMC integrates decision-making and motion control into a single optimal control problem (OCP), considering the dynamic interactions with surrounding vehicles, pedestrians, road lanes, and traffic signals. By employing innovative potential functions to model traffic participants and regulations, and incorporating a specialized motion prediction module, our framework enhances on-road safety and rule adherence. The integrated design allows for real-time execution of flexible maneuvers suited to diverse driving scenarios. High-fidelity simulations conducted in CARLA exemplify the framework's computational efficiency, robustness, and safety, resulting in superior driving performance when compared against various baseline models. Our open-source project is available at https://github.com/henryhcliu/udmc_carla.git.",
      "published_utc": "2025-01-05T13:16:05Z",
      "updated_utc": "2025-01-05T13:16:05Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.DC",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.02530v1",
      "abs_url": "http://arxiv.org/abs/2501.02530v1"
    },
    "2412.03689v1": {
      "arxiv_id": "2412.03689v1",
      "title": "Predicting Pedestrian Crossing Behavior in Germany and Japan: Insights into Model Transferability",
      "authors": [
        "Chi Zhang",
        "Janis Sprenger",
        "Zhongjun Ni",
        "Christian Berger"
      ],
      "summary": "Predicting pedestrian crossing behavior is important for intelligent traffic systems to avoid pedestrian-vehicle collisions. Most existing pedestrian crossing behavior models are trained and evaluated on datasets collected from a single country, overlooking differences between countries. To address this gap, we compared pedestrian road-crossing behavior at unsignalized crossings in Germany and Japan. We presented four types of machine learning models to predict gap selection behavior, zebra crossing usage, and their trajectories using simulator data collected from both countries. When comparing the differences between countries, pedestrians from the study conducted in Japan are more cautious, selecting larger gaps compared to those in Germany. We evaluate and analyze model transferability. Our results show that neural networks outperform other machine learning models in predicting gap selection and zebra crossing usage, while random forest models perform best on trajectory prediction tasks, demonstrating strong performance and transferability. We develop a transferable model using an unsupervised clustering method, which improves prediction accuracy for gap selection and trajectory prediction. These findings provide a deeper understanding of pedestrian crossing behaviors in different countries and offer valuable insights into model transferability.",
      "published_utc": "2024-12-04T19:55:40Z",
      "updated_utc": "2024-12-04T19:55:40Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.03689v1",
      "abs_url": "http://arxiv.org/abs/2412.03689v1"
    },
    "2409.20324v1": {
      "arxiv_id": "2409.20324v1",
      "title": "HEADS-UP: Head-Mounted Egocentric Dataset for Trajectory Prediction in Blind Assistance Systems",
      "authors": [
        "Yasaman Haghighi",
        "Celine Demonsant",
        "Panagiotis Chalimourdas",
        "Maryam Tavasoli Naeini",
        "Jhon Kevin Munoz",
        "Bladimir Bacca",
        "Silvan Suter",
        "Matthieu Gani",
        "Alexandre Alahi"
      ],
      "summary": "In this paper, we introduce HEADS-UP, the first egocentric dataset collected from head-mounted cameras, designed specifically for trajectory prediction in blind assistance systems. With the growing population of blind and visually impaired individuals, the need for intelligent assistive tools that provide real-time warnings about potential collisions with dynamic obstacles is becoming critical. These systems rely on algorithms capable of predicting the trajectories of moving objects, such as pedestrians, to issue timely hazard alerts. However, existing datasets fail to capture the necessary information from the perspective of a blind individual. To address this gap, HEADS-UP offers a novel dataset focused on trajectory prediction in this context. Leveraging this dataset, we propose a semi-local trajectory prediction approach to assess collision risks between blind individuals and pedestrians in dynamic environments. Unlike conventional methods that separately predict the trajectories of both the blind individual (ego agent) and pedestrians, our approach operates within a semi-local coordinate system, a rotated version of the camera's coordinate system, facilitating the prediction process. We validate our method on the HEADS-UP dataset and implement the proposed solution in ROS, performing real-time tests on an NVIDIA Jetson GPU through a user study. Results from both dataset evaluations and live tests demonstrate the robustness and efficiency of our approach.",
      "published_utc": "2024-09-30T14:26:09Z",
      "updated_utc": "2024-09-30T14:26:09Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.20324v1",
      "abs_url": "http://arxiv.org/abs/2409.20324v1"
    },
    "2409.15224v1": {
      "arxiv_id": "2409.15224v1",
      "title": "Enhancing Pedestrian Trajectory Prediction with Crowd Trip Information",
      "authors": [
        "Rei Tamaru",
        "Pei Li",
        "Bin Ran"
      ],
      "summary": "Pedestrian trajectory prediction is essential for various applications in active traffic management, urban planning, traffic control, crowd management, and autonomous driving, aiming to enhance traffic safety and efficiency. Accurately predicting pedestrian trajectories requires a deep understanding of individual behaviors, social interactions, and road environments. Existing studies have developed various models to capture the influence of social interactions and road conditions on pedestrian trajectories. However, these approaches are limited by the lack of a comprehensive view of social interactions and road environments. To address these limitations and enhance the accuracy of pedestrian trajectory prediction, we propose a novel approach incorporating trip information as a new modality into pedestrian trajectory models. We propose RNTransformer, a generic model that utilizes crowd trip information to capture global information on social interactions. We incorporated RNTransformer with various socially aware local pedestrian trajectory prediction models to demonstrate its performance. Specifically, by leveraging a pre-trained RNTransformer when training different pedestrian trajectory prediction models, we observed improvements in performance metrics: a 1.3/2.2% enhancement in ADE/FDE on Social-LSTM, a 6.5/28.4% improvement on Social-STGCNN, and an 8.6/4.3% improvement on S-Implicit. Evaluation results demonstrate that RNTransformer significantly enhances the accuracy of various pedestrian trajectory prediction models across multiple datasets. Further investigation reveals that the RNTransformer effectively guides local models to more accurate directions due to the consideration of global information. By exploring crowd behavior within the road network, our approach shows great promise in improving pedestrian safety through accurate trajectory predictions.",
      "published_utc": "2024-09-23T17:11:31Z",
      "updated_utc": "2024-09-23T17:11:31Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.15224v1",
      "abs_url": "http://arxiv.org/abs/2409.15224v1"
    },
    "2406.18050v1": {
      "arxiv_id": "2406.18050v1",
      "title": "A Multi-Stage Goal-Driven Network for Pedestrian Trajectory Prediction",
      "authors": [
        "Xiuen Wu",
        "Tao Wang",
        "Yuanzheng Cai",
        "Lingyu Liang",
        "George Papageorgiou"
      ],
      "summary": "Pedestrian trajectory prediction plays a pivotal role in ensuring the safety and efficiency of various applications, including autonomous vehicles and traffic management systems. This paper proposes a novel method for pedestrian trajectory prediction, called multi-stage goal-driven network (MGNet). Diverging from prior approaches relying on stepwise recursive prediction and the singular forecasting of a long-term goal, MGNet directs trajectory generation by forecasting intermediate stage goals, thereby reducing prediction errors. The network comprises three main components: a conditional variational autoencoder (CVAE), an attention module, and a multi-stage goal evaluator. Trajectories are encoded using conditional variational autoencoders to acquire knowledge about the approximate distribution of pedestrians' future trajectories, and combined with an attention mechanism to capture the temporal dependency between trajectory sequences. The pivotal module is the multi-stage goal evaluator, which utilizes the encoded feature vectors to predict intermediate goals, effectively minimizing cumulative errors in the recursive inference process. The effectiveness of MGNet is demonstrated through comprehensive experiments on the JAAD and PIE datasets. Comparative evaluations against state-of-the-art algorithms reveal significant performance improvements achieved by our proposed method.",
      "published_utc": "2024-06-26T03:59:21Z",
      "updated_utc": "2024-06-26T03:59:21Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.18050v1",
      "abs_url": "http://arxiv.org/abs/2406.18050v1"
    },
    "2406.02436v3": {
      "arxiv_id": "2406.02436v3",
      "title": "Safe, Out-of-Distribution-Adaptive MPC with Conformalized Neural Network Ensembles",
      "authors": [
        "Jose Leopoldo Contreras",
        "Ola Shorinwa",
        "Mac Schwager"
      ],
      "summary": "We present SODA-MPC, a Safe, Out-of-Distribution-Adaptive Model Predictive Control algorithm, which uses an ensemble of learned models for prediction, with a runtime monitor to flag unreliable out-of-distribution (OOD) predictions. When an OOD situation is detected, SODA-MPC triggers a safe fallback control strategy based on reachability, yielding a control framework that achieves the high performance of learning-based models while preserving the safety of reachability-based control. We demonstrate the method in the context of an autonomous vehicle, driving among dynamic pedestrians, where SODA-MPC uses a neural network ensemble for pedestrian prediction. We calibrate the OOD signal using conformal prediction to derive an OOD detector with probabilistic guarantees on the false-positive rate, given a user-specified confidence level. During in-distribution operation, the MPC controller avoids collisions with a pedestrian based on the trajectory predicted by the mean of the ensemble. When OOD conditions are detected, the MPC switches to a reachability-based controller to avoid collisions with the reachable set of the pedestrian assuming a maximum pedestrian speed, to guarantee safety under the worst-case actions of the pedestrian. We verify SODA-MPC in extensive autonomous driving simulations in a pedestrian-crossing scenario. Our model ensemble is trained and calibrated with real pedestrian data, showing that our OOD detector obtains the desired accuracy rate within a theoretically-predicted range. We empirically show improved safety and improved task completion compared with two state-of-the-art MPC methods that also use conformal prediction, but without OOD adaptation. Further, we demonstrate the effectiveness of our method with the large-scale multi-agent predictor Trajectron++, using large-scale traffic data from the nuScenes dataset for training and calibration.",
      "published_utc": "2024-06-04T15:58:14Z",
      "updated_utc": "2025-06-04T14:49:13Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.02436v3",
      "abs_url": "http://arxiv.org/abs/2406.02436v3"
    },
    "2404.15557v2": {
      "arxiv_id": "2404.15557v2",
      "title": "Safe POMDP Online Planning among Dynamic Agents via Adaptive Conformal Prediction",
      "authors": [
        "Shili Sheng",
        "Pian Yu",
        "David Parker",
        "Marta Kwiatkowska",
        "Lu Feng"
      ],
      "summary": "Online planning for partially observable Markov decision processes (POMDPs) provides efficient techniques for robot decision-making under uncertainty. However, existing methods fall short of preventing safety violations in dynamic environments. This work presents a novel safe POMDP online planning approach that maximizes expected returns while providing probabilistic safety guarantees amidst environments populated by multiple dynamic agents. Our approach utilizes data-driven trajectory prediction models of dynamic agents and applies Adaptive Conformal Prediction (ACP) to quantify the uncertainties in these predictions. Leveraging the obtained ACP-based trajectory predictions, our approach constructs safety shields on-the-fly to prevent unsafe actions within POMDP online planning. Through experimental evaluation in various dynamic environments using real-world pedestrian trajectory data, the proposed approach has been shown to effectively maintain probabilistic safety guarantees while accommodating up to hundreds of dynamic agents.",
      "published_utc": "2024-04-23T23:11:42Z",
      "updated_utc": "2024-09-06T20:21:15Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.15557v2",
      "abs_url": "http://arxiv.org/abs/2404.15557v2"
    },
    "2404.02227v1": {
      "arxiv_id": "2404.02227v1",
      "title": "OOSTraj: Out-of-Sight Trajectory Prediction With Vision-Positioning Denoising",
      "authors": [
        "Haichao Zhang",
        "Yi Xu",
        "Hongsheng Lu",
        "Takayuki Shimizu",
        "Yun Fu"
      ],
      "summary": "Trajectory prediction is fundamental in computer vision and autonomous driving, particularly for understanding pedestrian behavior and enabling proactive decision-making. Existing approaches in this field often assume precise and complete observational data, neglecting the challenges associated with out-of-view objects and the noise inherent in sensor data due to limited camera range, physical obstructions, and the absence of ground truth for denoised sensor data. Such oversights are critical safety concerns, as they can result in missing essential, non-visible objects. To bridge this gap, we present a novel method for out-of-sight trajectory prediction that leverages a vision-positioning technique. Our approach denoises noisy sensor observations in an unsupervised manner and precisely maps sensor-based trajectories of out-of-sight objects into visual trajectories. This method has demonstrated state-of-the-art performance in out-of-sight noisy sensor trajectory denoising and prediction on the Vi-Fi and JRDB datasets. By enhancing trajectory prediction accuracy and addressing the challenges of out-of-sight objects, our work significantly contributes to improving the safety and reliability of autonomous driving in complex environments. Our work represents the first initiative towards Out-Of-Sight Trajectory prediction (OOSTraj), setting a new benchmark for future research. The code is available at \\url{https://github.com/Hai-chao-Zhang/OOSTraj}.",
      "published_utc": "2024-04-02T18:30:29Z",
      "updated_utc": "2024-04-02T18:30:29Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.02227v1",
      "abs_url": "http://arxiv.org/abs/2404.02227v1"
    },
    "2403.16485v1": {
      "arxiv_id": "2403.16485v1",
      "title": "Real-time Model Predictive Control with Zonotope-Based Neural Networks for Bipedal Social Navigation",
      "authors": [
        "Abdulaziz Shamsah",
        "Krishanu Agarwal",
        "Shreyas Kousik",
        "Ye Zhao"
      ],
      "summary": "This study addresses the challenge of bipedal navigation in a dynamic human-crowded environment, a research area that remains largely underexplored in the field of legged navigation. We propose two cascaded zonotope-based neural networks: a Pedestrian Prediction Network (PPN) for pedestrians' future trajectory prediction and an Ego-agent Social Network (ESN) for ego-agent social path planning. Representing future paths as zonotopes allows for efficient reachability-based planning and collision checking. The ESN is then integrated with a Model Predictive Controller (ESN-MPC) for footstep planning for our bipedal robot Digit designed by Agility Robotics. ESN-MPC solves for a collision-free optimal trajectory by optimizing through the gradients of ESN. ESN-MPC optimal trajectory is sent to the low-level controller for full-order simulation of Digit. The overall proposed framework is validated with extensive simulations on randomly generated initial settings with varying human crowd densities.",
      "published_utc": "2024-03-25T07:12:51Z",
      "updated_utc": "2024-03-25T07:12:51Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.16485v1",
      "abs_url": "http://arxiv.org/abs/2403.16485v1"
    },
    "2402.08698v2": {
      "arxiv_id": "2402.08698v2",
      "title": "AMEND: A Mixture of Experts Framework for Long-tailed Trajectory Prediction",
      "authors": [
        "Ray Coden Mercurius",
        "Ehsan Ahmadi",
        "Soheil Mohamad Alizadeh Shabestary",
        "Amir Rasouli"
      ],
      "summary": "Accurate prediction of pedestrians' future motions is critical for intelligent driving systems. Developing models for this task requires rich datasets containing diverse sets of samples. However, the existing naturalistic trajectory prediction datasets are generally imbalanced in favor of simpler samples and lack challenging scenarios. Such a long-tail effect causes prediction models to underperform on the tail portion of the data distribution containing safety-critical scenarios. Previous methods tackle the long-tail problem using methods such as contrastive learning and class-conditioned hypernetworks. These approaches, however, are not modular and cannot be applied to many machine learning architectures. In this work, we propose a modular model-agnostic framework for trajectory prediction that leverages a specialized mixture of experts. In our approach, each expert is trained with a specialized skill with respect to a particular part of the data. To produce predictions, we utilise a router network that selects the best expert by generating relative confidence scores. We conduct experimentation on common pedestrian trajectory prediction datasets and show that our method improves performance on long-tail scenarios. We further conduct ablation studies to highlight the contribution of different proposed components.",
      "published_utc": "2024-02-13T02:43:41Z",
      "updated_utc": "2024-04-26T18:02:38Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.08698v2",
      "abs_url": "http://arxiv.org/abs/2402.08698v2"
    },
    "2402.03893v2": {
      "arxiv_id": "2402.03893v2",
      "title": "Prediction Horizon Requirements for Automated Driving: Optimizing Safety, Comfort, and Efficiency",
      "authors": [
        "Manuel Muñoz Sánchez",
        "Chris van der Ploeg",
        "Robin Smit",
        "Jos Elfring",
        "Emilia Silvas",
        "René van de Molengraft"
      ],
      "summary": "Predicting the movement of other road users is beneficial for improving automated vehicle (AV) performance. However, the relationship between the time horizon associated with these predictions and AV performance remains unclear. Despite the existence of numerous trajectory prediction algorithms, no studies have been conducted on how varying prediction lengths affect AV safety and other vehicle performance metrics, resulting in undefined horizon requirements for prediction methods. Our study addresses this gap by examining the effects of different prediction horizons on AV performance, focusing on safety, comfort, and efficiency. Through multiple experiments using a state-of-the-art, risk-based predictive trajectory planner, we simulated predictions with horizons up to 20 seconds. Based on our simulations, we propose a framework for specifying the minimum required and optimal prediction horizons based on specific AV performance criteria and application needs. Our results indicate that a horizon of 1.6 seconds is required to prevent collisions with crossing pedestrians, horizons of 7-8 seconds yield the best efficiency, and horizons up to 15 seconds improve passenger comfort. We conclude that prediction horizon requirements are application-dependent, and recommend aiming for a prediction horizon of 11.8 seconds as a general guideline for applications involving crossing pedestrians.",
      "published_utc": "2024-02-06T10:58:13Z",
      "updated_utc": "2024-04-10T13:34:24Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.03893v2",
      "abs_url": "http://arxiv.org/abs/2402.03893v2"
    },
    "2312.15881v2": {
      "arxiv_id": "2312.15881v2",
      "title": "Attention-aware Social Graph Transformer Networks for Stochastic Trajectory Prediction",
      "authors": [
        "Yao Liu",
        "Binghao Li",
        "Xianzhi Wang",
        "Claude Sammut",
        "Lina Yao"
      ],
      "summary": "Trajectory prediction is fundamental to various intelligent technologies, such as autonomous driving and robotics. The motion prediction of pedestrians and vehicles helps emergency braking, reduces collisions, and improves traffic safety. Current trajectory prediction research faces problems of complex social interactions, high dynamics and multi-modality. Especially, it still has limitations in long-time prediction. We propose Attention-aware Social Graph Transformer Networks for multi-modal trajectory prediction. We combine Graph Convolutional Networks and Transformer Networks by generating stable resolution pseudo-images from Spatio-temporal graphs through a designed stacking and interception method. Furthermore, we design the attention-aware module to handle social interaction information in scenarios involving mixed pedestrian-vehicle traffic. Thus, we maintain the advantages of the Graph and Transformer, i.e., the ability to aggregate information over an arbitrary number of neighbors and the ability to perform complex time-dependent data processing. We conduct experiments on datasets involving pedestrian, vehicle, and mixed trajectories, respectively. Our results demonstrate that our model minimizes displacement errors across various metrics and significantly reduces the likelihood of collisions. It is worth noting that our model effectively reduces the final displacement error, illustrating the ability of our model to predict for a long time.",
      "published_utc": "2023-12-26T04:24:01Z",
      "updated_utc": "2024-05-11T14:38:52Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.15881v2",
      "abs_url": "http://arxiv.org/abs/2312.15881v2"
    },
    "2312.03296v1": {
      "arxiv_id": "2312.03296v1",
      "title": "Cooperative Probabilistic Trajectory Forecasting under Occlusion",
      "authors": [
        "Anshul Nayak",
        "Azim Eskandarian"
      ],
      "summary": "Perception and planning under occlusion is essential for safety-critical tasks. Occlusion-aware planning often requires communicating the information of the occluded object to the ego agent for safe navigation. However, communicating rich sensor information under adverse conditions during communication loss and limited bandwidth may not be always feasible. Further, in GPS denied environments and indoor navigation, localizing and sharing of occluded objects can be challenging. To overcome this, relative pose estimation between connected agents sharing a common field of view can be a computationally effective way of communicating information about surrounding objects. In this paper, we design an end-to-end network that cooperatively estimates the current states of occluded pedestrian in the reference frame of ego agent and then predicts the trajectory with safety guarantees. Experimentally, we show that the uncertainty-aware trajectory prediction of occluded pedestrian by the ego agent is almost similar to the ground truth trajectory assuming no occlusion. The current research holds promise for uncertainty-aware navigation among multiple connected agents under occlusion.",
      "published_utc": "2023-12-06T05:36:52Z",
      "updated_utc": "2023-12-06T05:36:52Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.03296v1",
      "abs_url": "http://arxiv.org/abs/2312.03296v1"
    },
    "2311.15193v2": {
      "arxiv_id": "2311.15193v2",
      "title": "IA-LSTM: Interaction-Aware LSTM for Pedestrian Trajectory Prediction",
      "authors": [
        "Yuehai Chen"
      ],
      "summary": "Predicting the trajectory of pedestrians in crowd scenarios is indispensable in self-driving or autonomous mobile robot field because estimating the future locations of pedestrians around is beneficial for policy decision to avoid collision. It is a challenging issue because humans have different walking motions, and the interactions between humans and objects in the current environment, especially between humans themselves, are complex. Previous researchers focused on how to model human-human interactions but neglected the relative importance of interactions. To address this issue, a novel mechanism based on correntropy is introduced. The proposed mechanism not only can measure the relative importance of human-human interactions but also can build personal space for each pedestrian. An interaction module including this data-driven mechanism is further proposed. In the proposed module, the data-driven mechanism can effectively extract the feature representations of dynamic human-human interactions in the scene and calculate the corresponding weights to represent the importance of different interactions. To share such social messages among pedestrians, an interaction-aware architecture based on long short-term memory network for trajectory prediction is designed. Experiments are conducted on two public datasets. Experimental results demonstrate that our model can achieve better performance than several latest methods with good performance.",
      "published_utc": "2023-11-26T05:17:11Z",
      "updated_utc": "2024-01-25T06:32:22Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.15193v2",
      "abs_url": "http://arxiv.org/abs/2311.15193v2"
    },
    "2311.14922v3": {
      "arxiv_id": "2311.14922v3",
      "title": "GDTS: Goal-Guided Diffusion Model with Tree Sampling for Multi-Modal Pedestrian Trajectory Prediction",
      "authors": [
        "Ge Sun",
        "Sheng Wang",
        "Lei Zhu",
        "Ming Liu",
        "Jun Ma"
      ],
      "summary": "Accurate prediction of pedestrian trajectories is crucial for improving the safety of autonomous driving. However, this task is generally nontrivial due to the inherent stochasticity of human motion, which naturally requires the predictor to generate multi-modal prediction. Previous works leverage various generative methods, such as GAN and VAE, for pedestrian trajectory prediction. Nevertheless, these methods may suffer from mode collapse and relatively low-quality results. The denoising diffusion probabilistic model (DDPM) has recently been applied to trajectory prediction due to its simple training process and powerful reconstruction ability. However, current diffusion-based methods do not fully utilize input information and usually require many denoising iterations that lead to a long inference time or an additional network for initialization. To address these challenges and facilitate the use of diffusion models in multi-modal trajectory prediction, we propose GDTS, a novel Goal-Guided Diffusion Model with Tree Sampling for multi-modal trajectory prediction. Considering the \"goal-driven\" characteristics of human motion, GDTS leverages goal estimation to guide the generation of the diffusion network. A two-stage tree sampling algorithm is presented, which leverages common features to reduce the inference time and improve accuracy for multi-modal prediction. Experimental results demonstrate that our proposed framework achieves comparable state-of-the-art performance with real-time inference speed in public datasets.",
      "published_utc": "2023-11-25T03:55:06Z",
      "updated_utc": "2025-03-03T07:41:00Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.14922v3",
      "abs_url": "http://arxiv.org/abs/2311.14922v3"
    },
    "2312.10041v1": {
      "arxiv_id": "2312.10041v1",
      "title": "Digital Twin Technology Enabled Proactive Safety Application for Vulnerable Road Users: A Real-World Case Study",
      "authors": [
        "Erik Rua",
        "Kazi Hasan Shakib",
        "Sagar Dasgupta",
        "Mizanur Rahman",
        "Steven Jones"
      ],
      "summary": "While measures, such as traffic calming and advance driver assistance systems, can improve safety for Vulnerable Road Users (VRUs), their effectiveness ultimately relies on the responsible behavior of drivers and pedestrians who must adhere to traffic rules or take appropriate actions. However, these measures offer no solution in scenarios where a collision becomes imminent, leaving no time for warning or corrective actions. Recently, connected vehicle technology has introduced warning services that can alert drivers and VRUs about potential collisions. Nevertheless, there is still a significant gap in the system's ability to predict collisions in advance. The objective of this study is to utilize Digital Twin (DT) technology to enable a proactive safety alert system for VRUs. A pedestrian-vehicle trajectory prediction model has been developed using the Encoder-Decoder Long Short-Term Memory (LSTM) architecture to predict future trajectories of pedestrians and vehicles. Subsequently, parallel evaluation of all potential future safety-critical scenarios is carried out. Three Encoder-Decoder LSTM models, namely pedestrian-LSTM, vehicle-through-LSTM, and vehicle-left-turn-LSTM, are trained and validated using field-collected data, achieving corresponding root mean square errors (RMSE) of 0.049, 1.175, and 0.355 meters, respectively. A real-world case study has been conducted where a pedestrian crosses a road, and vehicles have the option to proceed through or left-turn, to evaluate the efficacy of DT-enabled proactive safety alert systems. Experimental results confirm that DT-enabled safety alert systems were succesfully able to detect potential crashes and proactively generate safety alerts to reduce potential crash risk.",
      "published_utc": "2023-11-24T15:27:57Z",
      "updated_utc": "2023-11-24T15:27:57Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.10041v1",
      "abs_url": "http://arxiv.org/abs/2312.10041v1"
    },
    "2311.04383v1": {
      "arxiv_id": "2311.04383v1",
      "title": "Active Collision Avoidance System for E-Scooters in Pedestrian Environment",
      "authors": [
        "Xuke Yan",
        "Dan Shen"
      ],
      "summary": "In the dense fabric of urban areas, electric scooters have rapidly become a preferred mode of transportation. As they cater to modern mobility demands, they present significant safety challenges, especially when interacting with pedestrians. In general, e-scooters are suggested to be ridden in bike lanes/sidewalks or share the road with cars at the maximum speed of about 15-20 mph, which is more flexible and much faster than pedestrians and bicyclists. Accurate prediction of pedestrian movement, coupled with assistant motion control of scooters, is essential in minimizing collision risks and seamlessly integrating scooters in areas dense with pedestrians. Addressing these safety concerns, our research introduces a novel e-Scooter collision avoidance system (eCAS) with a method for predicting pedestrian trajectories, employing an advanced LSTM network integrated with a state refinement module. This proactive model is designed to ensure unobstructed movement in areas with substantial pedestrian traffic without collisions. Results are validated on two public datasets, ETH and UCY, providing encouraging outcomes. Our model demonstrated proficiency in anticipating pedestrian paths and augmented scooter path planning, allowing for heightened adaptability in densely populated locales. This study shows the potential of melding pedestrian trajectory prediction with scooter motion planning. With the ubiquity of electric scooters in urban environments, such advancements have become crucial to safeguard all participants in urban transit.",
      "published_utc": "2023-11-07T23:07:22Z",
      "updated_utc": "2023-11-07T23:07:22Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.04383v1",
      "abs_url": "http://arxiv.org/abs/2311.04383v1"
    },
    "2310.14570v1": {
      "arxiv_id": "2310.14570v1",
      "title": "DICE: Diverse Diffusion Model with Scoring for Trajectory Prediction",
      "authors": [
        "Younwoo Choi",
        "Ray Coden Mercurius",
        "Soheil Mohamad Alizadeh Shabestary",
        "Amir Rasouli"
      ],
      "summary": "Road user trajectory prediction in dynamic environments is a challenging but crucial task for various applications, such as autonomous driving. One of the main challenges in this domain is the multimodal nature of future trajectories stemming from the unknown yet diverse intentions of the agents. Diffusion models have shown to be very effective in capturing such stochasticity in prediction tasks. However, these models involve many computationally expensive denoising steps and sampling operations that make them a less desirable option for real-time safety-critical applications. To this end, we present a novel framework that leverages diffusion models for predicting future trajectories in a computationally efficient manner. To minimize the computational bottlenecks in iterative sampling, we employ an efficient sampling mechanism that allows us to maximize the number of sampled trajectories for improved accuracy while maintaining inference time in real time. Moreover, we propose a scoring mechanism to select the most plausible trajectories by assigning relative ranks. We show the effectiveness of our approach by conducting empirical evaluations on common pedestrian (UCY/ETH) and autonomous driving (nuScenes) benchmark datasets on which our model achieves state-of-the-art performance on several subsets and metrics.",
      "published_utc": "2023-10-23T05:04:23Z",
      "updated_utc": "2023-10-23T05:04:23Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.14570v1",
      "abs_url": "http://arxiv.org/abs/2310.14570v1"
    },
    "2308.06654v1": {
      "arxiv_id": "2308.06654v1",
      "title": "Polar Collision Grids: Effective Interaction Modelling for Pedestrian Trajectory Prediction in Shared Space Using Collision Checks",
      "authors": [
        "Mahsa Golchoubian",
        "Moojan Ghafurian",
        "Kerstin Dautenhahn",
        "Nasser Lashgarian Azad"
      ],
      "summary": "Predicting pedestrians' trajectories is a crucial capability for autonomous vehicles' safe navigation, especially in spaces shared with pedestrians. Pedestrian motion in shared spaces is influenced by both the presence of vehicles and other pedestrians. Therefore, effectively modelling both pedestrian-pedestrian and pedestrian-vehicle interactions can increase the accuracy of the pedestrian trajectory prediction models. Despite the huge literature on ways to encode the effect of interacting agents on a pedestrian's predicted trajectory using deep-learning models, limited effort has been put into the effective selection of interacting agents. In the majority of cases, the interaction features used are mainly based on relative distances while paying less attention to the effect of the velocity and approaching direction in the interaction formulation. In this paper, we propose a heuristic-based process of selecting the interacting agents based on collision risk calculation. Focusing on interactions of potentially colliding agents with a target pedestrian, we propose the use of time-to-collision and the approach direction angle of two agents for encoding the interaction effect. This is done by introducing a novel polar collision grid map. Our results have shown predicted trajectories closer to the ground truth compared to existing methods (used as a baseline) on the HBS dataset.",
      "published_utc": "2023-08-13T00:20:22Z",
      "updated_utc": "2023-08-13T00:20:22Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.HC",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.06654v1",
      "abs_url": "http://arxiv.org/abs/2308.06654v1"
    },
    "2307.05288v2": {
      "arxiv_id": "2307.05288v2",
      "title": "Navigating Uncertainty: The Role of Short-Term Trajectory Prediction in Autonomous Vehicle Safety",
      "authors": [
        "Sushil Sharma",
        "Ganesh Sistu",
        "Lucie Yahiaoui",
        "Arindam Das",
        "Mark Halton",
        "Ciarán Eising"
      ],
      "summary": "Autonomous vehicles require accurate and reliable short-term trajectory predictions for safe and efficient driving. While most commercial automated vehicles currently use state machine-based algorithms for trajectory forecasting, recent efforts have focused on end-to-end data-driven systems. Often, the design of these models is limited by the availability of datasets, which are typically restricted to generic scenarios. To address this limitation, we have developed a synthetic dataset for short-term trajectory prediction tasks using the CARLA simulator. This dataset is extensive and incorporates what is considered complex scenarios - pedestrians crossing the road, vehicles overtaking - and comprises 6000 perspective view images with corresponding IMU and odometry information for each frame. Furthermore, an end-to-end short-term trajectory prediction model using convolutional neural networks (CNN) and long short-term memory (LSTM) networks has also been developed. This model can handle corner cases, such as slowing down near zebra crossings and stopping when pedestrians cross the road, without the need for explicit encoding of the surrounding environment. In an effort to accelerate this research and assist others, we are releasing our dataset and model to the research community. Our datasets are publicly available on https://github.com/sharmasushil/Navigating-Uncertainty-Trajectory-Prediction .",
      "published_utc": "2023-07-11T14:28:33Z",
      "updated_utc": "2023-07-12T09:25:03Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.05288v2",
      "abs_url": "http://arxiv.org/abs/2307.05288v2"
    },
    "2305.15942v1": {
      "arxiv_id": "2305.15942v1",
      "title": "Comparison of Pedestrian Prediction Models from Trajectory and Appearance Data for Autonomous Driving",
      "authors": [
        "Anthony Knittel",
        "Morris Antonello",
        "John Redford",
        "Subramanian Ramamoorthy"
      ],
      "summary": "The ability to anticipate pedestrian motion changes is a critical capability for autonomous vehicles. In urban environments, pedestrians may enter the road area and create a high risk for driving, and it is important to identify these cases. Typical predictors use the trajectory history to predict future motion, however in cases of motion initiation, motion in the trajectory may only be clearly visible after a delay, which can result in the pedestrian has entered the road area before an accurate prediction can be made. Appearance data includes useful information such as changes of gait, which are early indicators of motion changes, and can inform trajectory prediction. This work presents a comparative evaluation of trajectory-only and appearance-based methods for pedestrian prediction, and introduces a new dataset experiment for prediction using appearance. We create two trajectory and image datasets based on the combination of image and trajectory sequences from the popular NuScenes dataset, and examine prediction of trajectories using observed appearance to influence futures. This shows some advantages over trajectory prediction alone, although problems with the dataset prevent advantages of appearance-based models from being shown. We describe methods for improving the dataset and experiment to allow benefits of appearance-based models to be captured.",
      "published_utc": "2023-05-25T11:24:38Z",
      "updated_utc": "2023-05-25T11:24:38Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.15942v1",
      "abs_url": "http://arxiv.org/abs/2305.15942v1"
    },
    "2305.02859v2": {
      "arxiv_id": "2305.02859v2",
      "title": "Social Robot Navigation through Constrained Optimization: a Comparative Study of Uncertainty-based Objectives and Constraints",
      "authors": [
        "Timur Akhtyamov",
        "Aleksandr Kashirin",
        "Aleksey Postnikov",
        "Gonzalo Ferrer"
      ],
      "summary": "This work is dedicated to the study of how uncertainty estimation of the human motion prediction can be embedded into constrained optimization techniques, such as Model Predictive Control (MPC) for the social robot navigation. We propose several cost objectives and constraint functions obtained from the uncertainty of predicting pedestrian positions and related to the probability of the collision that can be applied to the MPC, and all the different variants are compared in challenging scenes with multiple agents. The main question this paper tries to answer is: what are the most important uncertainty-based criteria for social MPC? For that, we evaluate the proposed approaches with several social navigation metrics in an extensive set of scenarios of different complexity in reproducible synthetic environments. The main outcome of our study is a foundation for a practical guide on when and how to use uncertainty-aware approaches for social robot navigation in practice and what are the most effective criteria.",
      "published_utc": "2023-05-04T14:19:05Z",
      "updated_utc": "2023-07-17T18:35:46Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.02859v2",
      "abs_url": "http://arxiv.org/abs/2305.02859v2"
    },
    "2303.04320v2": {
      "arxiv_id": "2303.04320v2",
      "title": "SG-LSTM: Social Group LSTM for Robot Navigation Through Dense Crowds",
      "authors": [
        "Rashmi Bhaskara",
        "Maurice Chiu",
        "Aniket Bera"
      ],
      "summary": "With the increasing availability and affordability of personal robots, they will no longer be confined to large corporate warehouses or factories but will instead be expected to operate in less controlled environments alongside larger groups of people. In addition to ensuring safety and efficiency, it is crucial to minimize any negative psychological impact robots may have on humans and follow unwritten social norms in these situations. Our research aims to develop a model that can predict the movements of pedestrians and perceptually-social groups in crowded environments. We introduce a new Social Group Long Short-term Memory (SG-LSTM) model that models human groups and interactions in dense environments using a socially-aware LSTM to produce more accurate trajectory predictions. Our approach enables navigation algorithms to calculate collision-free paths faster and more accurately in crowded environments. Additionally, we also release a large video dataset with labeled pedestrian groups for the broader social navigation community. We show comparisons with different metrics on different datasets (ETH, Hotel, MOT15) and different prediction approaches (LIN, LSTM, O-LSTM, S-LSTM) as well as runtime performance.",
      "published_utc": "2023-03-08T01:38:20Z",
      "updated_utc": "2023-08-06T17:17:05Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.MA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.04320v2",
      "abs_url": "http://arxiv.org/abs/2303.04320v2"
    },
    "2303.01424v1": {
      "arxiv_id": "2303.01424v1",
      "title": "From Crowd Motion Prediction to Robot Navigation in Crowds",
      "authors": [
        "Sriyash Poddar",
        "Christoforos Mavrogiannis",
        "Siddhartha S. Srinivasa"
      ],
      "summary": "We focus on robot navigation in crowded environments. To navigate safely and efficiently within crowds, robots need models for crowd motion prediction. Building such models is hard due to the high dimensionality of multiagent domains and the challenge of collecting or simulating interaction-rich crowd-robot demonstrations. While there has been important progress on models for offline pedestrian motion forecasting, transferring their performance on real robots is nontrivial due to close interaction settings and novelty effects on users. In this paper, we investigate the utility of a recent state-of-the-art motion prediction model (S-GAN) for crowd navigation tasks. We incorporate this model into a model predictive controller (MPC) and deploy it on a self-balancing robot which we subject to a diverse range of crowd behaviors in the lab. We demonstrate that while S-GAN motion prediction accuracy transfers to the real world, its value is not reflected on navigation performance, measured with respect to safety and efficiency; in fact, the MPC performs indistinguishably even when using a simple constant-velocity prediction model, suggesting that substantial model improvements might be needed to yield significant gains for crowd navigation tasks. Footage from our experiments can be found at https://youtu.be/mzFiXg8KsZ0.",
      "published_utc": "2023-03-02T17:20:17Z",
      "updated_utc": "2023-03-02T17:20:17Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.01424v1",
      "abs_url": "http://arxiv.org/abs/2303.01424v1"
    },
    "2302.07583v1": {
      "arxiv_id": "2302.07583v1",
      "title": "ForceFormer: Exploring Social Force and Transformer for Pedestrian Trajectory Prediction",
      "authors": [
        "Weicheng Zhang",
        "Hao Cheng",
        "Fatema T. Johora",
        "Monika Sester"
      ],
      "summary": "Predicting trajectories of pedestrians based on goal information in highly interactive scenes is a crucial step toward Intelligent Transportation Systems and Autonomous Driving. The challenges of this task come from two key sources: (1) complex social interactions in high pedestrian density scenarios and (2) limited utilization of goal information to effectively associate with past motion information. To address these difficulties, we integrate social forces into a Transformer-based stochastic generative model backbone and propose a new goal-based trajectory predictor called ForceFormer. Differentiating from most prior works that simply use the destination position as an input feature, we leverage the driving force from the destination to efficiently simulate the guidance of a target on a pedestrian. Additionally, repulsive forces are used as another input feature to describe the avoidance action among neighboring pedestrians. Extensive experiments show that our proposed method achieves on-par performance measured by distance errors with the state-of-the-art models but evidently decreases collisions, especially in dense pedestrian scenarios on widely used pedestrian datasets.",
      "published_utc": "2023-02-15T10:54:14Z",
      "updated_utc": "2023-02-15T10:54:14Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.MA",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.07583v1",
      "abs_url": "http://arxiv.org/abs/2302.07583v1"
    },
    "2207.02281v1": {
      "arxiv_id": "2207.02281v1",
      "title": "BiPOCO: Bi-Directional Trajectory Prediction with Pose Constraints for Pedestrian Anomaly Detection",
      "authors": [
        "Asiegbu Miracle Kanu-Asiegbu",
        "Ram Vasudevan",
        "Xiaoxiao Du"
      ],
      "summary": "We present BiPOCO, a Bi-directional trajectory predictor with POse COnstraints, for detecting anomalous activities of pedestrians in videos. In contrast to prior work based on feature reconstruction, our work identifies pedestrian anomalous events by forecasting their future trajectories and comparing the predictions with their expectations. We introduce a set of novel compositional pose-based losses with our predictor and leverage prediction errors of each body joint for pedestrian anomaly detection. Experimental results show that our BiPOCO approach can detect pedestrian anomalous activities with a high detection rate (up to 87.0%) and incorporating pose constraints helps distinguish normal and anomalous poses in prediction. This work extends current literature of using prediction-based methods for anomaly detection and can benefit safety-critical applications such as autonomous driving and surveillance. Code is available at https://github.com/akanuasiegbu/BiPOCO.",
      "published_utc": "2022-07-05T19:45:49Z",
      "updated_utc": "2022-07-05T19:45:49Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.02281v1",
      "abs_url": "http://arxiv.org/abs/2207.02281v1"
    },
    "2207.02279v1": {
      "arxiv_id": "2207.02279v1",
      "title": "Leveraging Trajectory Prediction for Pedestrian Video Anomaly Detection",
      "authors": [
        "Asiegbu Miracle Kanu-Asiegbu",
        "Ram Vasudevan",
        "Xiaoxiao Du"
      ],
      "summary": "Video anomaly detection is a core problem in vision. Correctly detecting and identifying anomalous behaviors in pedestrians from video data will enable safety-critical applications such as surveillance, activity monitoring, and human-robot interaction. In this paper, we propose to leverage trajectory localization and prediction for unsupervised pedestrian anomaly event detection. Different than previous reconstruction-based approaches, our proposed framework rely on the prediction errors of normal and abnormal pedestrian trajectories to detect anomalies spatially and temporally. We present experimental results on real-world benchmark datasets on varying timescales and show that our proposed trajectory-predictor-based anomaly detection pipeline is effective and efficient at identifying anomalous activities of pedestrians in videos. Code will be made available at https://github.com/akanuasiegbu/Leveraging-Trajectory-Prediction-for-Pedestrian-Video-Anomaly-Detection.",
      "published_utc": "2022-07-05T19:44:34Z",
      "updated_utc": "2022-07-05T19:44:34Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.02279v1",
      "abs_url": "http://arxiv.org/abs/2207.02279v1"
    },
    "2206.13387v1": {
      "arxiv_id": "2206.13387v1",
      "title": "ScePT: Scene-consistent, Policy-based Trajectory Predictions for Planning",
      "authors": [
        "Yuxiao Chen",
        "Boris Ivanovic",
        "Marco Pavone"
      ],
      "summary": "Trajectory prediction is a critical functionality of autonomous systems that share environments with uncontrolled agents, one prominent example being self-driving vehicles. Currently, most prediction methods do not enforce scene consistency, i.e., there are a substantial amount of self-collisions between predicted trajectories of different agents in the scene. Moreover, many approaches generate individual trajectory predictions per agent instead of joint trajectory predictions of the whole scene, which makes downstream planning difficult. In this work, we present ScePT, a policy planning-based trajectory prediction model that generates accurate, scene-consistent trajectory predictions suitable for autonomous system motion planning. It explicitly enforces scene consistency and learns an agent interaction policy that can be used for conditional prediction. Experiments on multiple real-world pedestrians and autonomous vehicle datasets show that ScePT} matches current state-of-the-art prediction accuracy with significantly improved scene consistency. We also demonstrate ScePT's ability to work with a downstream contingency planner.",
      "published_utc": "2022-06-18T00:00:02Z",
      "updated_utc": "2022-06-18T00:00:02Z",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.13387v1",
      "abs_url": "http://arxiv.org/abs/2206.13387v1"
    },
    "2203.09293v1": {
      "arxiv_id": "2203.09293v1",
      "title": "PreTR: Spatio-Temporal Non-Autoregressive Trajectory Prediction Transformer",
      "authors": [
        "Lina Achaji",
        "Thierno Barry",
        "Thibault Fouqueray",
        "Julien Moreau",
        "Francois Aioun",
        "Francois Charpillet"
      ],
      "summary": "Nowadays, our mobility systems are evolving into the era of intelligent vehicles that aim to improve road safety. Due to their vulnerability, pedestrians are the users who will benefit the most from these developments. However, predicting their trajectory is one of the most challenging concerns. Indeed, accurate prediction requires a good understanding of multi-agent interactions that can be complex. Learning the underlying spatial and temporal patterns caused by these interactions is even more of a competitive and open problem that many researchers are tackling. In this paper, we introduce a model called PRediction Transformer (PReTR) that extracts features from the multi-agent scenes by employing a factorized spatio-temporal attention module. It shows less computational needs than previously studied models with empirically better results. Besides, previous works in motion prediction suffer from the exposure bias problem caused by generating future sequences conditioned on model prediction samples rather than ground-truth samples. In order to go beyond the proposed solutions, we leverage encoder-decoder Transformer networks for parallel decoding a set of learned object queries. This non-autoregressive solution avoids the need for iterative conditioning and arguably decreases training and testing computational time. We evaluate our model on the ETH/UCY datasets, a publicly available benchmark for pedestrian trajectory prediction. Finally, we justify our usage of the parallel decoding technique by showing that the trajectory prediction task can be better solved as a non-autoregressive task.",
      "published_utc": "2022-03-17T12:52:23Z",
      "updated_utc": "2022-03-17T12:52:23Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG",
        "cs.MA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2203.09293v1",
      "abs_url": "http://arxiv.org/abs/2203.09293v1"
    },
    "2202.03954v1": {
      "arxiv_id": "2202.03954v1",
      "title": "Social-DualCVAE: Multimodal Trajectory Forecasting Based on Social Interactions Pattern Aware and Dual Conditional Variational Auto-Encoder",
      "authors": [
        "Jiashi Gao",
        "Xinming Shi",
        "James J. Q. Yu"
      ],
      "summary": "Pedestrian trajectory forecasting is a fundamental task in multiple utility areas, such as self-driving, autonomous robots, and surveillance systems. The future trajectory forecasting is multi-modal, influenced by physical interaction with scene contexts and intricate social interactions among pedestrians. The mainly existing literature learns representations of social interactions by deep learning networks, while the explicit interaction patterns are not utilized. Different interaction patterns, such as following or collision avoiding, will generate different trends of next movement, thus, the awareness of social interaction patterns is important for trajectory forecasting. Moreover, the social interaction patterns are privacy concerned or lack of labels. To jointly address the above issues, we present a social-dual conditional variational auto-encoder (Social-DualCVAE) for multi-modal trajectory forecasting, which is based on a generative model conditioned not only on the past trajectories but also the unsupervised classification of interaction patterns. After generating the category distribution of the unlabeled social interaction patterns, DualCVAE, conditioned on the past trajectories and social interaction pattern, is proposed for multi-modal trajectory prediction by latent variables estimating. A variational bound is derived as the minimization objective during training. The proposed model is evaluated on widely used trajectory benchmarks and outperforms the prior state-of-the-art methods.",
      "published_utc": "2022-02-08T16:04:47Z",
      "updated_utc": "2022-02-08T16:04:47Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2202.03954v1",
      "abs_url": "http://arxiv.org/abs/2202.03954v1"
    },
    "2112.06624v1": {
      "arxiv_id": "2112.06624v1",
      "title": "Pedestrian Trajectory Prediction via Spatial Interaction Transformer Network",
      "authors": [
        "Tong Su",
        "Yu Meng",
        "Yan Xu"
      ],
      "summary": "As a core technology of the autonomous driving system, pedestrian trajectory prediction can significantly enhance the function of active vehicle safety and reduce road traffic injuries. In traffic scenes, when encountering with oncoming people, pedestrians may make sudden turns or stop immediately, which often leads to complicated trajectories. To predict such unpredictable trajectories, we can gain insights into the interaction between pedestrians. In this paper, we present a novel generative method named Spatial Interaction Transformer (SIT), which learns the spatio-temporal correlation of pedestrian trajectories through attention mechanisms. Furthermore, we introduce the conditional variational autoencoder (CVAE) framework to model the future latent motion states of pedestrians. In particular, the experiments based on large-scale trafc dataset nuScenes [2] show that SIT has an outstanding performance than state-of-the-art (SOTA) methods. Experimental evaluation on the challenging ETH and UCY datasets conrms the robustness of our proposed model",
      "published_utc": "2021-12-13T13:08:04Z",
      "updated_utc": "2021-12-13T13:08:04Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2112.06624v1",
      "abs_url": "http://arxiv.org/abs/2112.06624v1"
    },
    "2111.03822v1": {
      "arxiv_id": "2111.03822v1",
      "title": "Prediction of Pedestrian Spatiotemporal Risk Levels for Intelligent Vehicles: A Data-driven Approach",
      "authors": [
        "Zheyu Zhang",
        "Boyang Wang",
        "Chao Lu",
        "Jinghang Li",
        "Cheng Gong",
        "Jianwei Gong"
      ],
      "summary": "In recent years, road safety has attracted significant attention from researchers and practitioners in the intelligent transport systems domain. As one of the most common and vulnerable groups of road users, pedestrians cause great concerns due to their unpredictable behavior and movement, as subtle misunderstandings in vehicle-pedestrian interaction can easily lead to risky situations or collisions. Existing methods use either predefined collision-based models or human-labeling approaches to estimate the pedestrians' risks. These approaches are usually limited by their poor generalization ability and lack of consideration of interactions between the ego vehicle and a pedestrian. This work tackles the listed problems by proposing a Pedestrian Risk Level Prediction system. The system consists of three modules. Firstly, vehicle-perspective pedestrian data are collected. Since the data contains information regarding the movement of both the ego vehicle and pedestrian, it can simplify the prediction of spatiotemporal features in an interaction-aware fashion. Using the long short-term memory model, the pedestrian trajectory prediction module predicts their spatiotemporal features in the subsequent five frames. As the predicted trajectory follows certain interaction and risk patterns, a hybrid clustering and classification method is adopted to explore the risk patterns in the spatiotemporal features and train a risk level classifier using the learned patterns. Upon predicting the spatiotemporal features of pedestrians and identifying the corresponding risk level, the risk patterns between the ego vehicle and pedestrians are determined. Experimental results verified the capability of the PRLP system to predict the risk level of pedestrians, thus supporting the collision risk assessment of intelligent vehicles and providing safety warnings to both vehicles and pedestrians.",
      "published_utc": "2021-11-06T07:32:05Z",
      "updated_utc": "2021-11-06T07:32:05Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2111.03822v1",
      "abs_url": "http://arxiv.org/abs/2111.03822v1"
    },
    "2108.10879v2": {
      "arxiv_id": "2108.10879v2",
      "title": "Are socially-aware trajectory prediction models really socially-aware?",
      "authors": [
        "Saeed Saadatnejad",
        "Mohammadhossein Bahari",
        "Pedram Khorsandi",
        "Mohammad Saneian",
        "Seyed-Mohsen Moosavi-Dezfooli",
        "Alexandre Alahi"
      ],
      "summary": "Our field has recently witnessed an arms race of neural network-based trajectory predictors. While these predictors are at the core of many applications such as autonomous navigation or pedestrian flow simulations, their adversarial robustness has not been carefully studied. In this paper, we introduce a socially-attended attack to assess the social understanding of prediction models in terms of collision avoidance. An attack is a small yet carefully-crafted perturbations to fail predictors. Technically, we define collision as a failure mode of the output, and propose hard- and soft-attention mechanisms to guide our attack. Thanks to our attack, we shed light on the limitations of the current models in terms of their social understanding. We demonstrate the strengths of our method on the recent trajectory prediction models. Finally, we show that our attack can be employed to increase the social understanding of state-of-the-art models. The code is available online: https://s-attack.github.io/",
      "published_utc": "2021-08-24T17:59:09Z",
      "updated_utc": "2022-02-11T17:21:56Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2108.10879v2",
      "abs_url": "http://arxiv.org/abs/2108.10879v2"
    },
    "2108.08236v3": {
      "arxiv_id": "2108.08236v3",
      "title": "LOKI: Long Term and Key Intentions for Trajectory Prediction",
      "authors": [
        "Harshayu Girase",
        "Haiming Gang",
        "Srikanth Malla",
        "Jiachen Li",
        "Akira Kanehara",
        "Karttikeya Mangalam",
        "Chiho Choi"
      ],
      "summary": "Recent advances in trajectory prediction have shown that explicit reasoning about agents' intent is important to accurately forecast their motion. However, the current research activities are not directly applicable to intelligent and safety critical systems. This is mainly because very few public datasets are available, and they only consider pedestrian-specific intents for a short temporal horizon from a restricted egocentric view. To this end, we propose LOKI (LOng term and Key Intentions), a novel large-scale dataset that is designed to tackle joint trajectory and intention prediction for heterogeneous traffic agents (pedestrians and vehicles) in an autonomous driving setting. The LOKI dataset is created to discover several factors that may affect intention, including i) agent's own will, ii) social interactions, iii) environmental constraints, and iv) contextual information. We also propose a model that jointly performs trajectory and intention prediction, showing that recurrently reasoning about intention can assist with trajectory prediction. We show our method outperforms state-of-the-art trajectory prediction methods by upto $27\\%$ and also provide a baseline for frame-wise intention estimation.",
      "published_utc": "2021-08-18T16:57:03Z",
      "updated_utc": "2021-09-17T16:38:31Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MA",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2108.08236v3",
      "abs_url": "http://arxiv.org/abs/2108.08236v3"
    },
    "2107.11637v5": {
      "arxiv_id": "2107.11637v5",
      "title": "Group-based Motion Prediction for Navigation in Crowded Environments",
      "authors": [
        "Allan Wang",
        "Christoforos Mavrogiannis",
        "Aaron Steinfeld"
      ],
      "summary": "We focus on the problem of planning the motion of a robot in a dynamic multiagent environment such as a pedestrian scene. Enabling the robot to navigate safely and in a socially compliant fashion in such scenes requires a representation that accounts for the unfolding multiagent dynamics. Existing approaches to this problem tend to employ microscopic models of motion prediction that reason about the individual behavior of other agents. While such models may achieve high tracking accuracy in trajectory prediction benchmarks, they often lack an understanding of the group structures unfolding in crowded scenes. Inspired by the Gestalt theory from psychology, we build a Model Predictive Control framework (G-MPC) that leverages group-based prediction for robot motion planning. We conduct an extensive simulation study involving a series of challenging navigation tasks in scenes extracted from two real-world pedestrian datasets. We illustrate that G-MPC enables a robot to achieve statistically significantly higher safety and lower number of group intrusions than a series of baselines featuring individual pedestrian motion prediction models. Finally, we show that G-MPC can handle noisy lidar-scan estimates without significant performance losses.",
      "published_utc": "2021-07-24T15:51:43Z",
      "updated_utc": "2022-03-16T04:57:13Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2107.11637v5",
      "abs_url": "http://arxiv.org/abs/2107.11637v5"
    },
    "2106.12442v1": {
      "arxiv_id": "2106.12442v1",
      "title": "Euro-PVI: Pedestrian Vehicle Interactions in Dense Urban Centers",
      "authors": [
        "Apratim Bhattacharyya",
        "Daniel Olmeda Reino",
        "Mario Fritz",
        "Bernt Schiele"
      ],
      "summary": "Accurate prediction of pedestrian and bicyclist paths is integral to the development of reliable autonomous vehicles in dense urban environments. The interactions between vehicle and pedestrian or bicyclist have a significant impact on the trajectories of traffic participants e.g. stopping or turning to avoid collisions. Although recent datasets and trajectory prediction approaches have fostered the development of autonomous vehicles yet the amount of vehicle-pedestrian (bicyclist) interactions modeled are sparse. In this work, we propose Euro-PVI, a dataset of pedestrian and bicyclist trajectories. In particular, our dataset caters more diverse and complex interactions in dense urban scenarios compared to the existing datasets. To address the challenges in predicting future trajectories with dense interactions, we develop a joint inference model that learns an expressive multi-modal shared latent space across agents in the urban scene. This enables our Joint-$β$-cVAE approach to better model the distribution of future trajectories. We achieve state of the art results on the nuScenes and Euro-PVI datasets demonstrating the importance of capturing interactions between ego-vehicle and pedestrians (bicyclists) for accurate predictions.",
      "published_utc": "2021-06-22T15:40:21Z",
      "updated_utc": "2021-06-22T15:40:21Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2106.12442v1",
      "abs_url": "http://arxiv.org/abs/2106.12442v1"
    },
    "2011.10670v3": {
      "arxiv_id": "2011.10670v3",
      "title": "From Recognition to Prediction: Analysis of Human Action and Trajectory Prediction in Video",
      "authors": [
        "Junwei Liang"
      ],
      "summary": "With the advancement in computer vision deep learning, systems now are able to analyze an unprecedented amount of rich visual information from videos to enable applications such as autonomous driving, socially-aware robot assistant and public safety monitoring. Deciphering human behaviors to predict their future paths/trajectories and what they would do from videos is important in these applications. However, human trajectory prediction still remains a challenging task, as scene semantics and human intent are difficult to model. Many systems do not provide high-level semantic attributes to reason about pedestrian future. This design hinders prediction performance in video data from diverse domains and unseen scenarios. To enable optimal future human behavioral forecasting, it is crucial for the system to be able to detect and analyze human activities as well as scene semantics, passing informative features to the subsequent prediction module for context understanding.",
      "published_utc": "2020-11-20T22:23:34Z",
      "updated_utc": "2021-07-16T13:45:43Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2011.10670v3",
      "abs_url": "http://arxiv.org/abs/2011.10670v3"
    },
    "2010.12007v2": {
      "arxiv_id": "2010.12007v2",
      "title": "PRANK: motion Prediction based on RANKing",
      "authors": [
        "Yuriy Biktairov",
        "Maxim Stebelev",
        "Irina Rudenko",
        "Oleh Shliazhko",
        "Boris Yangel"
      ],
      "summary": "Predicting the motion of agents such as pedestrians or human-driven vehicles is one of the most critical problems in the autonomous driving domain. The overall safety of driving and the comfort of a passenger directly depend on its successful solution. The motion prediction problem also remains one of the most challenging problems in autonomous driving engineering, mainly due to high variance of the possible agent's future behavior given a situation. The two phenomena responsible for the said variance are the multimodality caused by the uncertainty of the agent's intent (e.g., turn right or move forward) and uncertainty in the realization of a given intent (e.g., which lane to turn into). To be useful within a real-time autonomous driving pipeline, a motion prediction system must provide efficient ways to describe and quantify this uncertainty, such as computing posterior modes and their probabilities or estimating density at the point corresponding to a given trajectory. It also should not put substantial density on physically impossible trajectories, as they can confuse the system processing the predictions. In this paper, we introduce the PRANK method, which satisfies these requirements. PRANK takes rasterized bird-eye images of agent's surroundings as an input and extracts features of the scene with a convolutional neural network. It then produces the conditional distribution of agent's trajectories plausible in the given scene. The key contribution of PRANK is a way to represent that distribution using nearest-neighbor methods in latent trajectory space, which allows for efficient inference in real time. We evaluate PRANK on the in-house and Argoverse datasets, where it shows competitive results.",
      "published_utc": "2020-10-22T19:58:02Z",
      "updated_utc": "2021-06-15T09:39:33Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2010.12007v2",
      "abs_url": "http://arxiv.org/abs/2010.12007v2"
    },
    "2009.10468v2": {
      "arxiv_id": "2009.10468v2",
      "title": "Spatial-Temporal Block and LSTM Network for Pedestrian Trajectories Prediction",
      "authors": [
        "Xiong Dan"
      ],
      "summary": "Pedestrian trajectory prediction is a critical to avoid autonomous driving collision. But this prediction is a challenging problem due to social forces and cluttered scenes. Such human-human and human-space interactions lead to many socially plausible trajectories. In this paper, we propose a novel LSTM-based algorithm. We tackle the problem by considering the static scene and pedestrian which combine the Graph Convolutional Networks and Temporal Convolutional Networks to extract features from pedestrians. Each pedestrian in the scene is regarded as a node, and we can obtain the relationship between each node and its neighborhoods by graph embedding. It is LSTM that encode the relationship so that our model predicts nodes trajectories in crowd scenarios simultaneously. To effectively predict multiple possible future trajectories, we further introduce Spatio-Temporal Convolutional Block to make the network flexible. Experimental results on two public datasets, i.e. ETH and UCY, demonstrate the effectiveness of our proposed ST-Block and we achieve state-of-the-art approaches in human trajectory prediction.",
      "published_utc": "2020-09-22T11:43:40Z",
      "updated_utc": "2020-09-23T07:51:39Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2009.10468v2",
      "abs_url": "http://arxiv.org/abs/2009.10468v2"
    },
    "2007.14558v2": {
      "arxiv_id": "2007.14558v2",
      "title": "BiTraP: Bi-directional Pedestrian Trajectory Prediction with Multi-modal Goal Estimation",
      "authors": [
        "Yu Yao",
        "Ella Atkins",
        "Matthew Johnson-Roberson",
        "Ram Vasudevan",
        "Xiaoxiao Du"
      ],
      "summary": "Pedestrian trajectory prediction is an essential task in robotic applications such as autonomous driving and robot navigation. State-of-the-art trajectory predictors use a conditional variational autoencoder (CVAE) with recurrent neural networks (RNNs) to encode observed trajectories and decode multi-modal future trajectories. This process can suffer from accumulated errors over long prediction horizons (>=2 seconds). This paper presents BiTraP, a goal-conditioned bi-directional multi-modal trajectory prediction method based on the CVAE. BiTraP estimates the goal (end-point) of trajectories and introduces a novel bi-directional decoder to improve longer-term trajectory prediction accuracy. Extensive experiments show that BiTraP generalizes to both first-person view (FPV) and bird's-eye view (BEV) scenarios and outperforms state-of-the-art results by ~10-50%. We also show that different choices of non-parametric versus parametric target models in the CVAE directly influence the predicted multi-modal trajectory distributions. These results provide guidance on trajectory predictor design for robotic applications such as collision avoidance and navigation systems.",
      "published_utc": "2020-07-29T02:40:17Z",
      "updated_utc": "2020-11-16T17:30:24Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2007.14558v2",
      "abs_url": "http://arxiv.org/abs/2007.14558v2"
    },
    "2003.06594v1": {
      "arxiv_id": "2003.06594v1",
      "title": "Collaborative Motion Prediction via Neural Motion Message Passing",
      "authors": [
        "Yue Hu",
        "Siheng Chen",
        "Ya Zhang",
        "Xiao Gu"
      ],
      "summary": "Motion prediction is essential and challenging for autonomous vehicles and social robots. One challenge of motion prediction is to model the interaction among traffic actors, which could cooperate with each other to avoid collisions or form groups. To address this challenge, we propose neural motion message passing (NMMP) to explicitly model the interaction and learn representations for directed interactions between actors. Based on the proposed NMMP, we design the motion prediction systems for two settings: the pedestrian setting and the joint pedestrian and vehicle setting. Both systems share a common pattern: we use an individual branch to model the behavior of a single actor and an interactive branch to model the interaction between actors, while with different wrappers to handle the varied input formats and characteristics. The experimental results show that both systems outperform the previous state-of-the-art methods on several existing benchmarks. Besides, we provide interpretability for interaction learning.",
      "published_utc": "2020-03-14T10:12:54Z",
      "updated_utc": "2020-03-14T10:12:54Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2003.06594v1",
      "abs_url": "http://arxiv.org/abs/2003.06594v1"
    },
    "2002.03038v1": {
      "arxiv_id": "2002.03038v1",
      "title": "DenseCAvoid: Real-time Navigation in Dense Crowds using Anticipatory Behaviors",
      "authors": [
        "Adarsh Jagan Sathyamoorthy",
        "Jing Liang",
        "Utsav Patel",
        "Tianrui Guan",
        "Rohan Chandra",
        "Dinesh Manocha"
      ],
      "summary": "We present DenseCAvoid, a novel navigation algorithm for navigating a robot through dense crowds and avoiding collisions by anticipating pedestrian behaviors. Our formulation uses visual sensors and a pedestrian trajectory prediction algorithm to track pedestrians in a set of input frames and provide bounding boxes that extrapolate the pedestrian positions in a future time. Our hybrid approach combines this trajectory prediction with a Deep Reinforcement Learning-based collision avoidance method to train a policy to generate smoother, safer, and more robust trajectories during run-time. We train our policy in realistic 3-D simulations of static and dynamic scenarios with multiple pedestrians. In practice, our hybrid approach generalizes well to unseen, real-world scenarios and can navigate a robot through dense crowds (~1-2 humans per square meter) in indoor scenarios, including narrow corridors and lobbies. As compared to cases where prediction was not used, we observe that our method reduces the occurrence of the robot freezing in a crowd by up to 48%, and performs comparably with respect to trajectory lengths and mean arrival times to goal.",
      "published_utc": "2020-02-07T22:46:21Z",
      "updated_utc": "2020-02-07T22:46:21Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2002.03038v1",
      "abs_url": "http://arxiv.org/abs/2002.03038v1"
    },
    "2001.11597v1": {
      "arxiv_id": "2001.11597v1",
      "title": "Path Planning in Dynamic Environments using Generative RNNs and Monte Carlo Tree Search",
      "authors": [
        "Stuart Eiffert",
        "He Kong",
        "Navid Pirmarzdashti",
        "Salah Sukkarieh"
      ],
      "summary": "State of the art methods for robotic path planning in dynamic environments, such as crowds or traffic, rely on hand crafted motion models for agents. These models often do not reflect interactions of agents in real world scenarios. To overcome this limitation, this paper proposes an integrated path planning framework using generative Recurrent Neural Networks within a Monte Carlo Tree Search (MCTS). This approach uses a learnt model of social response to predict crowd dynamics during planning across the action space. This extends our recent work using generative RNNs to learn the relationship between planned robotic actions and the likely response of a crowd. We show that the proposed framework can considerably improve motion prediction accuracy during interactions, allowing more effective path planning. The performance of our method is compared in simulation with existing methods for collision avoidance in a crowd of pedestrians, demonstrating the ability to control future states of nearby individuals. We also conduct preliminary real world tests to validate the effectiveness of our method.",
      "published_utc": "2020-01-30T22:46:37Z",
      "updated_utc": "2020-01-30T22:46:37Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2001.11597v1",
      "abs_url": "http://arxiv.org/abs/2001.11597v1"
    },
    "1910.06673v1": {
      "arxiv_id": "1910.06673v1",
      "title": "SafeCritic: Collision-Aware Trajectory Prediction",
      "authors": [
        "Tessa van der Heiden",
        "Naveen Shankar Nagaraja",
        "Christian Weiss",
        "Efstratios Gavves"
      ],
      "summary": "Navigating complex urban environments safely is a key to realize fully autonomous systems. Predicting future locations of vulnerable road users, such as pedestrians and cyclists, thus, has received a lot of attention in the recent years. While previous works have addressed modeling interactions with the static (obstacles) and dynamic (humans) environment agents, we address an important gap in trajectory prediction. We propose SafeCritic, a model that synergizes generative adversarial networks for generating multiple \"real\" trajectories with reinforcement learning to generate \"safe\" trajectories. The Discriminator evaluates the generated candidates on whether they are consistent with the observed inputs. The Critic network is environmentally aware to prune trajectories that are in collision or are in violation with the environment. The auto-encoding loss stabilizes training and prevents mode-collapse. We demonstrate results on two large scale data sets with a considerable improvement over state-of-the-art. We also show that the Critic is able to classify the safety of trajectories.",
      "published_utc": "2019-10-15T12:15:19Z",
      "updated_utc": "2019-10-15T12:15:19Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CV",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/1910.06673v1",
      "abs_url": "http://arxiv.org/abs/1910.06673v1"
    },
    "1907.01577v2": {
      "arxiv_id": "1907.01577v2",
      "title": "SVM Enhanced Frenet Frame Planner For Safe Navigation Amidst Moving Agents",
      "authors": [
        "Unni Krishnan R Nair",
        "Nivedita Rufus",
        "Vashist Madiraju",
        "K Madhava Krishna"
      ],
      "summary": "This paper proposes an SVM Enhanced Trajectory Planner for dynamic scenes, typically those encountered in on road settings. Frenet frame based trajectory generation is popular in the context of autonomous driving both in research and industry. We incorporate a safety based maximal margin criteria using a SVM layer that generates control points that are maximally separated from all dynamic obstacles in the scene. A kinematically consistent trajectory generator then computes a path through these waypoints. We showcase through simulations as well as real world experiments on a self driving car that the SVM enhanced planner provides for a larger offset with dynamic obstacles than the regular Frenet frame based trajectory generation. Thereby, the authors argue that such a formulation is inherently suited for navigation amongst pedestrians. We assume the availability of an intent or trajectory prediction module that predicts the future trajectories of all dynamic actors in the scene.",
      "published_utc": "2019-07-02T18:22:53Z",
      "updated_utc": "2020-09-11T06:42:00Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/1907.01577v2",
      "abs_url": "http://arxiv.org/abs/1907.01577v2"
    },
    "1906.08469v2": {
      "arxiv_id": "1906.08469v2",
      "title": "Predicting Motion of Vulnerable Road Users using High-Definition Maps and Efficient ConvNets",
      "authors": [
        "Fang-Chieh Chou",
        "Tsung-Han Lin",
        "Henggang Cui",
        "Vladan Radosavljevic",
        "Thi Nguyen",
        "Tzu-Kuo Huang",
        "Matthew Niedoba",
        "Jeff Schneider",
        "Nemanja Djuric"
      ],
      "summary": "Following detection and tracking of traffic actors, prediction of their future motion is the next critical component of a self-driving vehicle (SDV) technology, allowing the SDV to operate safely and efficiently in its environment. This is particularly important when it comes to vulnerable road users (VRUs), such as pedestrians and bicyclists. These actors need to be handled with special care due to an increased risk of injury, as well as the fact that their behavior is less predictable than that of motorized actors. To address this issue, in the current study we present a deep learning-based method for predicting VRU movement, where we rasterize high-definition maps and actor's surroundings into a bird's-eye view image used as an input to deep convolutional networks. In addition, we propose a fast architecture suitable for real-time inference, and perform an ablation study of various rasterization approaches to find the optimal choice for accurate prediction. The results strongly indicate benefits of using the proposed approach for motion prediction of VRUs, both in terms of accuracy and latency.",
      "published_utc": "2019-06-20T07:16:16Z",
      "updated_utc": "2020-06-11T06:54:12Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.LG",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/1906.08469v2",
      "abs_url": "http://arxiv.org/abs/1906.08469v2"
    },
    "1903.01860v1": {
      "arxiv_id": "1903.01860v1",
      "title": "Stochastic Sampling Simulation for Pedestrian Trajectory Prediction",
      "authors": [
        "Cyrus Anderson",
        "Xiaoxiao Du",
        "Ram Vasudevan",
        "Matthew Johnson-Roberson"
      ],
      "summary": "Urban environments pose a significant challenge for autonomous vehicles (AVs) as they must safely navigate while in close proximity to many pedestrians. It is crucial for the AV to correctly understand and predict the future trajectories of pedestrians to avoid collision and plan a safe path. Deep neural networks (DNNs) have shown promising results in accurately predicting pedestrian trajectories, relying on large amounts of annotated real-world data to learn pedestrian behavior. However, collecting and annotating these large real-world pedestrian datasets is costly in both time and labor. This paper describes a novel method using a stochastic sampling-based simulation to train DNNs for pedestrian trajectory prediction with social interaction. Our novel simulation method can generate vast amounts of automatically-annotated, realistic, and naturalistic synthetic pedestrian trajectories based on small amounts of real annotation. We then use such synthetic trajectories to train an off-the-shelf state-of-the-art deep learning approach Social GAN (Generative Adversarial Network) to perform pedestrian trajectory prediction. Our proposed architecture, trained only using synthetic trajectories, achieves better prediction results compared to those trained on human-annotated real-world data using the same network. Our work demonstrates the effectiveness and potential of using simulation as a substitution for human annotation efforts to train high-performing prediction algorithms such as the DNNs.",
      "published_utc": "2019-03-05T14:39:04Z",
      "updated_utc": "2019-03-05T14:39:04Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/1903.01860v1",
      "abs_url": "http://arxiv.org/abs/1903.01860v1"
    },
    "1902.05437v1": {
      "arxiv_id": "1902.05437v1",
      "title": "Situation-Aware Pedestrian Trajectory Prediction with Spatio-Temporal Attention Model",
      "authors": [
        "Sirin Haddad",
        "Meiqing Wu",
        "He Wei",
        "Siew Kei Lam"
      ],
      "summary": "Pedestrian trajectory prediction is essential for collision avoidance in autonomous driving and robot navigation. However, predicting a pedestrian's trajectory in crowded environments is non-trivial as it is influenced by other pedestrians' motion and static structures that are present in the scene. Such human-human and human-space interactions lead to non-linearities in the trajectories. In this paper, we present a new spatio-temporal graph based Long Short-Term Memory (LSTM) network for predicting pedestrian trajectory in crowded environments, which takes into account the interaction with static (physical objects) and dynamic (other pedestrians) elements in the scene. Our results are based on two widely-used datasets to demonstrate that the proposed method outperforms the state-of-the-art approaches in human trajectory prediction. In particular, our method leads to a reduction in Average Displacement Error (ADE) and Final Displacement Error (FDE) of up to 55% and 61% respectively over state-of-the-art approaches.",
      "published_utc": "2019-02-13T17:57:50Z",
      "updated_utc": "2019-02-13T17:57:50Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/1902.05437v1",
      "abs_url": "http://arxiv.org/abs/1902.05437v1"
    },
    "1706.05904v2": {
      "arxiv_id": "1706.05904v2",
      "title": "Pedestrian Prediction by Planning using Deep Neural Networks",
      "authors": [
        "Eike Rehder",
        "Florian Wirth",
        "Martin Lauer",
        "Christoph Stiller"
      ],
      "summary": "Accurate traffic participant prediction is the prerequisite for collision avoidance of autonomous vehicles. In this work, we predict pedestrians by emulating their own motion planning. From online observations, we infer a mixture density function for possible destinations. We use this result as the goal states of a planning stage that performs motion prediction based on common behavior patterns. The entire system is modeled as one monolithic neural network and trained via inverse reinforcement learning. Experimental validation on real world data shows the system's ability to predict both, destinations and trajectories accurately.",
      "published_utc": "2017-06-19T12:40:30Z",
      "updated_utc": "2017-06-20T07:25:49Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/1706.05904v2",
      "abs_url": "http://arxiv.org/abs/1706.05904v2"
    },
    "2601.10554v1": {
      "arxiv_id": "2601.10554v1",
      "title": "DeepUrban: Interaction-Aware Trajectory Prediction and Planning for Automated Driving by Aerial Imagery",
      "authors": [
        "Constantin Selzer",
        "Fabian B. Flohr"
      ],
      "summary": "The efficacy of autonomous driving systems hinges critically on robust prediction and planning capabilities. However, current benchmarks are impeded by a notable scarcity of scenarios featuring dense traffic, which is essential for understanding and modeling complex interactions among road users. To address this gap, we collaborated with our industrial partner, DeepScenario, to develop DeepUrban-a new drone dataset designed to enhance trajectory prediction and planning benchmarks focusing on dense urban settings. DeepUrban provides a rich collection of 3D traffic objects, extracted from high-resolution images captured over urban intersections at approximately 100 meters altitude. The dataset is further enriched with comprehensive map and scene information to support advanced modeling and simulation tasks. We evaluate state-of-the-art (SOTA) prediction and planning methods, and conducted experiments on generalization capabilities. Our findings demonstrate that adding DeepUrban to nuScenes can boost the accuracy of vehicle predictions and planning, achieving improvements up to 44.1 % / 44.3% on the ADE / FDE metrics. Website: https://iv.ee.hm.edu/deepurban",
      "published_utc": "2026-01-15T16:18:42Z",
      "updated_utc": "2026-01-15T16:18:42Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2601.10554v1",
      "abs_url": "http://arxiv.org/abs/2601.10554v1"
    },
    "2601.09856v1": {
      "arxiv_id": "2601.09856v1",
      "title": "How Human Motion Prediction Quality Shapes Social Robot Navigation Performance in Constrained Spaces",
      "authors": [
        "Andrew Stratton",
        "Phani Teja Singamaneni",
        "Pranav Goyal",
        "Rachid Alami",
        "Christoforos Mavrogiannis"
      ],
      "summary": "Motivated by the vision of integrating mobile robots closer to humans in warehouses, hospitals, manufacturing plants, and the home, we focus on robot navigation in dynamic and spatially constrained environments. Ensuring human safety, comfort, and efficiency in such settings requires that robots are endowed with a model of how humans move around them. Human motion prediction around robots is especially challenging due to the stochasticity of human behavior, differences in user preferences, and data scarcity. In this work, we perform a methodical investigation of the effects of human motion prediction quality on robot navigation performance, as well as human productivity and impressions. We design a scenario involving robot navigation among two human subjects in a constrained workspace and instantiate it in a user study ($N=80$) involving two different robot platforms, conducted across two sites from different world regions. Key findings include evidence that: 1) the widely adopted average displacement error is not a reliable predictor of robot navigation performance and human impressions; 2) the common assumption of human cooperation breaks down in constrained environments, with users often not reciprocating robot cooperation, and causing performance degradations; 3) more efficient robot navigation often comes at the expense of human efficiency and comfort.",
      "published_utc": "2026-01-14T20:34:34Z",
      "updated_utc": "2026-01-14T20:34:34Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2601.09856v1",
      "abs_url": "http://arxiv.org/abs/2601.09856v1"
    },
    "2601.09377v1": {
      "arxiv_id": "2601.09377v1",
      "title": "ReflexDiffusion: Reflection-Enhanced Trajectory Planning for High-lateral-acceleration Scenarios in Autonomous Driving",
      "authors": [
        "Xuemei Yao",
        "Xiao Yang",
        "Jianbin Sun",
        "Liuwei Xie",
        "Xuebin Shao",
        "Xiyu Fang",
        "Hang Su",
        "Kewei Yang"
      ],
      "summary": "Generating safe and reliable trajectories for autonomous vehicles in long-tail scenarios remains a significant challenge, particularly for high-lateral-acceleration maneuvers such as sharp turns, which represent critical safety situations. Existing trajectory planners exhibit systematic failures in these scenarios due to data imbalance. This results in insufficient modelling of vehicle dynamics, road geometry, and environmental constraints in high-risk situations, leading to suboptimal or unsafe trajectory prediction when vehicles operate near their physical limits. In this paper, we introduce ReflexDiffusion, a novel inference-stage framework that enhances diffusion-based trajectory planners through reflective adjustment. Our method introduces a gradient-based adjustment mechanism during the iterative denoising process: after each standard trajectory update, we compute the gradient between the conditional and unconditional noise predictions to explicitly amplify critical conditioning signals, including road curvature and lateral vehicle dynamics. This amplification enforces strict adherence to physical constraints, particularly improving stability during high-lateral-acceleration maneuvers where precise vehicle-road interaction is paramount. Evaluated on the nuPlan Test14-hard benchmark, ReflexDiffusion achieves a 14.1% improvement in driving score for high-lateral-acceleration scenarios over the state-of-the-art (SOTA) methods. This demonstrates that inference-time trajectory optimization can effectively compensate for training data sparsity by dynamically reinforcing safety-critical constraints near handling limits. The framework's architecture-agnostic design enables direct deployment to existing diffusion-based planners, offering a practical solution for improving autonomous vehicle safety in challenging driving conditions.",
      "published_utc": "2026-01-14T11:03:29Z",
      "updated_utc": "2026-01-14T11:03:29Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2601.09377v1",
      "abs_url": "http://arxiv.org/abs/2601.09377v1"
    },
    "2601.10521v1": {
      "arxiv_id": "2601.10521v1",
      "title": "BikeActions: An Open Platform and Benchmark for Cyclist-Centric VRU Action Recognition",
      "authors": [
        "Max A. Buettner",
        "Kanak Mazumder",
        "Luca Koecher",
        "Mario Finkbeiner",
        "Sebastian Niebler",
        "Fabian B. Flohr"
      ],
      "summary": "Anticipating the intentions of Vulnerable Road Users (VRUs) is a critical challenge for safe autonomous driving (AD) and mobile robotics. While current research predominantly focuses on pedestrian crossing behaviors from a vehicle's perspective, interactions within dense shared spaces remain underexplored. To bridge this gap, we introduce FUSE-Bike, the first fully open perception platform of its kind. Equipped with two LiDARs, a camera, and GNSS, it facilitates high-fidelity, close-range data capture directly from a cyclist's viewpoint. Leveraging this platform, we present BikeActions, a novel multi-modal dataset comprising 852 annotated samples across 5 distinct action classes, specifically tailored to improve VRU behavior modeling. We establish a rigorous benchmark by evaluating state-of-the-art graph convolution and transformer-based models on our publicly released data splits, establishing the first performance baselines for this challenging task. We release the full dataset together with data curation tools, the open hardware design, and the benchmark code to foster future research in VRU action understanding under https://iv.ee.hm.edu/bikeactions/.",
      "published_utc": "2026-01-15T15:47:46Z",
      "updated_utc": "2026-01-15T15:47:46Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2601.10521v1",
      "abs_url": "http://arxiv.org/abs/2601.10521v1"
    },
    "2601.10233v1": {
      "arxiv_id": "2601.10233v1",
      "title": "Proactive Local-Minima-Free Robot Navigation: Blending Motion Prediction with Safe Control",
      "authors": [
        "Yifan Xue",
        "Ze Zhang",
        "Knut Åkesson",
        "Nadia Figueroa"
      ],
      "summary": "This work addresses the challenge of safe and efficient mobile robot navigation in complex dynamic environments with concave moving obstacles. Reactive safe controllers like Control Barrier Functions (CBFs) design obstacle avoidance strategies based only on the current states of the obstacles, risking future collisions. To alleviate this problem, we use Gaussian processes to learn barrier functions online from multimodal motion predictions of obstacles generated by neural networks trained with energy-based learning. The learned barrier functions are then fed into quadratic programs using modulated CBFs (MCBFs), a local-minimum-free version of CBFs, to achieve safe and efficient navigation. The proposed framework makes two key contributions. First, it develops a prediction-to-barrier function online learning pipeline. Second, it introduces an autonomous parameter tuning algorithm that adapts MCBFs to deforming, prediction-based barrier functions. The framework is evaluated in both simulations and real-world experiments, consistently outperforming baselines and demonstrating superior safety and efficiency in crowded dynamic environments.",
      "published_utc": "2026-01-15T09:46:03Z",
      "updated_utc": "2026-01-15T09:46:03Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2601.10233v1",
      "abs_url": "http://arxiv.org/abs/2601.10233v1"
    },
    "2511.11164v2": {
      "arxiv_id": "2511.11164v2",
      "title": "Reverberation: Learning the Latencies Before Forecasting Trajectories",
      "authors": [
        "Conghao Wong",
        "Ziqian Zou",
        "Beihao Xia",
        "Xinge You"
      ],
      "summary": "Bridging the past to the future, connecting agents both spatially and temporally, lies at the core of the trajectory prediction task. Despite great efforts, it remains challenging to explicitly learn and predict latencies, i.e., response intervals or temporal delays with which agents respond to various trajectory-changing events and adjust their future paths, whether on their own or interactively. Different agents may exhibit distinct latency preferences for noticing, processing, and reacting to a specific trajectory-changing event. The lack of consideration of such latencies may undermine the causal continuity of forecasting systems, leading to implausible or unintended trajectories. Inspired by reverberation in acoustics, we propose a new reverberation transform and the corresponding Reverberation (short for Rev) trajectory prediction model, which predicts both individual latency preferences and their stochastic variations accordingly, by using two explicit and learnable reverberation kernels, enabling latency-conditioned and controllable trajectory prediction of both non-interactive and social latencies. Experiments on multiple datasets, whether pedestrians or vehicles, demonstrate that Rev achieves competitive accuracy while revealing interpretable latency dynamics across agents and scenarios. Qualitative analyses further verify the properties of the reverberation transform, highlighting its potential as a general latency modeling approach.",
      "published_utc": "2025-11-14T10:59:24Z",
      "updated_utc": "2025-11-28T03:39:46Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2511.11164v2",
      "abs_url": "http://arxiv.org/abs/2511.11164v2"
    },
    "2510.17731v1": {
      "arxiv_id": "2510.17731v1",
      "title": "Can Image-To-Video Models Simulate Pedestrian Dynamics?",
      "authors": [
        "Aaron Appelle",
        "Jerome P. Lynch"
      ],
      "summary": "Recent high-performing image-to-video (I2V) models based on variants of the diffusion transformer (DiT) have displayed remarkable inherent world-modeling capabilities by virtue of training on large scale video datasets. We investigate whether these models can generate realistic pedestrian movement patterns in crowded public scenes. Our framework conditions I2V models on keyframes extracted from pedestrian trajectory benchmarks, then evaluates their trajectory prediction performance using quantitative measures of pedestrian dynamics.",
      "published_utc": "2025-10-20T16:44:40Z",
      "updated_utc": "2025-10-20T16:44:40Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2510.17731v1",
      "abs_url": "http://arxiv.org/abs/2510.17731v1"
    },
    "2510.02469v1": {
      "arxiv_id": "2510.02469v1",
      "title": "SIMSplat: Predictive Driving Scene Editing with Language-aligned 4D Gaussian Splatting",
      "authors": [
        "Sung-Yeon Park",
        "Adam Lee",
        "Juanwu Lu",
        "Can Cui",
        "Luyang Jiang",
        "Rohit Gupta",
        "Kyungtae Han",
        "Ahmadreza Moradipari",
        "Ziran Wang"
      ],
      "summary": "Driving scene manipulation with sensor data is emerging as a promising alternative to traditional virtual driving simulators. However, existing frameworks struggle to generate realistic scenarios efficiently due to limited editing capabilities. To address these challenges, we present SIMSplat, a predictive driving scene editor with language-aligned Gaussian splatting. As a language-controlled editor, SIMSplat enables intuitive manipulation using natural language prompts. By aligning language with Gaussian-reconstructed scenes, it further supports direct querying of road objects, allowing precise and flexible editing. Our method provides detailed object-level editing, including adding new objects and modifying the trajectories of both vehicles and pedestrians, while also incorporating predictive path refinement through multi-agent motion prediction to generate realistic interactions among all agents in the scene. Experiments on the Waymo dataset demonstrate SIMSplat's extensive editing capabilities and adaptability across a wide range of scenarios. Project page: https://sungyeonparkk.github.io/simsplat/",
      "published_utc": "2025-10-02T18:22:03Z",
      "updated_utc": "2025-10-02T18:22:03Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2510.02469v1",
      "abs_url": "http://arxiv.org/abs/2510.02469v1"
    },
    "2509.24020v1": {
      "arxiv_id": "2509.24020v1",
      "title": "Hazy Pedestrian Trajectory Prediction via Physical Priors and Graph-Mamba",
      "authors": [
        "Jian Chen",
        "Zhuoran Zheng",
        "Han Hu",
        "Guijuan Zhang",
        "Dianjie Lu",
        "Liang Li",
        "Chen Lyu"
      ],
      "summary": "To address the issues of physical information degradation and ineffective pedestrian interaction modeling in pedestrian trajectory prediction under hazy weather conditions, we propose a deep learning model that combines physical priors of atmospheric scattering with topological modeling of pedestrian relationships. Specifically, we first construct a differentiable atmospheric scattering model that decouples haze concentration from light degradation through a network with physical parameter estimation, enabling the learning of haze-mitigated feature representations. Second, we design an adaptive scanning state space model for feature extraction. Our adaptive Mamba variant achieves a 78% inference speed increase over native Mamba while preserving long-range dependency modeling. Finally, to efficiently model pedestrian relationships, we develop a heterogeneous graph attention network, using graph matrices to model multi-granularity interactions between pedestrians and groups, combined with a spatio-temporal fusion module to capture the collaborative evolution patterns of pedestrian movements. Furthermore, we constructed a new pedestrian trajectory prediction dataset based on ETH/UCY to evaluate the effectiveness of the proposed method. Experiments show that our method reduces the minADE / minFDE metrics by 37.2% and 41.5%, respectively, compared to the SOTA models in dense haze scenarios (visibility < 30m), providing a new modeling paradigm for reliable perception in intelligent transportation systems in adverse environments.",
      "published_utc": "2025-09-28T18:29:43Z",
      "updated_utc": "2025-09-28T18:29:43Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2509.24020v1",
      "abs_url": "http://arxiv.org/abs/2509.24020v1"
    },
    "2508.07146v1": {
      "arxiv_id": "2508.07146v1",
      "title": "Intention-Aware Diffusion Model for Pedestrian Trajectory Prediction",
      "authors": [
        "Yu Liu",
        "Zhijie Liu",
        "Xiao Ren",
        "You-Fu Li",
        "He Kong"
      ],
      "summary": "Predicting pedestrian motion trajectories is critical for the path planning and motion control of autonomous vehicles. Recent diffusion-based models have shown promising results in capturing the inherent stochasticity of pedestrian behavior for trajectory prediction. However, the absence of explicit semantic modelling of pedestrian intent in many diffusion-based methods may result in misinterpreted behaviors and reduced prediction accuracy. To address the above challenges, we propose a diffusion-based pedestrian trajectory prediction framework that incorporates both short-term and long-term motion intentions. Short-term intent is modelled using a residual polar representation, which decouples direction and magnitude to capture fine-grained local motion patterns. Long-term intent is estimated through a learnable, token-based endpoint predictor that generates multiple candidate goals with associated probabilities, enabling multimodal and context-aware intention modelling. Furthermore, we enhance the diffusion process by incorporating adaptive guidance and a residual noise predictor that dynamically refines denoising accuracy. The proposed framework is evaluated on the widely used ETH, UCY, and SDD benchmarks, demonstrating competitive results against state-of-the-art methods.",
      "published_utc": "2025-08-10T02:36:33Z",
      "updated_utc": "2025-08-10T02:36:33Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2508.07146v1",
      "abs_url": "http://arxiv.org/abs/2508.07146v1"
    },
    "2508.04229v1": {
      "arxiv_id": "2508.04229v1",
      "title": "Intention Enhanced Diffusion Model for Multimodal Pedestrian Trajectory Prediction",
      "authors": [
        "Yu Liu",
        "Zhijie Liu",
        "Xiao Ren",
        "You-Fu Li",
        "He Kong"
      ],
      "summary": "Predicting pedestrian motion trajectories is critical for path planning and motion control of autonomous vehicles. However, accurately forecasting crowd trajectories remains a challenging task due to the inherently multimodal and uncertain nature of human motion. Recent diffusion-based models have shown promising results in capturing the stochasticity of pedestrian behavior for trajectory prediction. However, few diffusion-based approaches explicitly incorporate the underlying motion intentions of pedestrians, which can limit the interpretability and precision of prediction models. In this work, we propose a diffusion-based multimodal trajectory prediction model that incorporates pedestrians' motion intentions into the prediction framework. The motion intentions are decomposed into lateral and longitudinal components, and a pedestrian intention recognition module is introduced to enable the model to effectively capture these intentions. Furthermore, we adopt an efficient guidance mechanism that facilitates the generation of interpretable trajectories. The proposed framework is evaluated on two widely used human trajectory prediction benchmarks, ETH and UCY, on which it is compared against state-of-the-art methods. The experimental results demonstrate that our method achieves competitive performance.",
      "published_utc": "2025-08-06T09:04:54Z",
      "updated_utc": "2025-08-06T09:04:54Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2508.04229v1",
      "abs_url": "http://arxiv.org/abs/2508.04229v1"
    },
    "2508.03541v1": {
      "arxiv_id": "2508.03541v1",
      "title": "Vision-based Perception System for Automated Delivery Robot-Pedestrians Interactions",
      "authors": [
        "Ergi Tushe",
        "Bilal Farooq"
      ],
      "summary": "The integration of Automated Delivery Robots (ADRs) into pedestrian-heavy urban spaces introduces unique challenges in terms of safe, efficient, and socially acceptable navigation. We develop the complete pipeline for a single vision sensor based multi-pedestrian detection and tracking, pose estimation, and monocular depth perception. Leveraging the real-world MOT17 dataset sequences, this study demonstrates how integrating human-pose estimation and depth cues enhances pedestrian trajectory prediction and identity maintenance, even under occlusions and dense crowds. Results show measurable improvements, including up to a 10% increase in identity preservation (IDF1), a 7% improvement in multiobject tracking accuracy (MOTA), and consistently high detection precision exceeding 85%, even in challenging scenarios. Notably, the system identifies vulnerable pedestrian groups supporting more socially aware and inclusive robot behaviour.",
      "published_utc": "2025-08-05T15:10:09Z",
      "updated_utc": "2025-08-05T15:10:09Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2508.03541v1",
      "abs_url": "http://arxiv.org/abs/2508.03541v1"
    },
    "2507.19119v3": {
      "arxiv_id": "2507.19119v3",
      "title": "PatchTraj: Unified Time-Frequency Representation Learning via Dynamic Patches for Trajectory Prediction",
      "authors": [
        "Yanghong Liu",
        "Xingping Dong",
        "Ming Li",
        "Weixing Zhang",
        "Yidong Lou"
      ],
      "summary": "Pedestrian trajectory prediction is crucial for autonomous driving and robotics. While existing point-based and grid-based methods expose two main limitations: insufficiently modeling human motion dynamics, as they fail to balance local motion details with long-range spatiotemporal dependencies, and the time representations lack interaction with their frequency components in jointly modeling trajectory sequences. To address these challenges, we propose PatchTraj, a dynamic patch-based framework that integrates time-frequency joint modeling for trajectory prediction. Specifically, we decompose the trajectory into raw time sequences and frequency components, and employ dynamic patch partitioning to perform multi-scale segmentation, capturing hierarchical motion patterns. Each patch undergoes adaptive embedding with scale-aware feature extraction, followed by hierarchical feature aggregation to model both fine-grained and long-range dependencies. The outputs of the two branches are further enhanced via cross-modal attention, facilitating complementary fusion of temporal and spectral cues. The resulting enhanced embeddings exhibit strong expressive power, enabling accurate predictions even when using a vanilla Transformer architecture. Extensive experiments on ETH-UCY, SDD, NBA, and JRDB datasets demonstrate that our method achieves state-of-the-art performance. Notably, on the egocentric JRDB dataset, PatchTraj attains significant relative improvements of 26.7% in ADE and 17.4% in FDE, underscoring its substantial potential in embodied intelligence.",
      "published_utc": "2025-07-25T09:55:33Z",
      "updated_utc": "2025-07-31T15:04:27Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2507.19119v3",
      "abs_url": "http://arxiv.org/abs/2507.19119v3"
    },
    "2507.13397v3": {
      "arxiv_id": "2507.13397v3",
      "title": "Trustworthy Pedestrian Trajectory Prediction via Pattern-Aware Interaction Modeling",
      "authors": [
        "Kaiyuan Zhai",
        "Juan Chen",
        "Chao Wang",
        "Zeyi Xu",
        "Guoming Tang"
      ],
      "summary": "Accurate and reliable pedestrian trajectory prediction is critical for the application of intelligent applications, yet achieving trustworthy prediction remains highly challenging due to the complexity of interactions among pedestrians. Previous methods often adopt black-box modeling of pedestrian interactions. Despite their strong performance, such opaque modeling limits the reliability of predictions in real-world deployments. To address this issue, we propose InSyn (Interaction-Synchronization Network), a novel Transformer-based model that explicitly captures diverse interaction patterns (e.g., walking in sync or conflicting) while effectively modeling direction-sensitive social behaviors. Additionally, we introduce a training strategy, termed Seq-Start of Seq (SSOS), designed to alleviate the common issue of initial-step divergence in numerical time-series prediction. Experiments on the ETH and UCY datasets demonstrate that our model not only outperforms recent black-box baselines in prediction accuracy, especially under high-density scenarios, but also provides transparent interaction modeling, as shown in the case study. Furthermore, the SSOS strategy proves to be effective in improving sequential prediction performance, reducing the initial-step prediction error by approximately 6.58%. Code is avaliable at https://github.com/rickzky1001/InSyn",
      "published_utc": "2025-07-16T12:23:04Z",
      "updated_utc": "2025-11-12T04:45:16Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2507.13397v3",
      "abs_url": "http://arxiv.org/abs/2507.13397v3"
    },
    "2506.09626v1": {
      "arxiv_id": "2506.09626v1",
      "title": "ECAM: A Contrastive Learning Approach to Avoid Environmental Collision in Trajectory Forecasting",
      "authors": [
        "Giacomo Rosin",
        "Muhammad Rameez Ur Rahman",
        "Sebastiano Vascon"
      ],
      "summary": "Human trajectory forecasting is crucial in applications such as autonomous driving, robotics and surveillance. Accurate forecasting requires models to consider various factors, including social interactions, multi-modal predictions, pedestrian intention and environmental context. While existing methods account for these factors, they often overlook the impact of the environment, which leads to collisions with obstacles. This paper introduces ECAM (Environmental Collision Avoidance Module), a contrastive learning-based module to enhance collision avoidance ability with the environment. The proposed module can be integrated into existing trajectory forecasting models, improving their ability to generate collision-free predictions. We evaluate our method on the ETH/UCY dataset and quantitatively and qualitatively demonstrate its collision avoidance capabilities. Our experiments show that state-of-the-art methods significantly reduce (-40/50%) the collision rate when integrated with the proposed module. The code is available at https://github.com/CVML-CFU/ECAM.",
      "published_utc": "2025-06-11T11:35:36Z",
      "updated_utc": "2025-06-11T11:35:36Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.09626v1",
      "abs_url": "http://arxiv.org/abs/2506.09626v1"
    },
    "2506.00871v3": {
      "arxiv_id": "2506.00871v3",
      "title": "Towards Predicting Any Human Trajectory In Context",
      "authors": [
        "Ryo Fujii",
        "Hideo Saito",
        "Ryo Hachiuma"
      ],
      "summary": "Predicting accurate future trajectories of pedestrians is essential for autonomous systems but remains a challenging task due to the need for adaptability in different environments and domains. A common approach involves collecting scenario-specific data and performing fine-tuning via backpropagation. However, the need to fine-tune for each new scenario is often impractical for deployment on edge devices. To address this challenge, we introduce TrajICL, an In-Context Learning (ICL) framework for pedestrian trajectory prediction that enables adaptation without fine-tuning on the scenario-specific data at inference time without requiring weight updates. We propose a spatio-temporal similarity-based example selection (STES) method that selects relevant examples from previously observed trajectories within the same scene by identifying similar motion patterns at corresponding locations. To further refine this selection, we introduce prediction-guided example selection (PG-ES), which selects examples based on both the past trajectory and the predicted future trajectory, rather than relying solely on the past trajectory. This approach allows the model to account for long-term dynamics when selecting examples. Finally, instead of relying on small real-world datasets with limited scenario diversity, we train our model on a large-scale synthetic dataset to enhance its prediction ability by leveraging in-context examples. Extensive experiments demonstrate that TrajICL achieves remarkable adaptation across both in-domain and cross-domain scenarios, outperforming even fine-tuned approaches across multiple public benchmarks. Project Page: https://fujiry0.github.io/TrajICL-project-page/.",
      "published_utc": "2025-06-01T07:18:47Z",
      "updated_utc": "2025-11-04T03:42:54Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.00871v3",
      "abs_url": "http://arxiv.org/abs/2506.00871v3"
    },
    "2505.06743v3": {
      "arxiv_id": "2505.06743v3",
      "title": "TPK: Trustworthy Trajectory Prediction Integrating Prior Knowledge For Interpretability and Kinematic Feasibility",
      "authors": [
        "Marius Baden",
        "Ahmed Abouelazm",
        "Christian Hubschneider",
        "Yin Wu",
        "Daniel Slieter",
        "J. Marius Zöllner"
      ],
      "summary": "Trajectory prediction is crucial for autonomous driving, enabling vehicles to navigate safely by anticipating the movements of surrounding road users. However, current deep learning models often lack trustworthiness as their predictions can be physically infeasible and illogical to humans. To make predictions more trustworthy, recent research has incorporated prior knowledge, like the social force model for modeling interactions and kinematic models for physical realism. However, these approaches focus on priors that suit either vehicles or pedestrians and do not generalize to traffic with mixed agent classes. We propose incorporating interaction and kinematic priors of all agent classes--vehicles, pedestrians, and cyclists with class-specific interaction layers to capture agent behavioral differences. To improve the interpretability of the agent interactions, we introduce DG-SFM, a rule-based interaction importance score that guides the interaction layer. To ensure physically feasible predictions, we proposed suitable kinematic models for all agent classes with a novel pedestrian kinematic model. We benchmark our approach on the Argoverse 2 dataset, using the state-of-the-art transformer HPTR as our baseline. Experiments demonstrate that our method improves interaction interpretability, revealing a correlation between incorrect predictions and divergence from our interaction prior. Even though incorporating the kinematic models causes a slight decrease in accuracy, they eliminate infeasible trajectories found in the dataset and the baseline model. Thus, our approach fosters trust in trajectory prediction as its interaction reasoning is interpretable, and its predictions adhere to physics.",
      "published_utc": "2025-05-10T19:29:32Z",
      "updated_utc": "2025-07-27T08:14:24Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.06743v3",
      "abs_url": "http://arxiv.org/abs/2505.06743v3"
    },
    "2504.18944v1": {
      "arxiv_id": "2504.18944v1",
      "title": "Demonstrating DVS: Dynamic Virtual-Real Simulation Platform for Mobile Robotic Tasks",
      "authors": [
        "Zijie Zheng",
        "Zeshun Li",
        "Yunpeng Wang",
        "Qinghongbing Xie",
        "Long Zeng"
      ],
      "summary": "With the development of embodied artificial intelligence, robotic research has increasingly focused on complex tasks. Existing simulation platforms, however, are often limited to idealized environments, simple task scenarios and lack data interoperability. This restricts task decomposition and multi-task learning. Additionally, current simulation platforms face challenges in dynamic pedestrian modeling, scene editability, and synchronization between virtual and real assets. These limitations hinder real world robot deployment and feedback. To address these challenges, we propose DVS (Dynamic Virtual-Real Simulation Platform), a platform for dynamic virtual-real synchronization in mobile robotic tasks. DVS integrates a random pedestrian behavior modeling plugin and large-scale, customizable indoor scenes for generating annotated training datasets. It features an optical motion capture system, synchronizing object poses and coordinates between virtual and real world to support dynamic task benchmarking. Experimental validation shows that DVS supports tasks such as pedestrian trajectory prediction, robot path planning, and robotic arm grasping, with potential for both simulation and real world deployment. In this way, DVS represents more than just a versatile robotic platform; it paves the way for research in human intervention in robot execution tasks and real-time feedback algorithms in virtual-real fusion environments. More information about the simulation platform is available on https://immvlab.github.io/DVS/.",
      "published_utc": "2025-04-26T15:07:22Z",
      "updated_utc": "2025-04-26T15:07:22Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.18944v1",
      "abs_url": "http://arxiv.org/abs/2504.18944v1"
    },
    "2504.15616v1": {
      "arxiv_id": "2504.15616v1",
      "title": "SocialMOIF: Multi-Order Intention Fusion for Pedestrian Trajectory Prediction",
      "authors": [
        "Kai Chen",
        "Xiaodong Zhao",
        "Yujie Huang",
        "Guoyu Fang",
        "Xiao Song",
        "Ruiping Wang",
        "Ziyuan Wang"
      ],
      "summary": "The analysis and prediction of agent trajectories are crucial for decision-making processes in intelligent systems, with precise short-term trajectory forecasting being highly significant across a range of applications. Agents and their social interactions have been quantified and modeled by researchers from various perspectives; however, substantial limitations exist in the current work due to the inherent high uncertainty of agent intentions and the complex higher-order influences among neighboring groups. SocialMOIF is proposed to tackle these challenges, concentrating on the higher-order intention interactions among neighboring groups while reinforcing the primary role of first-order intention interactions between neighbors and the target agent. This method develops a multi-order intention fusion model to achieve a more comprehensive understanding of both direct and indirect intention information. Within SocialMOIF, a trajectory distribution approximator is designed to guide the trajectories toward values that align more closely with the actual data, thereby enhancing model interpretability. Furthermore, a global trajectory optimizer is introduced to enable more accurate and efficient parallel predictions. By incorporating a novel loss function that accounts for distance and direction during training, experimental results demonstrate that the model outperforms previous state-of-the-art baselines across multiple metrics in both dynamic and static datasets.",
      "published_utc": "2025-04-22T06:14:49Z",
      "updated_utc": "2025-04-22T06:14:49Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.15616v1",
      "abs_url": "http://arxiv.org/abs/2504.15616v1"
    },
    "2504.13647v1": {
      "arxiv_id": "2504.13647v1",
      "title": "Lightweight LiDAR-Camera 3D Dynamic Object Detection and Multi-Class Trajectory Prediction",
      "authors": [
        "Yushen He",
        "Lei Zhao",
        "Tianchen Deng",
        "Zipeng Fang",
        "Weidong Chen"
      ],
      "summary": "Service mobile robots are often required to avoid dynamic objects while performing their tasks, but they usually have only limited computational resources. So we present a lightweight multi-modal framework for 3D object detection and trajectory prediction. Our system synergistically integrates LiDAR and camera inputs to achieve real-time perception of pedestrians, vehicles, and riders in 3D space. The framework proposes two novel modules: 1) a Cross-Modal Deformable Transformer (CMDT) for object detection with high accuracy and acceptable amount of computation, and 2) a Reference Trajectory-based Multi-Class Transformer (RTMCT) for efficient and diverse trajectory prediction of mult-class objects with flexible trajectory lengths. Evaluations on the CODa benchmark demonstrate superior performance over existing methods across detection (+2.03% in mAP) and trajectory prediction (-0.408m in minADE5 of pedestrians) metrics. Remarkably, the system exhibits exceptional deployability - when implemented on a wheelchair robot with an entry-level NVIDIA 3060 GPU, it achieves real-time inference at 13.2 fps. To facilitate reproducibility and practical deployment, we release the related code of the method at https://github.com/TossherO/3D_Perception and its ROS inference version at https://github.com/TossherO/ros_packages.",
      "published_utc": "2025-04-18T11:59:34Z",
      "updated_utc": "2025-04-18T11:59:34Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.13647v1",
      "abs_url": "http://arxiv.org/abs/2504.13647v1"
    },
    "2503.24272v1": {
      "arxiv_id": "2503.24272v1",
      "title": "Learning Velocity and Acceleration: Self-Supervised Motion Consistency for Pedestrian Trajectory Prediction",
      "authors": [
        "Yizhou Huang",
        "Yihua Cheng",
        "Kezhi Wang"
      ],
      "summary": "Understanding human motion is crucial for accurate pedestrian trajectory prediction. Conventional methods typically rely on supervised learning, where ground-truth labels are directly optimized against predicted trajectories. This amplifies the limitations caused by long-tailed data distributions, making it difficult for the model to capture abnormal behaviors. In this work, we propose a self-supervised pedestrian trajectory prediction framework that explicitly models position, velocity, and acceleration. We leverage velocity and acceleration information to enhance position prediction through feature injection and a self-supervised motion consistency mechanism. Our model hierarchically injects velocity features into the position stream. Acceleration features are injected into the velocity stream. This enables the model to predict position, velocity, and acceleration jointly. From the predicted position, we compute corresponding pseudo velocity and acceleration, allowing the model to learn from data-generated pseudo labels and thus achieve self-supervised learning. We further design a motion consistency evaluation strategy grounded in physical principles; it selects the most reasonable predicted motion trend by comparing it with historical dynamics and uses this trend to guide and constrain trajectory generation. We conduct experiments on the ETH-UCY and Stanford Drone datasets, demonstrating that our method achieves state-of-the-art performance on both datasets.",
      "published_utc": "2025-03-31T16:17:45Z",
      "updated_utc": "2025-03-31T16:17:45Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.24272v1",
      "abs_url": "http://arxiv.org/abs/2503.24272v1"
    },
    "2503.22201v1": {
      "arxiv_id": "2503.22201v1",
      "title": "Multi-modal Knowledge Distillation-based Human Trajectory Forecasting",
      "authors": [
        "Jaewoo Jeong",
        "Seohee Lee",
        "Daehee Park",
        "Giwon Lee",
        "Kuk-Jin Yoon"
      ],
      "summary": "Pedestrian trajectory forecasting is crucial in various applications such as autonomous driving and mobile robot navigation. In such applications, camera-based perception enables the extraction of additional modalities (human pose, text) to enhance prediction accuracy. Indeed, we find that textual descriptions play a crucial role in integrating additional modalities into a unified understanding. However, online extraction of text requires the use of VLM, which may not be feasible for resource-constrained systems. To address this challenge, we propose a multi-modal knowledge distillation framework: a student model with limited modality is distilled from a teacher model trained with full range of modalities. The comprehensive knowledge of a teacher model trained with trajectory, human pose, and text is distilled into a student model using only trajectory or human pose as a sole supplement. In doing so, we separately distill the core locomotion insights from intra-agent multi-modality and inter-agent interaction. Our generalizable framework is validated with two state-of-the-art models across three datasets on both ego-view (JRDB, SIT) and BEV-view (ETH/UCY) setups, utilizing both annotated and VLM-generated text captions. Distilled student models show consistent improvement in all prediction metrics for both full and instantaneous observations, improving up to ~13%. The code is available at https://github.com/Jaewoo97/KDTF.",
      "published_utc": "2025-03-28T07:32:51Z",
      "updated_utc": "2025-03-28T07:32:51Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.22201v1",
      "abs_url": "http://arxiv.org/abs/2503.22201v1"
    },
    "2503.06832v1": {
      "arxiv_id": "2503.06832v1",
      "title": "GUIDE-CoT: Goal-driven and User-Informed Dynamic Estimation for Pedestrian Trajectory using Chain-of-Thought",
      "authors": [
        "Sungsik Kim",
        "Janghyun Baek",
        "Jinkyu Kim",
        "Jaekoo Lee"
      ],
      "summary": "While Large Language Models (LLMs) have recently shown impressive results in reasoning tasks, their application to pedestrian trajectory prediction remains challenging due to two key limitations: insufficient use of visual information and the difficulty of predicting entire trajectories. To address these challenges, we propose Goal-driven and User-Informed Dynamic Estimation for pedestrian trajectory using Chain-of-Thought (GUIDE-CoT). Our approach integrates two innovative modules: (1) a goal-oriented visual prompt, which enhances goal prediction accuracy combining visual prompts with a pretrained visual encoder, and (2) a chain-of-thought (CoT) LLM for trajectory generation, which generates realistic trajectories toward the predicted goal. Moreover, our method introduces controllable trajectory generation, allowing for flexible and user-guided modifications to the predicted paths. Through extensive experiments on the ETH/UCY benchmark datasets, our method achieves state-of-the-art performance, delivering both high accuracy and greater adaptability in pedestrian trajectory prediction. Our code is publicly available at https://github.com/ai-kmu/GUIDE-CoT.",
      "published_utc": "2025-03-10T01:39:24Z",
      "updated_utc": "2025-03-10T01:39:24Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.06832v1",
      "abs_url": "http://arxiv.org/abs/2503.06832v1"
    },
    "2503.04952v1": {
      "arxiv_id": "2503.04952v1",
      "title": "INTENT: Trajectory Prediction Framework with Intention-Guided Contrastive Clustering",
      "authors": [
        "Yihong Tang",
        "Wei Ma"
      ],
      "summary": "Accurate trajectory prediction of road agents (e.g., pedestrians, vehicles) is an essential prerequisite for various intelligent systems applications, such as autonomous driving and robotic navigation. Recent research highlights the importance of environmental contexts (e.g., maps) and the \"multi-modality\" of trajectories, leading to increasingly complex model structures. However, real-world deployments require lightweight models that can quickly migrate and adapt to new environments. Additionally, the core motivations of road agents, referred to as their intentions, deserves further exploration. In this study, we advocate that understanding and reasoning road agents' intention plays a key role in trajectory prediction tasks, and the main challenge is that the concept of intention is fuzzy and abstract. To this end, we present INTENT, an efficient intention-guided trajectory prediction model that relies solely on information contained in the road agent's trajectory. Our model distinguishes itself from existing models in several key aspects: (i) We explicitly model road agents' intentions through contrastive clustering, accommodating the fuzziness and abstraction of human intention in their trajectories. (ii) The proposed INTENT is based solely on multi-layer perceptrons (MLPs), resulting in reduced training and inference time, making it very efficient and more suitable for real-world deployment. (iii) By leveraging estimated intentions and an innovative algorithm for transforming trajectory observations, we obtain more robust trajectory representations that lead to superior prediction accuracy. Extensive experiments on real-world trajectory datasets for pedestrians and autonomous vehicles demonstrate the effectiveness and efficiency of INTENT.",
      "published_utc": "2025-03-06T20:31:11Z",
      "updated_utc": "2025-03-06T20:31:11Z",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.04952v1",
      "abs_url": "http://arxiv.org/abs/2503.04952v1"
    },
    "2502.16847v1": {
      "arxiv_id": "2502.16847v1",
      "title": "Characterizing Structured versus Unstructured Environments based on Pedestrians' and Vehicles' Motion Trajectories",
      "authors": [
        "Mahsa Golchoubian",
        "Moojan Ghafurian",
        "Nasser Lashgarian Azad",
        "Kerstin Dautenhahn"
      ],
      "summary": "Trajectory behaviours of pedestrians and vehicles operating close to each other can be different in unstructured compared to structured environments. These differences in the motion behaviour are valuable to be considered in the trajectory prediction algorithm of an autonomous vehicle. However, the available datasets on pedestrians' and vehicles' trajectories that are commonly used as benchmarks for trajectory prediction have not been classified based on the nature of their environment. On the other hand, the definitions provided for unstructured and structured environments are rather qualitative and hard to be used for justifying the type of a given environment. In this paper, we have compared different existing datasets based on a couple of extracted trajectory features, such as mean speed and trajectory variability. Through K-means clustering and generalized linear models, we propose more quantitative measures for distinguishing the two different types of environments. Our results show that features such as trajectory variability, stop fraction and density of pedestrians are different among the two environmental types and can be used to classify the existing datasets.",
      "published_utc": "2025-02-24T05:09:21Z",
      "updated_utc": "2025-02-24T05:09:21Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.16847v1",
      "abs_url": "http://arxiv.org/abs/2502.16847v1"
    },
    "2502.14676v2": {
      "arxiv_id": "2502.14676v2",
      "title": "BP-SGCN: Behavioral Pseudo-Label Informed Sparse Graph Convolution Network for Pedestrian and Heterogeneous Trajectory Prediction",
      "authors": [
        "Ruochen Li",
        "Stamos Katsigiannis",
        "Tae-Kyun Kim",
        "Hubert P. H. Shum"
      ],
      "summary": "Trajectory prediction allows better decision-making in applications of autonomous vehicles or surveillance by predicting the short-term future movement of traffic agents. It is classified into pedestrian or heterogeneous trajectory prediction. The former exploits the relatively consistent behavior of pedestrians, but is limited in real-world scenarios with heterogeneous traffic agents such as cyclists and vehicles. The latter typically relies on extra class label information to distinguish the heterogeneous agents, but such labels are costly to annotate and cannot be generalized to represent different behaviors within the same class of agents. In this work, we introduce the behavioral pseudo-labels that effectively capture the behavior distributions of pedestrians and heterogeneous agents solely based on their motion features, significantly improving the accuracy of trajectory prediction. To implement the framework, we propose the Behavioral Pseudo-Label Informed Sparse Graph Convolution Network (BP-SGCN) that learns pseudo-labels and informs to a trajectory predictor. For optimization, we propose a cascaded training scheme, in which we first learn the pseudo-labels in an unsupervised manner, and then perform end-to-end fine-tuning on the labels in the direction of increasing the trajectory prediction accuracy. Experiments show that our pseudo-labels effectively model different behavior clusters and improve trajectory prediction. Our proposed BP-SGCN outperforms existing methods using both pedestrian (ETH/UCY, pedestrian-only SDD) and heterogeneous agent datasets (SDD, Argoverse 1).",
      "published_utc": "2025-02-20T16:09:21Z",
      "updated_utc": "2025-02-21T21:29:38Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.14676v2",
      "abs_url": "http://arxiv.org/abs/2502.14676v2"
    },
    "2502.10585v1": {
      "arxiv_id": "2502.10585v1",
      "title": "Prediction uncertainty-aware planning using deep ensembles and trajectory optimisation",
      "authors": [
        "Anshul Nayak",
        "Azim Eskandarian"
      ],
      "summary": "Human motion is stochastic and ensuring safe robot navigation in a pedestrian-rich environment requires proactive decision-making. Past research relied on incorporating deterministic future states of surrounding pedestrians which can be overconfident leading to unsafe robot behaviour. The current paper proposes a predictive uncertainty-aware planner that integrates neural network based probabilistic trajectory prediction into planning. Our method uses a deep ensemble based network for probabilistic forecasting of surrounding humans and integrates the predictive uncertainty as constraints into the planner. We compare numerous constraint satisfaction methods on the planner and evaluated its performance on real world pedestrian datasets. Further, offline robot navigation was carried out on out-of-distribution pedestrian trajectories inside a narrow corridor",
      "published_utc": "2025-02-14T22:29:19Z",
      "updated_utc": "2025-02-14T22:29:19Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.10585v1",
      "abs_url": "http://arxiv.org/abs/2502.10585v1"
    },
    "2502.02504v1": {
      "arxiv_id": "2502.02504v1",
      "title": "Unified Spatial-Temporal Edge-Enhanced Graph Networks for Pedestrian Trajectory Prediction",
      "authors": [
        "Ruochen Li",
        "Tanqiu Qiao",
        "Stamos Katsigiannis",
        "Zhanxing Zhu",
        "Hubert P. H. Shum"
      ],
      "summary": "Pedestrian trajectory prediction aims to forecast future movements based on historical paths. Spatial-temporal (ST) methods often separately model spatial interactions among pedestrians and temporal dependencies of individuals. They overlook the direct impacts of interactions among different pedestrians across various time steps (i.e., high-order cross-time interactions). This limits their ability to capture ST inter-dependencies and hinders prediction performance. To address these limitations, we propose UniEdge with three major designs. Firstly, we introduce a unified ST graph data structure that simplifies high-order cross-time interactions into first-order relationships, enabling the learning of ST inter-dependencies in a single step. This avoids the information loss caused by multi-step aggregation. Secondly, traditional GNNs focus on aggregating pedestrian node features, neglecting the propagation of implicit interaction patterns encoded in edge features. We propose the Edge-to-Edge-Node-to-Node Graph Convolution (E2E-N2N-GCN), a novel dual-graph network that jointly models explicit N2N social interactions among pedestrians and implicit E2E influence propagation across these interaction patterns. Finally, to overcome the limited receptive fields and challenges in capturing long-range dependencies of auto-regressive architectures, we introduce a transformer encoder-based predictor that enables global modeling of temporal correlation. UniEdge outperforms state-of-the-arts on multiple datasets, including ETH, UCY, and SDD.",
      "published_utc": "2025-02-04T17:18:54Z",
      "updated_utc": "2025-02-04T17:18:54Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.02504v1",
      "abs_url": "http://arxiv.org/abs/2502.02504v1"
    },
    "2501.13848v1": {
      "arxiv_id": "2501.13848v1",
      "title": "Where Do You Go? Pedestrian Trajectory Prediction using Scene Features",
      "authors": [
        "Mohammad Ali Rezaei",
        "Fardin Ayar",
        "Ehsan Javanmardi",
        "Manabu Tsukada",
        "Mahdi Javanmardi"
      ],
      "summary": "Accurate prediction of pedestrian trajectories is crucial for enhancing the safety of autonomous vehicles and reducing traffic fatalities involving pedestrians. While numerous studies have focused on modeling interactions among pedestrians to forecast their movements, the influence of environmental factors and scene-object placements has been comparatively underexplored. In this paper, we present a novel trajectory prediction model that integrates both pedestrian interactions and environmental context to improve prediction accuracy. Our approach captures spatial and temporal interactions among pedestrians within a sparse graph framework. To account for pedestrian-scene interactions, we employ advanced image enhancement and semantic segmentation techniques to extract detailed scene features. These scene and interaction features are then fused through a cross-attention mechanism, enabling the model to prioritize relevant environmental factors that influence pedestrian movements. Finally, a temporal convolutional network processes the fused features to predict future pedestrian trajectories. Experimental results demonstrate that our method significantly outperforms existing state-of-the-art approaches, achieving ADE and FDE values of 0.252 and 0.372 meters, respectively, underscoring the importance of incorporating both social interactions and environmental context in pedestrian trajectory prediction.",
      "published_utc": "2025-01-23T17:15:26Z",
      "updated_utc": "2025-01-23T17:15:26Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.13848v1",
      "abs_url": "http://arxiv.org/abs/2501.13848v1"
    },
    "2501.13973v1": {
      "arxiv_id": "2501.13973v1",
      "title": "A Spatio-temporal Graph Network Allowing Incomplete Trajectory Input for Pedestrian Trajectory Prediction",
      "authors": [
        "Juncen Long",
        "Gianluca Bardaro",
        "Simone Mentasti",
        "Matteo Matteucci"
      ],
      "summary": "Pedestrian trajectory prediction is important in the research of mobile robot navigation in environments with pedestrians. Most pedestrian trajectory prediction algorithms require the input historical trajectories to be complete. If a pedestrian is unobservable in any frame in the past, then its historical trajectory become incomplete, the algorithm will not predict its future trajectory. To address this limitation, we propose the STGN-IT, a spatio-temporal graph network allowing incomplete trajectory input, which can predict the future trajectories of pedestrians with incomplete historical trajectories. STGN-IT uses the spatio-temporal graph with an additional encoding method to represent the historical trajectories and observation states of pedestrians. Moreover, STGN-IT introduces static obstacles in the environment that may affect the future trajectories as nodes to further improve the prediction accuracy. A clustering algorithm is also applied in the construction of spatio-temporal graphs. Experiments on public datasets show that STGN-IT outperforms state of the art algorithms on these metrics.",
      "published_utc": "2025-01-22T19:32:07Z",
      "updated_utc": "2025-01-22T19:32:07Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.13973v1",
      "abs_url": "http://arxiv.org/abs/2501.13973v1"
    },
    "2501.09878v1": {
      "arxiv_id": "2501.09878v1",
      "title": "ASTRA: A Scene-aware TRAnsformer-based model for trajectory prediction",
      "authors": [
        "Izzeddin Teeti",
        "Aniket Thomas",
        "Munish Monga",
        "Sachin Kumar",
        "Uddeshya Singh",
        "Andrew Bradley",
        "Biplab Banerjee",
        "Fabio Cuzzolin"
      ],
      "summary": "We present ASTRA (A} Scene-aware TRAnsformer-based model for trajectory prediction), a light-weight pedestrian trajectory forecasting model that integrates the scene context, spatial dynamics, social inter-agent interactions and temporal progressions for precise forecasting. We utilised a U-Net-based feature extractor, via its latent vector representation, to capture scene representations and a graph-aware transformer encoder for capturing social interactions. These components are integrated to learn an agent-scene aware embedding, enabling the model to learn spatial dynamics and forecast the future trajectory of pedestrians. The model is designed to produce both deterministic and stochastic outcomes, with the stochastic predictions being generated by incorporating a Conditional Variational Auto-Encoder (CVAE). ASTRA also proposes a simple yet effective weighted penalty loss function, which helps to yield predictions that outperform a wide array of state-of-the-art deterministic and generative models. ASTRA demonstrates an average improvement of 27%/10% in deterministic/stochastic settings on the ETH-UCY dataset, and 26% improvement on the PIE dataset, respectively, along with seven times fewer parameters than the existing state-of-the-art model (see Figure 1). Additionally, the model's versatility allows it to generalize across different perspectives, such as Bird's Eye View (BEV) and Ego-Vehicle View (EVV).",
      "published_utc": "2025-01-16T23:28:30Z",
      "updated_utc": "2025-01-16T23:28:30Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.09878v1",
      "abs_url": "http://arxiv.org/abs/2501.09878v1"
    },
    "2501.07711v1": {
      "arxiv_id": "2501.07711v1",
      "title": "Pedestrian Trajectory Prediction Based on Social Interactions Learning With Random Weights",
      "authors": [
        "Jiajia Xie",
        "Sheng Zhang",
        "Beihao Xia",
        "Zhu Xiao",
        "Hongbo Jiang",
        "Siwang Zhou",
        "Zheng Qin",
        "Hongyang Chen"
      ],
      "summary": "Pedestrian trajectory prediction is a critical technology in the evolution of self-driving cars toward complete artificial intelligence. Over recent years, focusing on the trajectories of pedestrians to model their social interactions has surged with great interest in more accurate trajectory predictions. However, existing methods for modeling pedestrian social interactions rely on pre-defined rules, struggling to capture non-explicit social interactions. In this work, we propose a novel framework named DTGAN, which extends the application of Generative Adversarial Networks (GANs) to graph sequence data, with the primary objective of automatically capturing implicit social interactions and achieving precise predictions of pedestrian trajectory. DTGAN innovatively incorporates random weights within each graph to eliminate the need for pre-defined interaction rules. We further enhance the performance of DTGAN by exploring diverse task loss functions during adversarial training, which yields improvements of 16.7\\% and 39.3\\% on metrics ADE and FDE, respectively. The effectiveness and accuracy of our framework are verified on two public datasets. The experimental results show that our proposed DTGAN achieves superior performance and is well able to understand pedestrians' intentions.",
      "published_utc": "2025-01-13T21:45:01Z",
      "updated_utc": "2025-01-13T21:45:01Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.07711v1",
      "abs_url": "http://arxiv.org/abs/2501.07711v1"
    },
    "2501.06680v2": {
      "arxiv_id": "2501.06680v2",
      "title": "Application of Vision-Language Model to Pedestrians Behavior and Scene Understanding in Autonomous Driving",
      "authors": [
        "Haoxiang Gao",
        "Li Zhang",
        "Yu Zhao",
        "Zhou Yang",
        "Jinghan Cao"
      ],
      "summary": "Vision-language models (VLMs) have become a promising approach to enhancing perception and decision-making in autonomous driving. The gap remains in applying VLMs to understand complex scenarios interacting with pedestrians and efficient vehicle deployment. In this paper, we propose a knowledge distillation method that transfers knowledge from large-scale vision-language foundation models to efficient vision networks, and we apply it to pedestrian behavior prediction and scene understanding tasks, achieving promising results in generating more diverse and comprehensive semantic attributes. We also utilize multiple pre-trained models and ensemble techniques to boost the model's performance. We further examined the effectiveness of the model after knowledge distillation; the results show significant metric improvements in open-vocabulary perception and trajectory prediction tasks, which can potentially enhance the end-to-end performance of autonomous driving.",
      "published_utc": "2025-01-12T01:31:07Z",
      "updated_utc": "2025-07-30T17:16:46Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.06680v2",
      "abs_url": "http://arxiv.org/abs/2501.06680v2"
    },
    "2501.01915v1": {
      "arxiv_id": "2501.01915v1",
      "title": "Social Processes: Probabilistic Meta-learning for Adaptive Multiparty Interaction Forecasting",
      "authors": [
        "Augustinas Jučas",
        "Chirag Raman"
      ],
      "summary": "Adaptively forecasting human behavior in social settings is an important step toward achieving Artificial General Intelligence. Most existing research in social forecasting has focused either on unfocused interactions, such as pedestrian trajectory prediction, or on monadic and dyadic behavior forecasting. In contrast, social psychology emphasizes the importance of group interactions for understanding complex social dynamics. This creates a gap that we address in this paper: forecasting social interactions at the group (conversation) level. Additionally, it is important for a forecasting model to be able to adapt to groups unseen at train time, as even the same individual behaves differently across different groups. This highlights the need for a forecasting model to explicitly account for each group's unique dynamics. To achieve this, we adopt a meta-learning approach to human behavior forecasting, treating every group as a separate meta-learning task. As a result, our method conditions its predictions on the specific behaviors within the group, leading to generalization to unseen groups. Specifically, we introduce Social Process (SP) models, which predict a distribution over future multimodal cues jointly for all group members based on their preceding low-level multimodal cues, while incorporating other past sequences of the same group's interactions. In this work we also analyze the generalization capabilities of SP models in both their outputs and latent spaces through the use of realistic synthetic datasets.",
      "published_utc": "2025-01-03T17:34:53Z",
      "updated_utc": "2025-01-03T17:34:53Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.01915v1",
      "abs_url": "http://arxiv.org/abs/2501.01915v1"
    },
    "2412.08096v1": {
      "arxiv_id": "2412.08096v1",
      "title": "THUD++: Large-Scale Dynamic Indoor Scene Dataset and Benchmark for Mobile Robots",
      "authors": [
        "Zeshun Li",
        "Fuhao Li",
        "Wanting Zhang",
        "Zijie Zheng",
        "Xueping Liu",
        "Yongjin Liu",
        "Long Zeng"
      ],
      "summary": "Most existing mobile robotic datasets primarily capture static scenes, limiting their utility for evaluating robotic performance in dynamic environments. To address this, we present a mobile robot oriented large-scale indoor dataset, denoted as THUD++ (TsingHua University Dynamic) robotic dataset, for dynamic scene understanding. Our current dataset includes 13 large-scale dynamic scenarios, combining both real-world and synthetic data collected with a real robot platform and a physical simulation platform, respectively. The RGB-D dataset comprises over 90K image frames, 20M 2D/3D bounding boxes of static and dynamic objects, camera poses, and IMU. The trajectory dataset covers over 6,000 pedestrian trajectories in indoor scenes. Additionally, the dataset is augmented with a Unity3D-based simulation platform, allowing researchers to create custom scenes and test algorithms in a controlled environment. We evaluate state-of-the-art methods on THUD++ across mainstream indoor scene understanding tasks, e.g., 3D object detection, semantic segmentation, relocalization, pedestrian trajectory prediction, and navigation. Our experiments highlight the challenges mobile robots encounter in indoor environments, especially when navigating in complex, crowded, and dynamic scenes. By sharing this dataset, we aim to accelerate the development and testing of mobile robot algorithms, contributing to real-world robotic applications.",
      "published_utc": "2024-12-11T04:37:15Z",
      "updated_utc": "2024-12-11T04:37:15Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.08096v1",
      "abs_url": "http://arxiv.org/abs/2412.08096v1"
    },
    "2412.04673v1": {
      "arxiv_id": "2412.04673v1",
      "title": "Socially-Informed Reconstruction for Pedestrian Trajectory Forecasting",
      "authors": [
        "Haleh Damirchi",
        "Ali Etemad",
        "Michael Greenspan"
      ],
      "summary": "Pedestrian trajectory prediction remains a challenge for autonomous systems, particularly due to the intricate dynamics of social interactions. Accurate forecasting requires a comprehensive understanding not only of each pedestrian's previous trajectory but also of their interaction with the surrounding environment, an important part of which are other pedestrians moving dynamically in the scene. To learn effective socially-informed representations, we propose a model that uses a reconstructor alongside a conditional variational autoencoder-based trajectory forecasting module. This module generates pseudo-trajectories, which we use as augmentations throughout the training process. To further guide the model towards social awareness, we propose a novel social loss that aids in forecasting of more stable trajectories. We validate our approach through extensive experiments, demonstrating strong performances in comparison to state of-the-art methods on the ETH/UCY and SDD benchmarks.",
      "published_utc": "2024-12-05T23:54:06Z",
      "updated_utc": "2024-12-05T23:54:06Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.04673v1",
      "abs_url": "http://arxiv.org/abs/2412.04673v1"
    },
    "2412.02395v1": {
      "arxiv_id": "2412.02395v1",
      "title": "Who Walks With You Matters: Perceiving Social Interactions with Groups for Pedestrian Trajectory Prediction",
      "authors": [
        "Ziqian Zou",
        "Conghao Wong",
        "Beihao Xia",
        "Qinmu Peng",
        "Xinge You"
      ],
      "summary": "Understanding and anticipating human movement has become more critical and challenging in diverse applications such as autonomous driving and surveillance. The complex interactions brought by different relations between agents are a crucial reason that poses challenges to this task. Researchers have put much effort into designing a system using rule-based or data-based models to extract and validate the patterns between pedestrian trajectories and these interactions, which has not been adequately addressed yet. Inspired by how humans perceive social interactions with different level of relations to themself, this work proposes the GrouP ConCeption (short for GPCC) model composed of the Group method, which categorizes nearby agents into either group members or non-group members based on a long-term distance kernel function, and the Conception module, which perceives both visual and acoustic information surrounding the target agent. Evaluated across multiple datasets, the GPCC model demonstrates significant improvements in trajectory prediction accuracy, validating its effectiveness in modeling both social and individual dynamics. The qualitative analysis also indicates that the GPCC framework successfully leverages grouping and perception cues human-like intuitively to validate the proposed model's explainability in pedestrian trajectory forecasting.",
      "published_utc": "2024-12-03T11:47:33Z",
      "updated_utc": "2024-12-03T11:47:33Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.02395v1",
      "abs_url": "http://arxiv.org/abs/2412.02395v1"
    },
    "2411.17376v3": {
      "arxiv_id": "2411.17376v3",
      "title": "RealTraj: Towards Real-World Pedestrian Trajectory Forecasting",
      "authors": [
        "Ryo Fujii",
        "Hideo Saito",
        "Ryo Hachiuma"
      ],
      "summary": "This paper jointly addresses three key limitations in conventional pedestrian trajectory forecasting: pedestrian perception errors, real-world data collection costs, and person ID annotation costs. We propose a novel framework, RealTraj, that enhances the real-world applicability of trajectory forecasting. Our approach includes two training phases -- self-supervised pretraining on synthetic data and weakly-supervised fine-tuning with limited real-world data -- to minimize data collection efforts. To improve robustness to real-world errors, we focus on both model design and training objectives. Specifically, we present Det2TrajFormer, a trajectory forecasting model that remains invariant to tracking noise by using past detections as inputs. Additionally, we pretrain the model using multiple pretext tasks, which enhance robustness and improve forecasting performance based solely on detection data. Unlike previous trajectory forecasting methods, our approach fine-tunes the model using only ground-truth detections, reducing the need for costly person ID annotations. In the experiments, we comprehensively verify the effectiveness of the proposed method against the limitations, and the method outperforms state-of-the-art trajectory forecasting methods on multiple datasets. The code will be released at https://fujiry0.github.io/RealTraj-project-page.",
      "published_utc": "2024-11-26T12:35:26Z",
      "updated_utc": "2025-03-09T13:26:35Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.17376v3",
      "abs_url": "http://arxiv.org/abs/2411.17376v3"
    },
    "2411.00174v1": {
      "arxiv_id": "2411.00174v1",
      "title": "Pedestrian Trajectory Prediction with Missing Data: Datasets, Imputation, and Benchmarking",
      "authors": [
        "Pranav Singh Chib",
        "Pravendra Singh"
      ],
      "summary": "Pedestrian trajectory prediction is crucial for several applications such as robotics and self-driving vehicles. Significant progress has been made in the past decade thanks to the availability of pedestrian trajectory datasets, which enable trajectory prediction methods to learn from pedestrians' past movements and predict future trajectories. However, these datasets and methods typically assume that the observed trajectory sequence is complete, ignoring real-world issues such as sensor failure, occlusion, and limited fields of view that can result in missing values in observed trajectories. To address this challenge, we present TrajImpute, a pedestrian trajectory prediction dataset that simulates missing coordinates in the observed trajectory, enhancing real-world applicability. TrajImpute maintains a uniform distribution of missing data within the observed trajectories. In this work, we comprehensively examine several imputation methods to reconstruct the missing coordinates and benchmark them for imputing pedestrian trajectories. Furthermore, we provide a thorough analysis of recent trajectory prediction methods and evaluate the performance of these models on the imputed trajectories. Our experimental evaluation of the imputation and trajectory prediction methods offers several valuable insights. Our dataset provides a foundational resource for future research on imputation-aware pedestrian trajectory prediction, potentially accelerating the deployment of these methods in real-world applications. Publicly accessible links to the datasets and code files are available at https://github.com/Pranav-chib/TrajImpute.",
      "published_utc": "2024-10-31T19:42:42Z",
      "updated_utc": "2024-10-31T19:42:42Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.00174v1",
      "abs_url": "http://arxiv.org/abs/2411.00174v1"
    },
    "2410.19544v1": {
      "arxiv_id": "2410.19544v1",
      "title": "PMM-Net: Single-stage Multi-agent Trajectory Prediction with Patching-based Embedding and Explicit Modal Modulation",
      "authors": [
        "Huajian Liu",
        "Wei Dong",
        "Kunpeng Fan",
        "Chao Wang",
        "Yongzhuo Gao"
      ],
      "summary": "Analyzing and forecasting trajectories of agents like pedestrians plays a pivotal role for embodied intelligent applications. The inherent indeterminacy of human behavior and complex social interaction among a rich variety of agents make this task more challenging than common time-series forecasting. In this letter, we aim to explore a distinct formulation for multi-agent trajectory prediction framework. Specifically, we proposed a patching-based temporal feature extraction module and a graph-based social feature extraction module, enabling effective feature extraction and cross-scenario generalization. Moreover, we reassess the role of social interaction and present a novel method based on explicit modality modulation to integrate temporal and social features, thereby constructing an efficient single-stage inference pipeline. Results on public benchmark datasets demonstrate the superior performance of our model compared with the state-of-the-art methods. The code is available at: github.com/TIB-K330/pmm-net.",
      "published_utc": "2024-10-25T13:16:27Z",
      "updated_utc": "2024-10-25T13:16:27Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.19544v1",
      "abs_url": "http://arxiv.org/abs/2410.19544v1"
    },
    "2410.16864v1": {
      "arxiv_id": "2410.16864v1",
      "title": "Pedestrian motion prediction evaluation for urban autonomous driving",
      "authors": [
        "Dmytro Zabolotnii",
        "Yar Muhammad",
        "Naveed Muhammad"
      ],
      "summary": "Pedestrian motion prediction is a key part of the modular-based autonomous driving pipeline, ensuring safe, accurate, and timely awareness of human agents' possible future trajectories. The autonomous vehicle can use this information to prevent any possible accidents and create a comfortable and pleasant driving experience for the passengers and pedestrians. A wealth of research was done on the topic from the authors of robotics, computer vision, intelligent transportation systems, and other fields. However, a relatively unexplored angle is the integration of the state-of-art solutions into existing autonomous driving stacks and evaluating them in real-life conditions rather than sanitized datasets. We analyze selected publications with provided open-source solutions and provide a perspective obtained by integrating them into existing Autonomous Driving framework - Autoware Mini and performing experiments in natural urban conditions in Tartu, Estonia to determine valuability of traditional motion prediction metrics. This perspective should be valuable to any potential autonomous driving or robotics engineer looking for the real-world performance of the existing state-of-art pedestrian motion prediction problem. The code with instructions on accessing the dataset is available at https://github.com/dmytrozabolotnii/autoware_mini.",
      "published_utc": "2024-10-22T10:06:50Z",
      "updated_utc": "2024-10-22T10:06:50Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.16864v1",
      "abs_url": "http://arxiv.org/abs/2410.16864v1"
    },
    "2409.14984v1": {
      "arxiv_id": "2409.14984v1",
      "title": "SocialCircle+: Learning the Angle-based Conditioned Interaction Representation for Pedestrian Trajectory Prediction",
      "authors": [
        "Conghao Wong",
        "Beihao Xia",
        "Ziqian Zou",
        "Xinge You"
      ],
      "summary": "Trajectory prediction is a crucial aspect of understanding human behaviors. Researchers have made efforts to represent socially interactive behaviors among pedestrians and utilize various networks to enhance prediction capability. Unfortunately, they still face challenges not only in fully explaining and measuring how these interactive behaviors work to modify trajectories but also in modeling pedestrians' preferences to plan or participate in social interactions in response to the changeable physical environments as extra conditions. This manuscript mainly focuses on the above explainability and conditionality requirements for trajectory prediction networks. Inspired by marine animals perceiving other companions and the environment underwater by echolocation, this work constructs an angle-based conditioned social interaction representation SocialCircle+ to represent the socially interactive context and its corresponding conditions. It employs a social branch and a conditional branch to describe how pedestrians are positioned in prediction scenes socially and physically in angle-based-cyclic-sequence forms. Then, adaptive fusion is applied to fuse the above conditional clues onto the social ones to learn the final interaction representation. Experiments demonstrate the superiority of SocialCircle+ with different trajectory prediction backbones. Moreover, counterfactual interventions have been made to simultaneously verify the modeling capacity of causalities among interactive variables and the conditioning capability.",
      "published_utc": "2024-09-23T13:02:12Z",
      "updated_utc": "2024-09-23T13:02:12Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.14984v1",
      "abs_url": "http://arxiv.org/abs/2409.14984v1"
    },
    "2409.04069v2": {
      "arxiv_id": "2409.04069v2",
      "title": "Online Residual Learning from Offline Experts for Pedestrian Tracking",
      "authors": [
        "Anastasios Vlachos",
        "Anastasios Tsiamis",
        "Aren Karapetyan",
        "Efe C. Balta",
        "John Lygeros"
      ],
      "summary": "In this paper, we consider the problem of predicting unknown targets from data. We propose Online Residual Learning (ORL), a method that combines online adaptation with offline-trained predictions. At a lower level, we employ multiple offline predictions generated before or at the beginning of the prediction horizon. We augment every offline prediction by learning their respective residual error concerning the true target state online, using the recursive least squares algorithm. At a higher level, we treat the augmented lower-level predictors as experts, adopting the Prediction with Expert Advice framework. We utilize an adaptive softmax weighting scheme to form an aggregate prediction and provide guarantees for ORL in terms of regret. We employ ORL to boost performance in the setting of online pedestrian trajectory prediction. Based on data from the Stanford Drone Dataset, we show that ORL can demonstrate best-of-both-worlds performance.",
      "published_utc": "2024-09-06T07:20:45Z",
      "updated_utc": "2024-09-09T14:28:03Z",
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY",
        "cs.LG",
        "math.OC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.04069v2",
      "abs_url": "http://arxiv.org/abs/2409.04069v2"
    },
    "2409.01971v2": {
      "arxiv_id": "2409.01971v2",
      "title": "Snapshot: Towards Application-centered Models for Pedestrian Trajectory Prediction in Urban Traffic Environments",
      "authors": [
        "Nico Uhlemann",
        "Yipeng Zhou",
        "Tobias Simeon Mohr",
        "Markus Lienkamp"
      ],
      "summary": "This paper explores pedestrian trajectory prediction in urban traffic while focusing on both model accuracy and real-world applicability. While promising approaches exist, they often revolve around pedestrian datasets excluding traffic-related information, or resemble architectures that are either not real-time capable or robust. To address these limitations, we first introduce a dedicated benchmark based on Argoverse 2, specifically targeting pedestrians in traffic environments. Following this, we present Snapshot, a modular, feed-forward neural network that outperforms the current state of the art, reducing the Average Displacement Error (ADE) by 8.8% while utilizing significantly less information. Despite its agent-centric encoding scheme, Snapshot demonstrates scalability, real-time performance, and robustness to varying motion histories. Moreover, by integrating Snapshot into a modular autonomous driving software stack, we showcase its real-world applicability.",
      "published_utc": "2024-09-03T15:15:49Z",
      "updated_utc": "2025-01-09T17:57:53Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.01971v2",
      "abs_url": "http://arxiv.org/abs/2409.01971v2"
    },
    "2408.15250v1": {
      "arxiv_id": "2408.15250v1",
      "title": "Pedestrian Motion Prediction Using Transformer-based Behavior Clustering and Data-Driven Reachability Analysis",
      "authors": [
        "Kleio Fragkedaki",
        "Frank J. Jiang",
        "Karl H. Johansson",
        "Jonas Mårtensson"
      ],
      "summary": "In this work, we present a transformer-based framework for predicting future pedestrian states based on clustered historical trajectory data. In previous studies, researchers propose enhancing pedestrian trajectory predictions by using manually crafted labels to categorize pedestrian behaviors and intentions. However, these approaches often only capture a limited range of pedestrian behaviors and introduce human bias into the predictions. To alleviate the dependency on manually crafted labels, we utilize a transformer encoder coupled with hierarchical density-based clustering to automatically identify diverse behavior patterns, and use these clusters in data-driven reachability analysis. By using a transformer-based approach, we seek to enhance the representation of pedestrian trajectories and uncover characteristics or features that are subsequently used to group trajectories into different \"behavior\" clusters. We show that these behavior clusters can be used with data-driven reachability analysis, yielding an end-to-end data-driven approach to predicting the future motion of pedestrians. We train and evaluate our approach on a real pedestrian dataset, showcasing its effectiveness in forecasting pedestrian movements.",
      "published_utc": "2024-08-09T07:24:30Z",
      "updated_utc": "2024-08-09T07:24:30Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.RO",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.15250v1",
      "abs_url": "http://arxiv.org/abs/2408.15250v1"
    },
    "2408.12609v1": {
      "arxiv_id": "2408.12609v1",
      "title": "Enhanced Prediction of Multi-Agent Trajectories via Control Inference and State-Space Dynamics",
      "authors": [
        "Yu Zhang",
        "Yongxiang Zou",
        "Haoyu Zhang",
        "Zeyu Liu",
        "Houcheng Li",
        "Long Cheng"
      ],
      "summary": "In the field of autonomous systems, accurately predicting the trajectories of nearby vehicles and pedestrians is crucial for ensuring both safety and operational efficiency. This paper introduces a novel methodology for trajectory forecasting based on state-space dynamic system modeling, which endows agents with models that have tangible physical implications. To enhance the precision of state estimations within the dynamic system, the paper also presents a novel modeling technique for control variables. This technique utilizes a newly introduced model, termed \"Mixed Mamba,\" to derive initial control states, thereby improving the predictive accuracy of these variables. Moverover, the proposed approach ingeniously integrates graph neural networks with state-space models, effectively capturing the complexities of multi-agent interactions. This combination provides a robust and scalable framework for forecasting multi-agent trajectories across a range of scenarios. Comprehensive evaluations demonstrate that this model outperforms several established benchmarks across various metrics and datasets, highlighting its significant potential to advance trajectory forecasting in autonomous systems.",
      "published_utc": "2024-08-08T08:33:02Z",
      "updated_utc": "2024-08-08T08:33:02Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.12609v1",
      "abs_url": "http://arxiv.org/abs/2408.12609v1"
    },
    "2407.17162v1": {
      "arxiv_id": "2407.17162v1",
      "title": "Context-aware Multi-task Learning for Pedestrian Intent and Trajectory Prediction",
      "authors": [
        "Farzeen Munir",
        "Tomasz Piotr Kucner"
      ],
      "summary": "The advancement of socially-aware autonomous vehicles hinges on precise modeling of human behavior. Within this broad paradigm, the specific challenge lies in accurately predicting pedestrian's trajectory and intention. Traditional methodologies have leaned heavily on historical trajectory data, frequently overlooking vital contextual cues such as pedestrian-specific traits and environmental factors. Furthermore, there's a notable knowledge gap as trajectory and intention prediction have largely been approached as separate problems, despite their mutual dependence. To bridge this gap, we introduce PTINet (Pedestrian Trajectory and Intention Prediction Network), which jointly learns the trajectory and intention prediction by combining past trajectory observations, local contextual features (individual pedestrian behaviors), and global features (signs, markings etc.). The efficacy of our approach is evaluated on widely used public datasets: JAAD and PIE, where it has demonstrated superior performance over existing state-of-the-art models in trajectory and intention prediction. The results from our experiments and ablation studies robustly validate PTINet's effectiveness in jointly exploring intention and trajectory prediction for pedestrian behaviour modelling. The experimental evaluation indicates the advantage of using global and local contextual features for pedestrian trajectory and intention prediction. The effectiveness of PTINet in predicting pedestrian behavior paves the way for the development of automated systems capable of seamlessly interacting with pedestrians in urban settings.",
      "published_utc": "2024-07-24T11:06:47Z",
      "updated_utc": "2024-07-24T11:06:47Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.17162v1",
      "abs_url": "http://arxiv.org/abs/2407.17162v1"
    },
    "2407.11588v1": {
      "arxiv_id": "2407.11588v1",
      "title": "Progressive Pretext Task Learning for Human Trajectory Prediction",
      "authors": [
        "Xiaotong Lin",
        "Tianming Liang",
        "Jianhuang Lai",
        "Jian-Fang Hu"
      ],
      "summary": "Human trajectory prediction is a practical task of predicting the future positions of pedestrians on the road, which typically covers all temporal ranges from short-term to long-term within a trajectory. However, existing works attempt to address the entire trajectory prediction with a singular, uniform training paradigm, neglecting the distinction between short-term and long-term dynamics in human trajectories. To overcome this limitation, we introduce a novel Progressive Pretext Task learning (PPT) framework, which progressively enhances the model's capacity of capturing short-term dynamics and long-term dependencies for the final entire trajectory prediction. Specifically, we elaborately design three stages of training tasks in the PPT framework. In the first stage, the model learns to comprehend the short-term dynamics through a stepwise next-position prediction task. In the second stage, the model is further enhanced to understand long-term dependencies through a destination prediction task. In the final stage, the model aims to address the entire future trajectory task by taking full advantage of the knowledge from previous stages. To alleviate the knowledge forgetting, we further apply a cross-task knowledge distillation. Additionally, we design a Transformer-based trajectory predictor, which is able to achieve highly efficient two-step reasoning by integrating a destination-driven prediction strategy and a group of learnable prompt embeddings. Extensive experiments on popular benchmarks have demonstrated that our proposed approach achieves state-of-the-art performance with high efficiency. Code is available at https://github.com/iSEE-Laboratory/PPT.",
      "published_utc": "2024-07-16T10:48:18Z",
      "updated_utc": "2024-07-16T10:48:18Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.11588v1",
      "abs_url": "http://arxiv.org/abs/2407.11588v1"
    },
    "2407.05811v3": {
      "arxiv_id": "2407.05811v3",
      "title": "MapsTP: HD Map Images Based Multimodal Trajectory Prediction for Automated Vehicles",
      "authors": [
        "Sushil Sharma",
        "Arindam Das",
        "Ganesh Sistu",
        "Mark Halton",
        "Ciarán Eising"
      ],
      "summary": "Predicting ego vehicle trajectories remains a critical challenge, especially in urban and dense areas due to the unpredictable behaviours of other vehicles and pedestrians. Multimodal trajectory prediction enhances decision-making by considering multiple possible future trajectories based on diverse sources of environmental data. In this approach, we leverage ResNet-50 to extract image features from high-definition map data and use IMU sensor data to calculate speed, acceleration, and yaw rate. A temporal probabilistic network is employed to compute potential trajectories, selecting the most accurate and highly probable trajectory paths. This method integrates HD map data to improve the robustness and reliability of trajectory predictions for autonomous vehicles.",
      "published_utc": "2024-07-08T10:45:30Z",
      "updated_utc": "2024-10-01T09:18:10Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.05811v3",
      "abs_url": "http://arxiv.org/abs/2407.05811v3"
    },
    "2406.14746v3": {
      "arxiv_id": "2406.14746v3",
      "title": "Behavior-Inspired Neural Networks for Relational Inference",
      "authors": [
        "Yulong Yang",
        "Bowen Feng",
        "Keqin Wang",
        "Naomi Ehrich Leonard",
        "Adji Bousso Dieng",
        "Christine Allen-Blanchette"
      ],
      "summary": "From pedestrians to Kuramoto oscillators, interactions between agents govern how dynamical systems evolve in space and time. Discovering how these agents relate to each other has the potential to improve our understanding of the often complex dynamics that underlie these systems. Recent works learn to categorize relationships between agents based on observations of their physical behavior. These approaches model relationship categories as outcomes of a categorical distribution which is limiting and contrary to real-world systems, where relationship categories often intermingle and interact. In this work, we introduce a level of abstraction between the observable behavior of agents and the latent categories that determine their behavior. To do this, we learn a mapping from agent observations to agent preferences for a set of latent categories. The learned preferences and inter-agent proximity are integrated in a nonlinear opinion dynamics model, which allows us to naturally identify mutually exclusive categories, predict an agent's evolution in time, and control an agent's behavior. Through extensive experiments, we demonstrate the utility of our model for learning interpretable categories, and the efficacy of our model for long-horizon trajectory prediction.",
      "published_utc": "2024-06-20T21:36:54Z",
      "updated_utc": "2025-03-11T05:00:19Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.14746v3",
      "abs_url": "http://arxiv.org/abs/2406.14746v3"
    },
    "2406.14422v1": {
      "arxiv_id": "2406.14422v1",
      "title": "FutureNet-LOF: Joint Trajectory Prediction and Lane Occupancy Field Prediction with Future Context Encoding",
      "authors": [
        "Mingkun Wang",
        "Xiaoguang Ren",
        "Ruochun Jin",
        "Minglong Li",
        "Xiaochuan Zhang",
        "Changqian Yu",
        "Mingxu Wang",
        "Wenjing Yang"
      ],
      "summary": "Most prior motion prediction endeavors in autonomous driving have inadequately encoded future scenarios, leading to predictions that may fail to accurately capture the diverse movements of agents (e.g., vehicles or pedestrians). To address this, we propose FutureNet, which explicitly integrates initially predicted trajectories into the future scenario and further encodes these future contexts to enhance subsequent forecasting. Additionally, most previous motion forecasting works have focused on predicting independent futures for each agent. However, safe and smooth autonomous driving requires accurately predicting the diverse future behaviors of numerous surrounding agents jointly in complex dynamic environments. Given that all agents occupy certain potential travel spaces and possess lane driving priority, we propose Lane Occupancy Field (LOF), a new representation with lane semantics for motion forecasting in autonomous driving. LOF can simultaneously capture the joint probability distribution of all road participants' future spatial-temporal positions. Due to the high compatibility between lane occupancy field prediction and trajectory prediction, we propose a novel network with future context encoding for the joint prediction of these two tasks. Our approach ranks 1st on two large-scale motion forecasting benchmarks: Argoverse 1 and Argoverse 2.",
      "published_utc": "2024-06-20T15:41:53Z",
      "updated_utc": "2024-06-20T15:41:53Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.14422v1",
      "abs_url": "http://arxiv.org/abs/2406.14422v1"
    },
    "2406.00749v1": {
      "arxiv_id": "2406.00749v1",
      "title": "CCF: Cross Correcting Framework for Pedestrian Trajectory Prediction",
      "authors": [
        "Pranav Singh Chib",
        "Pravendra Singh"
      ],
      "summary": "Accurately predicting future pedestrian trajectories is crucial across various domains. Due to the uncertainty in future pedestrian trajectories, it is important to learn complex spatio-temporal representations in multi-agent scenarios. To address this, we propose a novel Cross-Correction Framework (CCF) to learn spatio-temporal representations of pedestrian trajectories better. Our framework consists of two trajectory prediction models, known as subnets, which share the same architecture and are trained with both cross-correction loss and trajectory prediction loss. Cross-correction leverages the learning from both subnets and enables them to refine their underlying representations of trajectories through a mutual correction mechanism. Specifically, we use the cross-correction loss to learn how to correct each other through an inter-subnet interaction. To induce diverse learning among the subnets, we use the transformed observed trajectories produced by a neural network as input to one subnet and the original observed trajectories as input to the other subnet. We utilize transformer-based encoder-decoder architecture for each subnet to capture motion and social interaction among pedestrians. The encoder of the transformer captures motion patterns in trajectories, while the decoder focuses on pedestrian interactions with neighbors. Each subnet performs the primary task of predicting future trajectories (a regression task) along with the secondary task of classifying the predicted trajectories (a classification task). Extensive experiments on real-world benchmark datasets such as ETH-UCY and SDD demonstrate the efficacy of our proposed framework, CCF, in precisely predicting pedestrian future trajectories. We also conducted several ablation experiments to demonstrate the effectiveness of various modules and loss functions used in our approach.",
      "published_utc": "2024-06-02T14:07:13Z",
      "updated_utc": "2024-06-02T14:07:13Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.00749v1",
      "abs_url": "http://arxiv.org/abs/2406.00749v1"
    },
    "2405.18945v1": {
      "arxiv_id": "2405.18945v1",
      "title": "WTTFNet: A Weather-Time-Trajectory Fusion Network for Pedestrian Trajectory Prediction in Urban Complex",
      "authors": [
        "Ho Chun Wu",
        "Esther Hoi Shan Lau",
        "Paul Yuen",
        "Kevin Hung",
        "John Kwok Tai Chui",
        "Andrew Kwok Fai Lui"
      ],
      "summary": "Pedestrian trajectory modelling in an urban complex is challenging because pedestrians can have many possible destinations, such as shops, escalators, and attractions. Moreover, weather and time-of-day may affect pedestrian behavior. In this paper, a new weather-time-trajectory fusion network (WTTFNet) is proposed to improve the performance of baseline deep neural network architecture. By incorporating weather and time-of-day information as an embedding structure, a novel WTTFNet based on gate multimodal unit is used to fuse the multimodal information and deep representation of trajectories. A joint loss function based on focal loss is used to co-optimize both the deep trajectory features and final classifier, which helps to improve the accuracy in predicting the intended destination of pedestrians and hence the trajectories under possible scenarios of class imbalances. Experimental results using the Osaka Asia and Pacific Trade Center (ATC) dataset shows improved performance of the proposed approach over state-of-the-art algorithms by 23.67% increase in classification accuracy, 9.16% and 7.07% reduction of average and final displacement error. The proposed approach may serve as an attractive approach for improving existing baseline trajectory prediction models when they are applied to scenarios with influences of weather-time conditions. It can be employed in numerous applications such as pedestrian facility engineering, public space development and technology-driven retail.",
      "published_utc": "2024-05-29T09:56:54Z",
      "updated_utc": "2024-05-29T09:56:54Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.18945v1",
      "abs_url": "http://arxiv.org/abs/2405.18945v1"
    },
    "2405.17680v2": {
      "arxiv_id": "2405.17680v2",
      "title": "Sports-Traj: A Unified Trajectory Generation Model for Multi-Agent Movement in Sports",
      "authors": [
        "Yi Xu",
        "Yun Fu"
      ],
      "summary": "Understanding multi-agent movement is critical across various fields. The conventional approaches typically focus on separate tasks such as trajectory prediction, imputation, or spatial-temporal recovery. Considering the unique formulation and constraint of each task, most existing methods are tailored for only one, limiting the ability to handle multiple tasks simultaneously, which is a common requirement in real-world scenarios. Another limitation is that widely used public datasets mainly focus on pedestrian movements with casual, loosely connected patterns, where interactions between individuals are not always present, especially at a long distance, making them less representative of more structured environments. To overcome these limitations, we propose a Unified Trajectory Generation model, UniTraj, that processes arbitrary trajectories as masked inputs, adaptable to diverse scenarios in the domain of sports games. Specifically, we introduce a Ghost Spatial Masking (GSM) module, embedded within a Transformer encoder, for spatial feature extraction. We further extend recent State Space Models (SSMs), known as the Mamba model, into a Bidirectional Temporal Mamba (BTM) to better capture temporal dependencies. Additionally, we incorporate a Bidirectional Temporal Scaled (BTS) module to thoroughly scan trajectories while preserving temporal missing relationships. Furthermore, we curate and benchmark three practical sports datasets, Basketball-U, Football-U, and Soccer-U, for evaluation. Extensive experiments demonstrate the superior performance of our model. We hope that our work can advance the understanding of human movement in real-world applications, particularly in sports. Our datasets, code, and model weights are available here https://github.com/colorfulfuture/UniTraj-pytorch.",
      "published_utc": "2024-05-27T22:15:23Z",
      "updated_utc": "2025-02-26T23:35:18Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.17680v2",
      "abs_url": "http://arxiv.org/abs/2405.17680v2"
    },
    "2405.07164v1": {
      "arxiv_id": "2405.07164v1",
      "title": "Modeling Pedestrian Intrinsic Uncertainty for Multimodal Stochastic Trajectory Prediction via Energy Plan Denoising",
      "authors": [
        "Yao Liu",
        "Quan Z. Sheng",
        "Lina Yao"
      ],
      "summary": "Pedestrian trajectory prediction plays a pivotal role in the realms of autonomous driving and smart cities. Despite extensive prior research employing sequence and generative models, the unpredictable nature of pedestrians, influenced by their social interactions and individual preferences, presents challenges marked by uncertainty and multimodality. In response, we propose the Energy Plan Denoising (EPD) model for stochastic trajectory prediction. EPD initially provides a coarse estimation of the distribution of future trajectories, termed the Plan, utilizing the Langevin Energy Model. Subsequently, it refines this estimation through denoising via the Probabilistic Diffusion Model. By initiating denoising with the Plan, EPD effectively reduces the need for iterative steps, thereby enhancing efficiency. Furthermore, EPD differs from conventional approaches by modeling the distribution of trajectories instead of individual trajectories. This allows for the explicit modeling of pedestrian intrinsic uncertainties and eliminates the need for multiple denoising operations. A single denoising operation produces a distribution from which multiple samples can be drawn, significantly enhancing efficiency. Moreover, EPD's fine-tuning of the Plan contributes to improved model performance. We validate EPD on two publicly available datasets, where it achieves state-of-the-art results. Additionally, ablation experiments underscore the contributions of individual modules, affirming the efficacy of the proposed approach.",
      "published_utc": "2024-05-12T05:11:23Z",
      "updated_utc": "2024-05-12T05:11:23Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.07164v1",
      "abs_url": "http://arxiv.org/abs/2405.07164v1"
    },
    "2412.20368v1": {
      "arxiv_id": "2412.20368v1",
      "title": "Subconscious Robotic Imitation Learning",
      "authors": [
        "Jun Xie",
        "Zhicheng Wang",
        "Jianwei Tan",
        "Huanxu Lin",
        "Xiaoguang Ma"
      ],
      "summary": "Although robotic imitation learning (RIL) is promising for embodied intelligent robots, existing RIL approaches rely on computationally intensive multi-model trajectory predictions, resulting in slow execution and limited real-time responsiveness. Instead, human beings subconscious can constantly process and store vast amounts of information from their experiences, perceptions, and learning, allowing them to fulfill complex actions such as riding a bike, without consciously thinking about each. Inspired by this phenomenon in action neurology, we introduced subconscious robotic imitation learning (SRIL), wherein cognitive offloading was combined with historical action chunkings to reduce delays caused by model inferences, thereby accelerating task execution. This process was further enhanced by subconscious downsampling and pattern augmented learning policy wherein intent-rich information was addressed with quantized sampling techniques to improve manipulation efficiency. Experimental results demonstrated that execution speeds of the SRIL were 100\\% to 200\\% faster over SOTA policies for comprehensive dual-arm tasks, with consistently higher success rates.",
      "published_utc": "2024-12-29T06:19:49Z",
      "updated_utc": "2024-12-29T06:19:49Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.20368v1",
      "abs_url": "http://arxiv.org/abs/2412.20368v1"
    },
    "2406.01431v4": {
      "arxiv_id": "2406.01431v4",
      "title": "Deep Stochastic Kinematic Models for Probabilistic Motion Forecasting in Traffic",
      "authors": [
        "Laura Zheng",
        "Sanghyun Son",
        "Jing Liang",
        "Xijun Wang",
        "Brian Clipp",
        "Ming C. Lin"
      ],
      "summary": "In trajectory forecasting tasks for traffic, future output trajectories can be computed by advancing the ego vehicle's state with predicted actions according to a kinematics model. By unrolling predicted trajectories via time integration and models of kinematic dynamics, predicted trajectories should not only be kinematically feasible but also relate uncertainty from one timestep to the next. While current works in probabilistic prediction do incorporate kinematic priors for mean trajectory prediction, _variance_ is often left as a learnable parameter, despite uncertainty in one time step being inextricably tied to uncertainty in the previous time step. In this paper, we show simple and differentiable analytical approximations describing the relationship between variance at one timestep and that at the next with the kinematic bicycle model. In our results, we find that encoding the relationship between variance across timesteps works especially well in unoptimal settings, such as with small or noisy datasets. We observe up to a 50% performance boost in partial dataset settings and up to an 8% performance boost in large-scale learning compared to previous kinematic prediction methods on SOTA trajectory forecasting architectures out-of-the-box, with no fine-tuning.",
      "published_utc": "2024-06-03T15:27:16Z",
      "updated_utc": "2024-09-06T21:31:23Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.01431v4",
      "abs_url": "http://arxiv.org/abs/2406.01431v4"
    },
    "2404.08363v3": {
      "arxiv_id": "2404.08363v3",
      "title": "Let-It-Flow: Simultaneous Optimization of 3D Flow and Object Clustering",
      "authors": [
        "Patrik Vacek",
        "David Hurych",
        "Tomáš Svoboda",
        "Karel Zimmermann"
      ],
      "summary": "We study the problem of self-supervised 3D scene flow estimation from real large-scale raw point cloud sequences, which is crucial to various tasks like trajectory prediction or instance segmentation. In the absence of ground truth scene flow labels, contemporary approaches concentrate on deducing optimizing flow across sequential pairs of point clouds by incorporating structure based regularization on flow and object rigidity. The rigid objects are estimated by a variety of 3D spatial clustering methods. While state-of-the-art methods successfully capture overall scene motion using the Neural Prior structure, they encounter challenges in discerning multi-object motions. We identified the structural constraints and the use of large and strict rigid clusters as the main pitfall of the current approaches and we propose a novel clustering approach that allows for combination of overlapping soft clusters as well as non-overlapping rigid clusters representation. Flow is then jointly estimated with progressively growing non-overlapping rigid clusters together with fixed size overlapping soft clusters. We evaluate our method on multiple datasets with LiDAR point clouds, demonstrating the superior performance over the self-supervised baselines reaching new state of the art results. Our method especially excels in resolving flow in complicated dynamic scenes with multiple independently moving objects close to each other which includes pedestrians, cyclists and other vulnerable road users. Our codes are publicly available on https://github.com/ctu-vras/let-it-flow.",
      "published_utc": "2024-04-12T10:04:03Z",
      "updated_utc": "2024-08-13T14:24:12Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.08363v3",
      "abs_url": "http://arxiv.org/abs/2404.08363v3"
    },
    "2403.16374v1": {
      "arxiv_id": "2403.16374v1",
      "title": "ProIn: Learning to Predict Trajectory Based on Progressive Interactions for Autonomous Driving",
      "authors": [
        "Yinke Dong",
        "Haifeng Yuan",
        "Hongkun Liu",
        "Wei Jing",
        "Fangzhen Li",
        "Hongmin Liu",
        "Bin Fan"
      ],
      "summary": "Accurate motion prediction of pedestrians, cyclists, and other surrounding vehicles (all called agents) is very important for autonomous driving. Most existing works capture map information through an one-stage interaction with map by vector-based attention, to provide map constraints for social interaction and multi-modal differentiation. However, these methods have to encode all required map rules into the focal agent's feature, so as to retain all possible intentions' paths while at the meantime to adapt to potential social interaction. In this work, a progressive interaction network is proposed to enable the agent's feature to progressively focus on relevant maps, in order to better learn agents' feature representation capturing the relevant map constraints. The network progressively encode the complex influence of map constraints into the agent's feature through graph convolutions at the following three stages: after historical trajectory encoder, after social interaction, and after multi-modal differentiation. In addition, a weight allocation mechanism is proposed for multi-modal training, so that each mode can obtain learning opportunities from a single-mode ground truth. Experiments have validated the superiority of progressive interactions to the existing one-stage interaction, and demonstrate the effectiveness of each component. Encouraging results were obtained in the challenging benchmarks.",
      "published_utc": "2024-03-25T02:38:34Z",
      "updated_utc": "2024-03-25T02:38:34Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CV",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.16374v1",
      "abs_url": "http://arxiv.org/abs/2403.16374v1"
    },
    "2312.16168v3": {
      "arxiv_id": "2312.16168v3",
      "title": "Social-Transmotion: Promptable Human Trajectory Prediction",
      "authors": [
        "Saeed Saadatnejad",
        "Yang Gao",
        "Kaouther Messaoud",
        "Alexandre Alahi"
      ],
      "summary": "Accurate human trajectory prediction is crucial for applications such as autonomous vehicles, robotics, and surveillance systems. Yet, existing models often fail to fully leverage the non-verbal social cues human subconsciously communicate when navigating the space. To address this, we introduce Social-Transmotion, a generic Transformer-based model that exploits diverse and numerous visual cues to predict human behavior. We translate the idea of a prompt from Natural Language Processing (NLP) to the task of human trajectory prediction, where a prompt can be a sequence of x-y coordinates on the ground, bounding boxes in the image plane, or body pose keypoints in either 2D or 3D. This, in turn, augments trajectory data, leading to enhanced human trajectory prediction. Using masking technique, our model exhibits flexibility and adaptability by capturing spatiotemporal interactions between agents based on the available visual cues. We delve into the merits of using 2D versus 3D poses, and a limited set of poses. Additionally, we investigate the spatial and temporal attention map to identify which keypoints and time-steps in the sequence are vital for optimizing human trajectory prediction. Our approach is validated on multiple datasets, including JTA, JRDB, Pedestrians and Cyclists in Road Traffic, and ETH-UCY. The code is publicly available: https://github.com/vita-epfl/social-transmotion.",
      "published_utc": "2023-12-26T18:56:49Z",
      "updated_utc": "2024-12-04T00:06:52Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.16168v3",
      "abs_url": "http://arxiv.org/abs/2312.16168v3"
    },
    "2311.00815v1": {
      "arxiv_id": "2311.00815v1",
      "title": "PIAug -- Physics Informed Augmentation for Learning Vehicle Dynamics for Off-Road Navigation",
      "authors": [
        "Parv Maheshwari",
        "Wenshan Wang",
        "Samuel Triest",
        "Matthew Sivaprakasam",
        "Shubhra Aich",
        "John G. Rogers",
        "Jason M. Gregory",
        "Sebastian Scherer"
      ],
      "summary": "Modeling the precise dynamics of off-road vehicles is a complex yet essential task due to the challenging terrain they encounter and the need for optimal performance and safety. Recently, there has been a focus on integrating nominal physics-based models alongside data-driven neural networks using Physics Informed Neural Networks. These approaches often assume the availability of a well-distributed dataset; however, this assumption may not hold due to regions in the physical distribution that are hard to collect, such as high-speed motions and rare terrains. Therefore, we introduce a physics-informed data augmentation methodology called PIAug. We show an example use case of the same by modeling high-speed and aggressive motion predictions, given a dataset with only low-speed data. During the training phase, we leverage the nominal model for generating target domain (medium and high velocity) data using the available source data (low velocity). Subsequently, we employ a physics-inspired loss function with this augmented dataset to incorporate prior knowledge of physics into the neural network. Our methodology results in up to 67% less mean error in trajectory prediction in comparison to a standalone nominal model, especially during aggressive maneuvers at speeds outside the training domain. In real-life navigation experiments, our model succeeds in 4x tighter waypoint tracking constraints than the Kinematic Bicycle Model (KBM) at out-of-domain velocities.",
      "published_utc": "2023-11-01T19:56:58Z",
      "updated_utc": "2023-11-01T19:56:58Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.00815v1",
      "abs_url": "http://arxiv.org/abs/2311.00815v1"
    },
    "2211.01696v5": {
      "arxiv_id": "2211.01696v5",
      "title": "An Empirical Bayes Analysis of Object Trajectory Representation Models",
      "authors": [
        "Yue Yao",
        "Daniel Goehring",
        "Joerg Reichardt"
      ],
      "summary": "Linear trajectory models provide mathematical advantages to autonomous driving applications such as motion prediction. However, linear models' expressive power and bias for real-world trajectories have not been thoroughly analyzed. We present an in-depth empirical analysis of the trade-off between model complexity and fit error in modelling object trajectories. We analyze vehicle, cyclist, and pedestrian trajectories. Our methodology estimates observation noise and prior distributions over model parameters from several large-scale datasets. Incorporating these priors can then regularize prediction models. Our results show that linear models do represent real-world trajectories with high fidelity at very moderate model complexity. This suggests the feasibility of using linear trajectory models in future motion prediction systems with inherent mathematical advantages.",
      "published_utc": "2022-11-03T10:39:41Z",
      "updated_utc": "2025-05-21T14:43:11Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.01696v5",
      "abs_url": "http://arxiv.org/abs/2211.01696v5"
    },
    "2206.15275v1": {
      "arxiv_id": "2206.15275v1",
      "title": "Multiclass-SGCN: Sparse Graph-based Trajectory Prediction with Agent Class Embedding",
      "authors": [
        "Ruochen Li",
        "Stamos Katsigiannis",
        "Hubert P. H. Shum"
      ],
      "summary": "Trajectory prediction of road users in real-world scenarios is challenging because their movement patterns are stochastic and complex. Previous pedestrian-oriented works have been successful in modelling the complex interactions among pedestrians, but fail in predicting trajectories when other types of road users are involved (e.g., cars, cyclists, etc.), because they ignore user types. Although a few recent works construct densely connected graphs with user label information, they suffer from superfluous spatial interactions and temporal dependencies. To address these issues, we propose Multiclass-SGCN, a sparse graph convolution network based approach for multi-class trajectory prediction that takes into consideration velocity and agent label information and uses a novel interaction mask to adaptively decide the spatial and temporal connections of agents based on their interaction scores. The proposed approach significantly outperformed state-of-the-art approaches on the Stanford Drone Dataset, providing more realistic and plausible trajectory predictions.",
      "published_utc": "2022-06-30T13:28:53Z",
      "updated_utc": "2022-06-30T13:28:53Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.15275v1",
      "abs_url": "http://arxiv.org/abs/2206.15275v1"
    },
    "2106.15991v1": {
      "arxiv_id": "2106.15991v1",
      "title": "Cyclist Trajectory Forecasts by Incorporation of Multi-View Video Information",
      "authors": [
        "Stefan Zernetsch",
        "Oliver Trupp",
        "Viktor Kress",
        "Konrad Doll",
        "Bernhard Sick"
      ],
      "summary": "This article presents a novel approach to incorporate visual cues from video-data from a wide-angle stereo camera system mounted at an urban intersection into the forecast of cyclist trajectories. We extract features from image and optical flow (OF) sequences using 3D convolutional neural networks (3D-ConvNet) and combine them with features extracted from the cyclist's past trajectory to forecast future cyclist positions. By the use of additional information, we are able to improve positional accuracy by about 7.5 % for our test dataset and by up to 22 % for specific motion types compared to a method solely based on past trajectories. Furthermore, we compare the use of image sequences to the use of OF sequences as additional information, showing that OF alone leads to significant improvements in positional accuracy. By training and testing our methods using a real-world dataset recorded at a heavily frequented public intersection and evaluating the methods' runtimes, we demonstrate the applicability in real traffic scenarios. Our code and parts of our dataset are made publicly available.",
      "published_utc": "2021-06-30T11:34:43Z",
      "updated_utc": "2021-06-30T11:34:43Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2106.15991v1",
      "abs_url": "http://arxiv.org/abs/2106.15991v1"
    },
    "2104.11212v1": {
      "arxiv_id": "2104.11212v1",
      "title": "Imagining The Road Ahead: Multi-Agent Trajectory Prediction via Differentiable Simulation",
      "authors": [
        "Adam Scibior",
        "Vasileios Lioutas",
        "Daniele Reda",
        "Peyman Bateni",
        "Frank Wood"
      ],
      "summary": "We develop a deep generative model built on a fully differentiable simulator for multi-agent trajectory prediction. Agents are modeled with conditional recurrent variational neural networks (CVRNNs), which take as input an ego-centric birdview image representing the current state of the world and output an action, consisting of steering and acceleration, which is used to derive the subsequent agent state using a kinematic bicycle model. The full simulation state is then differentiably rendered for each agent, initiating the next time step. We achieve state-of-the-art results on the INTERACTION dataset, using standard neural architectures and a standard variational training objective, producing realistic multi-modal predictions without any ad-hoc diversity-inducing losses. We conduct ablation studies to examine individual components of the simulator, finding that both the kinematic bicycle model and the continuous feedback from the birdview image are crucial for achieving this level of performance. We name our model ITRA, for \"Imagining the Road Ahead\".",
      "published_utc": "2021-04-22T17:48:08Z",
      "updated_utc": "2021-04-22T17:48:08Z",
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2104.11212v1",
      "abs_url": "http://arxiv.org/abs/2104.11212v1"
    },
    "2104.09176v1": {
      "arxiv_id": "2104.09176v1",
      "title": "Cyclist Intention Detection: A Probabilistic Approach",
      "authors": [
        "Stefan Zernetsch",
        "Hannes Reichert",
        "Viktor Kress",
        "Konrad Doll",
        "Bernhard Sick"
      ],
      "summary": "This article presents a holistic approach for probabilistic cyclist intention detection. A basic movement detection based on motion history images (MHI) and a residual convolutional neural network (ResNet) are used to estimate probabilities for the current cyclist motion state. These probabilities are used as weights in a probabilistic ensemble trajectory forecast. The ensemble consists of specialized models, which produce individual forecasts in the form of Gaussian distributions under the assumption of a certain motion state of the cyclist (e.g. cyclist is starting or turning left). By weighting the specialized models, we create forecasts in the from of Gaussian mixtures that define regions within which the cyclists will reside with a certain probability. To evaluate our method, we rate the reliability, sharpness, and positional accuracy of our forecasted distributions. We compare our method to a single model approach which produces forecasts in the form of Gaussian distributions and show that our method is able to produce more reliable and sharper outputs while retaining comparable positional accuracy. Both methods are evaluated using a dataset created at a public traffic intersection. Our code and the dataset are made publicly available.",
      "published_utc": "2021-04-19T09:59:04Z",
      "updated_utc": "2021-04-19T09:59:04Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2104.09176v1",
      "abs_url": "http://arxiv.org/abs/2104.09176v1"
    },
    "2102.09117v1": {
      "arxiv_id": "2102.09117v1",
      "title": "Spatio-Temporal Graph Dual-Attention Network for Multi-Agent Prediction and Tracking",
      "authors": [
        "Jiachen Li",
        "Hengbo Ma",
        "Zhihao Zhang",
        "Jinning Li",
        "Masayoshi Tomizuka"
      ],
      "summary": "An effective understanding of the environment and accurate trajectory prediction of surrounding dynamic obstacles are indispensable for intelligent mobile systems (e.g. autonomous vehicles and social robots) to achieve safe and high-quality planning when they navigate in highly interactive and crowded scenarios. Due to the existence of frequent interactions and uncertainty in the scene evolution, it is desired for the prediction system to enable relational reasoning on different entities and provide a distribution of future trajectories for each agent. In this paper, we propose a generic generative neural system (called STG-DAT) for multi-agent trajectory prediction involving heterogeneous agents. The system takes a step forward to explicit interaction modeling by incorporating relational inductive biases with a dynamic graph representation and leverages both trajectory and scene context information. We also employ an efficient kinematic constraint layer applied to vehicle trajectory prediction. The constraint not only ensures physical feasibility but also enhances model performance. Moreover, the proposed prediction model can be easily adopted by multi-target tracking frameworks. The tracking accuracy proves to be improved by empirical results. The proposed system is evaluated on three public benchmark datasets for trajectory prediction, where the agents cover pedestrians, cyclists and on-road vehicles. The experimental results demonstrate that our model achieves better performance than various baseline approaches in terms of prediction and tracking accuracy.",
      "published_utc": "2021-02-18T02:25:35Z",
      "updated_utc": "2021-02-18T02:25:35Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MA",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2102.09117v1",
      "abs_url": "http://arxiv.org/abs/2102.09117v1"
    },
    "2006.14480v2": {
      "arxiv_id": "2006.14480v2",
      "title": "One Thousand and One Hours: Self-driving Motion Prediction Dataset",
      "authors": [
        "John Houston",
        "Guido Zuidhof",
        "Luca Bergamini",
        "Yawei Ye",
        "Long Chen",
        "Ashesh Jain",
        "Sammy Omari",
        "Vladimir Iglovikov",
        "Peter Ondruska"
      ],
      "summary": "Motivated by the impact of large-scale datasets on ML systems we present the largest self-driving dataset for motion prediction to date, containing over 1,000 hours of data. This was collected by a fleet of 20 autonomous vehicles along a fixed route in Palo Alto, California, over a four-month period. It consists of 170,000 scenes, where each scene is 25 seconds long and captures the perception output of the self-driving system, which encodes the precise positions and motions of nearby vehicles, cyclists, and pedestrians over time. On top of this, the dataset contains a high-definition semantic map with 15,242 labelled elements and a high-definition aerial view over the area. We show that using a dataset of this size dramatically improves performance for key self-driving problems. Combined with the provided software kit, this collection forms the largest and most detailed dataset to date for the development of self-driving machine learning tasks, such as motion forecasting, motion planning and simulation. The full dataset is available at http://level5.lyft.com/.",
      "published_utc": "2020-06-25T15:23:41Z",
      "updated_utc": "2020-11-16T21:16:49Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2006.14480v2",
      "abs_url": "http://arxiv.org/abs/2006.14480v2"
    },
    "2004.12255v2": {
      "arxiv_id": "2004.12255v2",
      "title": "TPNet: Trajectory Proposal Network for Motion Prediction",
      "authors": [
        "Liangji Fang",
        "Qinhong Jiang",
        "Jianping Shi",
        "Bolei Zhou"
      ],
      "summary": "Making accurate motion prediction of the surrounding traffic agents such as pedestrians, vehicles, and cyclists is crucial for autonomous driving. Recent data-driven motion prediction methods have attempted to learn to directly regress the exact future position or its distribution from massive amount of trajectory data. However, it remains difficult for these methods to provide multimodal predictions as well as integrate physical constraints such as traffic rules and movable areas. In this work we propose a novel two-stage motion prediction framework, Trajectory Proposal Network (TPNet). TPNet first generates a candidate set of future trajectories as hypothesis proposals, then makes the final predictions by classifying and refining the proposals which meets the physical constraints. By steering the proposal generation process, safe and multimodal predictions are realized. Thus this framework effectively mitigates the complexity of motion prediction problem while ensuring the multimodal output. Experiments on four large-scale trajectory prediction datasets, i.e. the ETH, UCY, Apollo and Argoverse datasets, show that TPNet achieves the state-of-the-art results both quantitatively and qualitatively.",
      "published_utc": "2020-04-26T00:01:49Z",
      "updated_utc": "2021-02-07T08:04:58Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2004.12255v2",
      "abs_url": "http://arxiv.org/abs/2004.12255v2"
    },
    "2002.06241v1": {
      "arxiv_id": "2002.06241v1",
      "title": "Social-WaGDAT: Interaction-aware Trajectory Prediction via Wasserstein Graph Double-Attention Network",
      "authors": [
        "Jiachen Li",
        "Hengbo Ma",
        "Zhihao Zhang",
        "Masayoshi Tomizuka"
      ],
      "summary": "Effective understanding of the environment and accurate trajectory prediction of surrounding dynamic obstacles are indispensable for intelligent mobile systems (like autonomous vehicles and social robots) to achieve safe and high-quality planning when they navigate in highly interactive and crowded scenarios. Due to the existence of frequent interactions and uncertainty in the scene evolution, it is desired for the prediction system to enable relational reasoning on different entities and provide a distribution of future trajectories for each agent. In this paper, we propose a generic generative neural system (called Social-WaGDAT) for multi-agent trajectory prediction, which makes a step forward to explicit interaction modeling by incorporating relational inductive biases with a dynamic graph representation and leverages both trajectory and scene context information. We also employ an efficient kinematic constraint layer applied to vehicle trajectory prediction which not only ensures physical feasibility but also enhances model performance. The proposed system is evaluated on three public benchmark datasets for trajectory prediction, where the agents cover pedestrians, cyclists and on-road vehicles. The experimental results demonstrate that our model achieves better performance than various baseline approaches in terms of prediction accuracy.",
      "published_utc": "2020-02-14T20:11:13Z",
      "updated_utc": "2020-02-14T20:11:13Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG",
        "cs.MA",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2002.06241v1",
      "abs_url": "http://arxiv.org/abs/2002.06241v1"
    },
    "2002.05966v5": {
      "arxiv_id": "2002.05966v5",
      "title": "MCENET: Multi-Context Encoder Network for Homogeneous Agent Trajectory Prediction in Mixed Traffic",
      "authors": [
        "Hao Cheng",
        "Wentong Liao",
        "Michael Ying Yang",
        "Monika Sester",
        "Bodo Rosenhahn"
      ],
      "summary": "Trajectory prediction in urban mixed-traffic zones (a.k.a. shared spaces) is critical for many intelligent transportation systems, such as intent detection for autonomous driving. However, there are many challenges to predict the trajectories of heterogeneous road agents (pedestrians, cyclists and vehicles) at a microscopical level. For example, an agent might be able to choose multiple plausible paths in complex interactions with other agents in varying environments. To this end, we propose an approach named Multi-Context Encoder Network (MCENET) that is trained by encoding both past and future scene context, interaction context and motion information to capture the patterns and variations of the future trajectories using a set of stochastic latent variables. In inference time, we combine the past context and motion information of the target agent with samplings of the latent variables to predict multiple realistic trajectories in the future. Through experiments on several datasets of varying scenes, our method outperforms some of the recent state-of-the-art methods for mixed traffic trajectory prediction by a large margin and more robust in a very challenging environment. The impact of each context is justified via ablation studies.",
      "published_utc": "2020-02-14T11:04:41Z",
      "updated_utc": "2020-06-23T13:06:17Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.CY",
        "cs.MA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2002.05966v5",
      "abs_url": "http://arxiv.org/abs/2002.05966v5"
    },
    "1909.13486v2": {
      "arxiv_id": "1909.13486v2",
      "title": "Predicting Responses to a Robot's Future Motion using Generative Recurrent Neural Networks",
      "authors": [
        "Stuart Eiffert",
        "Salah Sukkarieh"
      ],
      "summary": "Robotic navigation through crowds or herds requires the ability to both predict the future motion of nearby individuals and understand how these predictions might change in response to a robot's future action. State of the art trajectory prediction models using Recurrent Neural Networks (RNNs) do not currently account for a planned future action of a robot, and so cannot predict how an individual will move in response to a robot's planned path. We propose an approach that adapts RNNs to use a robot's next planned action as an input alongside the current position of nearby individuals. This allows the model to learn the response of individuals with regards to a robot's motion from real world observations. By linking a robot's actions to the response of those around it in training, we show that we are able to not only improve prediction accuracy in close range interactions, but also to predict the likely response of surrounding individuals to simulated actions. This allows the use of the model to simulate state transitions, without requiring any assumptions on agent interaction. We apply this model to varied datasets, including crowds of pedestrians interacting with vehicles and bicycles, and livestock interacting with a robotic vehicle.",
      "published_utc": "2019-09-30T07:15:29Z",
      "updated_utc": "2020-01-28T00:36:01Z",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/1909.13486v2",
      "abs_url": "http://arxiv.org/abs/1909.13486v2"
    },
    "1907.08752v1": {
      "arxiv_id": "1907.08752v1",
      "title": "RobustTP: End-to-End Trajectory Prediction for Heterogeneous Road-Agents in Dense Traffic with Noisy Sensor Inputs",
      "authors": [
        "Rohan Chandra",
        "Uttaran Bhattacharya",
        "Christian Roncal",
        "Aniket Bera",
        "Dinesh Manocha"
      ],
      "summary": "We present RobustTP, an end-to-end algorithm for predicting future trajectories of road-agents in dense traffic with noisy sensor input trajectories obtained from RGB cameras (either static or moving) through a tracking algorithm. In this case, we consider noise as the deviation from the ground truth trajectory. The amount of noise depends on the accuracy of the tracking algorithm. Our approach is designed for dense heterogeneous traffic, where the road agents corresponding to a mixture of buses, cars, scooters, bicycles, or pedestrians. RobustTP is an approach that first computes trajectories using a combination of a non-linear motion model and a deep learning-based instance segmentation algorithm. Next, these noisy trajectories are trained using an LSTM-CNN neural network architecture that models the interactions between road-agents in dense and heterogeneous traffic. Our trajectory prediction algorithm outperforms state-of-the-art methods for end-to-end trajectory prediction using sensor inputs. We achieve an improvement of upto 18% in average displacement error and an improvement ofup to 35.5% in final displacement error at the end of the prediction window (5 seconds) over the next best method. All experiments were set up on an Nvidia TiTan Xp GPU. Additionally, we release a software framework, TrackNPred. The framework consists of implementations of state-of-the-art tracking and trajectory prediction methods and tools to benchmark and evaluate them on real-world dense traffic datasets.",
      "published_utc": "2019-07-20T04:21:22Z",
      "updated_utc": "2019-07-20T04:21:22Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/1907.08752v1",
      "abs_url": "http://arxiv.org/abs/1907.08752v1"
    },
    "1907.12887v5": {
      "arxiv_id": "1907.12887v5",
      "title": "End-to-end Recurrent Multi-Object Tracking and Trajectory Prediction with Relational Reasoning",
      "authors": [
        "Fabian B. Fuchs",
        "Adam R. Kosiorek",
        "Li Sun",
        "Oiwi Parker Jones",
        "Ingmar Posner"
      ],
      "summary": "The majority of contemporary object-tracking approaches do not model interactions between objects. This contrasts with the fact that objects' paths are not independent: a cyclist might abruptly deviate from a previously planned trajectory in order to avoid colliding with a car. Building upon HART, a neural class-agnostic single-object tracker, we introduce a multi-object tracking method MOHART capable of relational reasoning. Importantly, the entire system, including the understanding of interactions and relations between objects, is class-agnostic and learned simultaneously in an end-to-end fashion. We explore a number of relational reasoning architectures and show that permutation-invariant models outperform non-permutation-invariant alternatives. We also find that architectures using a single permutation invariant operation like DeepSets, despite, in theory, being universal function approximators, are nonetheless outperformed by a more complex architecture based on multi-headed attention. The latter better accounts for complex physical interactions in a challenging toy experiment. Further, we find that modelling interactions leads to consistent performance gains in tracking as well as future trajectory prediction on three real-world datasets (MOTChallenge, UA-DETRAC, and Stanford Drone dataset), particularly in the presence of ego-motion, occlusions, crowded scenes, and faulty sensor inputs.",
      "published_utc": "2019-07-12T22:40:13Z",
      "updated_utc": "2020-09-28T14:25:23Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/1907.12887v5",
      "abs_url": "http://arxiv.org/abs/1907.12887v5"
    },
    "1907.03395v2": {
      "arxiv_id": "1907.03395v2",
      "title": "Social-BiGAT: Multimodal Trajectory Forecasting using Bicycle-GAN and Graph Attention Networks",
      "authors": [
        "Vineet Kosaraju",
        "Amir Sadeghian",
        "Roberto Martín-Martín",
        "Ian Reid",
        "S. Hamid Rezatofighi",
        "Silvio Savarese"
      ],
      "summary": "Predicting the future trajectories of multiple interacting agents in a scene has become an increasingly important problem for many different applications ranging from control of autonomous vehicles and social robots to security and surveillance. This problem is compounded by the presence of social interactions between humans and their physical interactions with the scene. While the existing literature has explored some of these cues, they mainly ignored the multimodal nature of each human's future trajectory. In this paper, we present Social-BiGAT, a graph-based generative adversarial network that generates realistic, multimodal trajectory predictions by better modelling the social interactions of pedestrians in a scene. Our method is based on a graph attention network (GAT) that learns reliable feature representations that encode the social interactions between humans in the scene, and a recurrent encoder-decoder architecture that is trained adversarially to predict, based on the features, the humans' paths. We explicitly account for the multimodal nature of the prediction problem by forming a reversible transformation between each scene and its latent noise vector, as in Bicycle-GAN. We show that our framework achieves state-of-the-art performance comparing it to several baselines on existing trajectory forecasting benchmarks.",
      "published_utc": "2019-07-04T23:48:07Z",
      "updated_utc": "2019-07-17T01:05:26Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/1907.03395v2",
      "abs_url": "http://arxiv.org/abs/1907.03395v2"
    },
    "1902.10928v4": {
      "arxiv_id": "1902.10928v4",
      "title": "Interaction-aware Kalman Neural Networks for Trajectory Prediction",
      "authors": [
        "Ce Ju",
        "Zheng Wang",
        "Cheng Long",
        "Xiaoyu Zhang",
        "Dong Eui Chang"
      ],
      "summary": "Forecasting the motion of surrounding obstacles (vehicles, bicycles, pedestrians and etc.) benefits the on-road motion planning for intelligent and autonomous vehicles. Complex scenes always yield great challenges in modeling the patterns of surrounding traffic. For example, one main challenge comes from the intractable interaction effects in a complex traffic system. In this paper, we propose a multi-layer architecture Interaction-aware Kalman Neural Networks (IaKNN) which involves an interaction layer for resolving high-dimensional traffic environmental observations as interaction-aware accelerations, a motion layer for transforming the accelerations to interaction aware trajectories, and a filter layer for estimating future trajectories with a Kalman filter network. Attributed to the multiple traffic data sources, our end-to-end trainable approach technically fuses dynamic and interaction-aware trajectories boosting the prediction performance. Experiments on the NGSIM dataset demonstrate that IaKNN outperforms the state-of-the-art methods in terms of effectiveness for traffic trajectory prediction.",
      "published_utc": "2019-02-28T07:10:30Z",
      "updated_utc": "2021-01-25T16:59:51Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/1902.10928v4",
      "abs_url": "http://arxiv.org/abs/1902.10928v4"
    },
    "1812.04767v4": {
      "arxiv_id": "1812.04767v4",
      "title": "TraPHic: Trajectory Prediction in Dense and Heterogeneous Traffic Using Weighted Interactions",
      "authors": [
        "Rohan Chandra",
        "Uttaran Bhattacharya",
        "Aniket Bera",
        "Dinesh Manocha"
      ],
      "summary": "We present a new algorithm for predicting the near-term trajectories of road-agents in dense traffic videos. Our approach is designed for heterogeneous traffic, where the road-agents may correspond to buses, cars, scooters, bicycles, or pedestrians. We model the interactions between different road-agents using a novel LSTM-CNN hybrid network for trajectory prediction. In particular, we take into account heterogeneous interactions that implicitly accounts for the varying shapes, dynamics, and behaviors of different road agents. In addition, we model horizon-based interactions which are used to implicitly model the driving behavior of each road-agent. We evaluate the performance of our prediction algorithm, TraPHic, on the standard datasets and also introduce a new dense, heterogeneous traffic dataset corresponding to urban Asian videos and agent trajectories. We outperform state-of-the-art methods on dense traffic datasets by 30%.",
      "published_utc": "2018-12-12T01:36:50Z",
      "updated_utc": "2021-07-31T16:08:26Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/1812.04767v4",
      "abs_url": "http://arxiv.org/abs/1812.04767v4"
    },
    "1811.02146v5": {
      "arxiv_id": "1811.02146v5",
      "title": "TrafficPredict: Trajectory Prediction for Heterogeneous Traffic-Agents",
      "authors": [
        "Yuexin Ma",
        "Xinge Zhu",
        "Sibo Zhang",
        "Ruigang Yang",
        "Wenping Wang",
        "Dinesh Manocha"
      ],
      "summary": "To safely and efficiently navigate in complex urban traffic, autonomous vehicles must make responsible predictions in relation to surrounding traffic-agents (vehicles, bicycles, pedestrians, etc.). A challenging and critical task is to explore the movement patterns of different traffic-agents and predict their future trajectories accurately to help the autonomous vehicle make reasonable navigation decision. To solve this problem, we propose a long short-term memory-based (LSTM-based) realtime traffic prediction algorithm, TrafficPredict. Our approach uses an instance layer to learn instances' movements and interactions and has a category layer to learn the similarities of instances belonging to the same type to refine the prediction. In order to evaluate its performance, we collected trajectory datasets in a large city consisting of varying conditions and traffic densities. The dataset includes many challenging scenarios where vehicles, bicycles, and pedestrians move among one another. We evaluate the performance of TrafficPredict on our new dataset and highlight its higher accuracy for trajectory prediction by comparing with prior prediction methods.",
      "published_utc": "2018-11-06T03:34:20Z",
      "updated_utc": "2019-04-09T07:08:02Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/1811.02146v5",
      "abs_url": "http://arxiv.org/abs/1811.02146v5"
    },
    "1601.00998v1": {
      "arxiv_id": "1601.00998v1",
      "title": "Forecasting Social Navigation in Crowded Complex Scenes",
      "authors": [
        "Alexandre Robicquet",
        "Alexandre Alahi",
        "Amir Sadeghian",
        "Bryan Anenberg",
        "John Doherty",
        "Eli Wu",
        "Silvio Savarese"
      ],
      "summary": "When humans navigate a crowed space such as a university campus or the sidewalks of a busy street, they follow common sense rules based on social etiquette. In this paper, we argue that in order to enable the design of new algorithms that can take fully advantage of these rules to better solve tasks such as target tracking or trajectory forecasting, we need to have access to better data in the first place. To that end, we contribute the very first large scale dataset (to the best of our knowledge) that collects images and videos of various types of targets (not just pedestrians, but also bikers, skateboarders, cars, buses, golf carts) that navigate in a real-world outdoor environment such as a university campus. We present an extensive evaluation where different methods for trajectory forecasting are evaluated and compared. Moreover, we present a new algorithm for trajectory prediction that exploits the complexity of our new dataset and allows to: i) incorporate inter-class interactions into trajectory prediction models (e.g, pedestrian vs bike) as opposed to just intra-class interactions (e.g., pedestrian vs pedestrian); ii) model the degree to which the social forces are regulating an interaction. We call the latter \"social sensitivity\"and it captures the sensitivity to which a target is responding to a certain interaction. An extensive experimental evaluation demonstrates the effectiveness of our novel approach.",
      "published_utc": "2016-01-05T22:10:15Z",
      "updated_utc": "2016-01-05T22:10:15Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.RO",
        "cs.SI"
      ],
      "pdf_url": "https://arxiv.org/pdf/1601.00998v1",
      "abs_url": "http://arxiv.org/abs/1601.00998v1"
    },
    "2601.01547v1": {
      "arxiv_id": "2601.01547v1",
      "title": "EscherVerse: An Open World Benchmark and Dataset for Teleo-Spatial Intelligence with Physical-Dynamic and Intent-Driven Understanding",
      "authors": [
        "Tianjun Gu",
        "Chenghua Gong",
        "Jingyu Gong",
        "Zhizhong Zhang",
        "Yuan Xie",
        "Lizhuang Ma",
        "Xin Tan"
      ],
      "summary": "The ability to reason about spatial dynamics is a cornerstone of intelligence, yet current research overlooks the human intent behind spatial changes. To address these limitations, we introduce Teleo-Spatial Intelligence (TSI), a new paradigm that unifies two critical pillars: Physical-Dynamic Reasoning--understanding the physical principles of object interactions--and Intent-Driven Reasoning--inferring the human goals behind these actions. To catalyze research in TSI, we present EscherVerse, consisting of a large-scale, open-world benchmark (Escher-Bench), a dataset (Escher-35k), and models (Escher series). Derived from real-world videos, EscherVerse moves beyond constrained settings to explicitly evaluate an agent's ability to reason about object permanence, state transitions, and trajectory prediction in dynamic, human-centric scenarios. Crucially, it is the first benchmark to systematically assess Intent-Driven Reasoning, challenging models to connect physical events to their underlying human purposes. Our work, including a novel data curation pipeline, provides a foundational resource to advance spatial intelligence from passive scene description toward a holistic, purpose-driven understanding of the world.",
      "published_utc": "2026-01-04T14:42:39Z",
      "updated_utc": "2026-01-04T14:42:39Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2601.01547v1",
      "abs_url": "http://arxiv.org/abs/2601.01547v1"
    },
    "2512.21133v1": {
      "arxiv_id": "2512.21133v1",
      "title": "SparScene: Efficient Traffic Scene Representation via Sparse Graph Learning for Large-Scale Trajectory Generation",
      "authors": [
        "Xiaoyu Mo",
        "Jintian Ge",
        "Zifan Wang",
        "Chen Lv",
        "Karl Henrik Johansson"
      ],
      "summary": "Multi-agent trajectory generation is a core problem for autonomous driving and intelligent transportation systems. However, efficiently modeling the dynamic interactions between numerous road users and infrastructures in complex scenes remains an open problem. Existing methods typically employ distance-based or fully connected dense graph structures to capture interaction information, which not only introduces a large number of redundant edges but also requires complex and heavily parameterized networks for encoding, thereby resulting in low training and inference efficiency, limiting scalability to large and complex traffic scenes. To overcome the limitations of existing methods, we propose SparScene, a sparse graph learning framework designed for efficient and scalable traffic scene representation. Instead of relying on distance thresholds, SparScene leverages the lane graph topology to construct structure-aware sparse connections between agents and lanes, enabling efficient yet informative scene graph representation. SparScene adopts a lightweight graph encoder that efficiently aggregates agent-map and agent-agent interactions, yielding compact scene representations with substantially improved efficiency and scalability. On the motion prediction benchmark of the Waymo Open Motion Dataset (WOMD), SparScene achieves competitive performance with remarkable efficiency. It generates trajectories for more than 200 agents in a scene within 5 ms and scales to more than 5,000 agents and 17,000 lanes with merely 54 ms of inference time with a GPU memory of 2.9 GB, highlighting its superior scalability for large-scale traffic scenes.",
      "published_utc": "2025-12-24T12:02:35Z",
      "updated_utc": "2025-12-24T12:02:35Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2512.21133v1",
      "abs_url": "http://arxiv.org/abs/2512.21133v1"
    },
    "2512.16907v2": {
      "arxiv_id": "2512.16907v2",
      "title": "Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos",
      "authors": [
        "Mingfei Chen",
        "Yifan Wang",
        "Zhengqin Li",
        "Homanga Bharadhwaj",
        "Yujin Chen",
        "Chuan Qin",
        "Ziyi Kou",
        "Yuan Tian",
        "Eric Whitmire",
        "Rajinder Sodhi",
        "Hrvoje Benko",
        "Eli Shlizerman",
        "Yue Liu"
      ],
      "summary": "Prior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface. Trained progressively to align reasoning with motion dynamics, our approach yields accurate and stage-aware trajectories with generalization across real-world scenes.",
      "published_utc": "2025-12-18T18:59:01Z",
      "updated_utc": "2025-12-30T21:52:53Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2512.16907v2",
      "abs_url": "http://arxiv.org/abs/2512.16907v2"
    },
    "2512.13903v1": {
      "arxiv_id": "2512.13903v1",
      "title": "PrediFlow: A Flow-Based Prediction-Refinement Framework for Real-Time Human Motion Prediction in Human-Robot Collaboration",
      "authors": [
        "Sibo Tian",
        "Minghui Zheng",
        "Xiao Liang"
      ],
      "summary": "Stochastic human motion prediction is critical for safe and effective human-robot collaboration (HRC) in industrial remanufacturing, as it captures human motion uncertainties and multi-modal behaviors that deterministic methods cannot handle. While earlier works emphasize highly diverse predictions, they often generate unrealistic human motions. More recent methods focus on accuracy and real-time performance, yet there remains potential to improve prediction quality further without exceeding time budgets. Additionally, current research on stochastic human motion prediction in HRC typically considers human motion in isolation, neglecting the influence of robot motion on human behavior. To address these research gaps and enable real-time, realistic, and interaction-aware human motion prediction, we propose a novel prediction-refinement framework that integrates both human and robot observed motion to refine the initial predictions produced by a pretrained state-of-the-art predictor. The refinement module employs a Flow Matching structure to account for uncertainty. Experimental studies on the HRC desktop disassembly dataset demonstrate that our method significantly improves prediction accuracy while preserving the uncertainties and multi-modalities of human motion. Moreover, the total inference time of the proposed framework remains within the time budget, highlighting the effectiveness and practicality of our approach.",
      "published_utc": "2025-12-15T21:20:11Z",
      "updated_utc": "2025-12-15T21:20:11Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2512.13903v1",
      "abs_url": "http://arxiv.org/abs/2512.13903v1"
    },
    "2512.12717v1": {
      "arxiv_id": "2512.12717v1",
      "title": "HMPCC: Human-Aware Model Predictive Coverage Control",
      "authors": [
        "Mattia Catellani",
        "Marta Gabbi",
        "Lorenzo Sabattini"
      ],
      "summary": "We address the problem of coordinating a team of robots to cover an unknown environment while ensuring safe operation and avoiding collisions with non-cooperative agents. Traditional coverage strategies often rely on simplified assumptions, such as known or convex environments and static density functions, and struggle to adapt to real-world scenarios, especially when humans are involved. In this work, we propose a human-aware coverage framework based on Model Predictive Control (MPC), namely HMPCC, where human motion predictions are integrated into the planning process. By anticipating human trajectories within the MPC horizon, robots can proactively coordinate their actions %avoid redundant exploration, and adapt to dynamic conditions. The environment is modeled as a Gaussian Mixture Model (GMM), representing regions of interest. Team members operate in a fully decentralized manner, without relying on explicit communication, an essential feature in hostile or communication-limited scenarios. Our results show that human trajectory forecasting enables more efficient and adaptive coverage, improving coordination between human and robotic agents.",
      "published_utc": "2025-12-14T14:41:36Z",
      "updated_utc": "2025-12-14T14:41:36Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2512.12717v1",
      "abs_url": "http://arxiv.org/abs/2512.12717v1"
    },
    "2512.12211v1": {
      "arxiv_id": "2512.12211v1",
      "title": "Measuring What Matters: Scenario-Driven Evaluation for Trajectory Predictors in Autonomous Driving",
      "authors": [
        "Longchao Da",
        "David Isele",
        "Hua Wei",
        "Manish Saroya"
      ],
      "summary": "Being able to anticipate the motion of surrounding agents is essential for the safe operation of autonomous driving systems in dynamic situations. While various methods have been proposed for trajectory prediction, the current evaluation practices still rely on error-based metrics (e.g., ADE, FDE), which reveal the accuracy from a post-hoc view but ignore the actual effect the predictor brings to the self-driving vehicles (SDVs), especially in complex interactive scenarios: a high-quality predictor not only chases accuracy, but should also captures all possible directions a neighbor agent might move, to support the SDVs' cautious decision-making. Given that the existing metrics hardly account for this standard, in our work, we propose a comprehensive pipeline that adaptively evaluates the predictor's performance by two dimensions: accuracy and diversity. Based on the criticality of the driving scenario, these two dimensions are dynamically combined and result in a final score for the predictor's performance. Extensive experiments on a closed-loop benchmark using real-world datasets show that our pipeline yields a more reasonable evaluation than traditional metrics by better reflecting the correlation of the predictors' evaluation with the autonomous vehicles' driving performance. This evaluation pipeline shows a robust way to select a predictor that potentially contributes most to the SDV's driving performance.",
      "published_utc": "2025-12-13T06:48:32Z",
      "updated_utc": "2025-12-13T06:48:32Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2512.12211v1",
      "abs_url": "http://arxiv.org/abs/2512.12211v1"
    },
    "2512.05270v1": {
      "arxiv_id": "2512.05270v1",
      "title": "XR-DT: Extended Reality-Enhanced Digital Twin for Agentic Mobile Robots",
      "authors": [
        "Tianyi Wang",
        "Jiseop Byeon",
        "Ahmad Yehia",
        "Huihai Wang",
        "Yiming Xu",
        "Tianyi Zeng",
        "Ziran Wang",
        "Junfeng Jiao",
        "Christian Claudel"
      ],
      "summary": "As mobile robots increasingly operate alongside humans in shared workspaces, ensuring safe, efficient, and interpretable Human-Robot Interaction (HRI) has become a pressing challenge. While substantial progress has been devoted to human behavior prediction, limited attention has been paid to how humans perceive, interpret, and trust robots' inferences, impeding deployment in safety-critical and socially embedded environments. This paper presents XR-DT, an eXtended Reality-enhanced Digital Twin framework for agentic mobile robots, that bridges physical and virtual spaces to enable bi-directional understanding between humans and robots. Our hierarchical XR-DT architecture integrates virtual-, augmented-, and mixed-reality layers, fusing real-time sensor data, simulated environments in the Unity game engine, and human feedback captured through wearable AR devices. Within this framework, we design an agentic mobile robot system with a unified diffusion policy for context-aware task adaptation. We further propose a chain-of-thought prompting mechanism that allows multimodal large language models to reason over human instructions and environmental context, while leveraging an AutoGen-based multi-agent coordination layer to enhance robustness and collaboration in dynamic tasks. Initial experimental results demonstrate accurate human and robot trajectory prediction, validating the XR-DT framework's effectiveness in HRI tasks. By embedding human intention, environmental dynamics, and robot cognition into the XR-DT framework, our system enables interpretable, trustworthy, and adaptive HRI.",
      "published_utc": "2025-12-04T21:49:14Z",
      "updated_utc": "2025-12-04T21:49:14Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC",
        "cs.MA",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2512.05270v1",
      "abs_url": "http://arxiv.org/abs/2512.05270v1"
    },
    "2512.05008v1": {
      "arxiv_id": "2512.05008v1",
      "title": "Contact-Implicit Modeling and Simulation of a Snake Robot on Compliant and Granular Terrain",
      "authors": [
        "Haroon Hublikar"
      ],
      "summary": "This thesis presents a unified modeling and simulation framework for analyzing sidewinding and tumbling locomotion of the COBRA snake robot across rigid, compliant, and granular terrains. A contact-implicit formulation is used to model distributed frictional interactions during sidewinding, and validated through MATLAB Simscape simulations and physical experiments on rigid ground and loose sand. To capture terrain deformation effects, Project Chrono's Soil Contact Model (SCM) is integrated with the articulated multibody dynamics, enabling prediction of slip, sinkage, and load redistribution that reduce stride efficiency on deformable substrates. For high-energy rolling locomotion on steep slopes, the Chrono DEM Engine is used to simulate particle-resolved granular interactions, revealing soil failure, intermittent lift-off, and energy dissipation mechanisms not captured by rigid models. Together, these methods span real-time control-oriented simulation and high-fidelity granular physics. Results demonstrate that rigid-ground models provide accurate short-horizon motion prediction, while continuum and particle-based terrain modeling becomes necessary for reliable mobility analysis in soft and highly dynamic environments. This work establishes a hierarchical simulation pipeline that advances robust, terrain-aware locomotion for robots operating in challenging unstructured settings.",
      "published_utc": "2025-12-04T17:20:32Z",
      "updated_utc": "2025-12-04T17:20:32Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2512.05008v1",
      "abs_url": "http://arxiv.org/abs/2512.05008v1"
    },
    "2512.03936v1": {
      "arxiv_id": "2512.03936v1",
      "title": "Driving is a Game: Combining Planning and Prediction with Bayesian Iterative Best Response",
      "authors": [
        "Aron Distelzweig",
        "Yiwei Wang",
        "Faris Janjoš",
        "Marcel Hallgarten",
        "Mihai Dobre",
        "Alexander Langmann",
        "Joschka Boedecker",
        "Johannes Betz"
      ],
      "summary": "Autonomous driving planning systems perform nearly perfectly in routine scenarios using lightweight, rule-based methods but still struggle in dense urban traffic, where lane changes and merges require anticipating and influencing other agents. Modern motion predictors offer highly accurate forecasts, yet their integration into planning is mostly rudimental: discarding unsafe plans. Similarly, end-to-end models offer a one-way integration that avoids the challenges of joint prediction and planning modeling under uncertainty. In contrast, game-theoretic formulations offer a principled alternative but have seen limited adoption in autonomous driving. We present Bayesian Iterative Best Response (BIBeR), a framework that unifies motion prediction and game-theoretic planning into a single interaction-aware process. BIBeR is the first to integrate a state-of-the-art predictor into an Iterative Best Response (IBR) loop, repeatedly refining the strategies of the ego vehicle and surrounding agents. This repeated best-response process approximates a Nash equilibrium, enabling bidirectional adaptation where the ego both reacts to and shapes the behavior of others. In addition, our proposed Bayesian confidence estimation quantifies prediction reliability and modulates update strength, more conservative under low confidence and more decisive under high confidence. BIBeR is compatible with modern predictors and planners, combining the transparency of structured planning with the flexibility of learned models. Experiments show that BIBeR achieves an 11% improvement over state-of-the-art planners on highly interactive interPlan lane-change scenarios, while also outperforming existing approaches on standard nuPlan benchmarks.",
      "published_utc": "2025-12-03T16:33:25Z",
      "updated_utc": "2025-12-03T16:33:25Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2512.03936v1",
      "abs_url": "http://arxiv.org/abs/2512.03936v1"
    },
    "2512.03795v1": {
      "arxiv_id": "2512.03795v1",
      "title": "MPCFormer: A physics-informed data-driven approach for explainable socially-aware autonomous driving",
      "authors": [
        "Jia Hu",
        "Zhexi Lian",
        "Xuerun Yan",
        "Ruiang Bi",
        "Dou Shen",
        "Yu Ruan",
        "Haoran Wang"
      ],
      "summary": "Autonomous Driving (AD) vehicles still struggle to exhibit human-like behavior in highly dynamic and interactive traffic scenarios. The key challenge lies in AD's limited ability to interact with surrounding vehicles, largely due to a lack of understanding the underlying mechanisms of social interaction. To address this issue, we introduce MPCFormer, an explainable socially-aware autonomous driving approach with physics-informed and data-driven coupled social interaction dynamics. In this model, the dynamics are formulated into a discrete space-state representation, which embeds physics priors to enhance modeling explainability. The dynamics coefficients are learned from naturalistic driving data via a Transformer-based encoder-decoder architecture. To the best of our knowledge, MPCFormer is the first approach to explicitly model the dynamics of multi-vehicle social interactions. The learned social interaction dynamics enable the planner to generate manifold, human-like behaviors when interacting with surrounding traffic. By leveraging the MPC framework, the approach mitigates the potential safety risks typically associated with purely learning-based methods. Open-looped evaluation on NGSIM dataset demonstrates that MPCFormer achieves superior social interaction awareness, yielding the lowest trajectory prediction errors compared with other state-of-the-art approach. The prediction achieves an ADE as low as 0.86 m over a long prediction horizon of 5 seconds. Close-looped experiments in highly intense interaction scenarios, where consecutive lane changes are required to exit an off-ramp, further validate the effectiveness of MPCFormer. Results show that MPCFormer achieves the highest planning success rate of 94.67%, improves driving efficiency by 15.75%, and reduces the collision rate from 21.25% to 0.5%, outperforming a frontier Reinforcement Learning (RL) based planner.",
      "published_utc": "2025-12-03T13:43:33Z",
      "updated_utc": "2025-12-03T13:43:33Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2512.03795v1",
      "abs_url": "http://arxiv.org/abs/2512.03795v1"
    },
    "2512.03756v1": {
      "arxiv_id": "2512.03756v1",
      "title": "Prediction-Driven Motion Planning: Route Integration Strategies in Attention-Based Prediction Models",
      "authors": [
        "Marlon Steiner",
        "Royden Wagner",
        "Ömer Sahin Tas",
        "Christoph Stiller"
      ],
      "summary": "Combining motion prediction and motion planning offers a promising framework for enhancing interactions between automated vehicles and other traffic participants. However, this introduces challenges in conditioning predictions on navigation goals and ensuring stable, kinematically feasible trajectories. Addressing the former challenge, this paper investigates the extension of attention-based motion prediction models with navigation information. By integrating the ego vehicle's intended route and goal pose into the model architecture, we bridge the gap between multi-agent motion prediction and goal-based motion planning. We propose and evaluate several architectural navigation integration strategies to our model on the nuPlan dataset. Our results demonstrate the potential of prediction-driven motion planning, highlighting how navigation information can enhance both prediction and planning tasks. Our implementation is at: https://github.com/KIT-MRT/future-motion.",
      "published_utc": "2025-12-03T12:57:03Z",
      "updated_utc": "2025-12-03T12:57:03Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2512.03756v1",
      "abs_url": "http://arxiv.org/abs/2512.03756v1"
    },
    "2512.02368v1": {
      "arxiv_id": "2512.02368v1",
      "title": "Multi-Domain Enhanced Map-Free Trajectory Prediction with Selective Attention",
      "authors": [
        "Wenyi Xiong",
        "Jian Chen"
      ],
      "summary": "Trajectory prediction is crucial for the reliability and safety of autonomous driving systems, yet it remains a challenging task in complex interactive scenarios. Existing methods often struggle to efficiently extract valuable scene information from redundant data, thereby reducing computational efficiency and prediction accuracy, especially when dealing with intricate agent interactions. To address these challenges, we propose a novel map-free trajectory prediction algorithm that achieves trajectory prediction across the temporal, spatial, and frequency domains. Specifically, in temporal information processing, We utilize a Mixture of Experts (MoE) mechanism to adaptively select critical frequency components. Concurrently, we extract these components and integrate multi-scale temporal features. Subsequently, a selective attention module is proposed to filter out redundant information in both temporal sequences and spatial interactions. Finally, we design a multimodal decoder. Under the supervision of patch-level and point-level losses, we obtain reasonable trajectory results. Experiments on Nuscences datasets demonstrate the superiority of our algorithm, validating its effectiveness in handling complex interactive scenarios.",
      "published_utc": "2025-12-02T03:20:07Z",
      "updated_utc": "2025-12-02T03:20:07Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2512.02368v1",
      "abs_url": "http://arxiv.org/abs/2512.02368v1"
    },
    "2512.01478v2": {
      "arxiv_id": "2512.01478v2",
      "title": "CourtMotion: Learning Event-Driven Motion Representations from Skeletal Data for Basketball",
      "authors": [
        "Omer Sela",
        "Michael Chertok",
        "Lior Wolf"
      ],
      "summary": "This paper presents CourtMotion, a spatiotemporal modeling framework for analyzing and predicting game events and plays as they develop in professional basketball. Anticipating basketball events requires understanding both physical motion patterns and their semantic significance in the context of the game. Traditional approaches that use only player positions fail to capture crucial indicators such as body orientation, defensive stance, or shooting preparation motions. Our two-stage approach first processes skeletal tracking data through Graph Neural Networks to capture nuanced motion patterns, then employs a Transformer architecture with specialized attention mechanisms to model player interactions. We introduce event projection heads that explicitly connect player movements to basketball events like passes, shots, and steals, training the model to associate physical motion patterns with their tactical purposes. Experiments on NBA tracking data demonstrate significant improvements over position-only baselines: 35% reduction in trajectory prediction error compared to state-of-the-art position-based models and consistent performance gains across key basketball analytics tasks. The resulting pretrained model serves as a powerful foundation for multiple downstream tasks, with pick detection, shot taker identification, assist prediction, shot location classification, and shot type recognition demonstrating substantial improvements over existing methods.",
      "published_utc": "2025-12-01T09:58:24Z",
      "updated_utc": "2025-12-09T13:43:34Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.MA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2512.01478v2",
      "abs_url": "http://arxiv.org/abs/2512.01478v2"
    },
    "2511.22181v1": {
      "arxiv_id": "2511.22181v1",
      "title": "MTR-VP: Towards End-to-End Trajectory Planning through Context-Driven Image Encoding and Multiple Trajectory Prediction",
      "authors": [
        "Maitrayee Keskar",
        "Mohan Trivedi",
        "Ross Greer"
      ],
      "summary": "We present a method for trajectory planning for autonomous driving, learning image-based context embeddings that align with motion prediction frameworks and planning-based intention input. Within our method, a ViT encoder takes raw images and past kinematic state as input and is trained to produce context embeddings, drawing inspiration from those generated by the recent MTR (Motion Transformer) encoder, effectively substituting map-based features with learned visual representations. MTR provides a strong foundation for multimodal trajectory prediction by localizing agent intent and refining motion iteratively via motion query pairs; we name our approach MTR-VP (Motion Transformer for Vision-based Planning), and instead of the learnable intention queries used in the MTR decoder, we use cross attention on the intent and the context embeddings, which reflect a combination of information encoded from the driving scene and past vehicle states. We evaluate our methods on the Waymo End-to-End Driving Dataset, which requires predicting the agent's future 5-second trajectory in bird's-eye-view coordinates using prior camera images, agent pose history, and routing goals. We analyze our architecture using ablation studies, removing input images and multiple trajectory output. Our results suggest that transformer-based methods that are used to combine the visual features along with the kinetic features such as the past trajectory features are not effective at combining both modes to produce useful scene context embeddings, even when intention embeddings are augmented with foundation-model representations of scene context from CLIP and DINOv2, but that predicting a distribution over multiple futures instead of a single future trajectory boosts planning performance.",
      "published_utc": "2025-11-27T07:42:06Z",
      "updated_utc": "2025-11-27T07:42:06Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2511.22181v1",
      "abs_url": "http://arxiv.org/abs/2511.22181v1"
    },
    "2511.18874v1": {
      "arxiv_id": "2511.18874v1",
      "title": "GContextFormer: A global context-aware hybrid multi-head attention approach with scaled additive aggregation for multimodal trajectory prediction",
      "authors": [
        "Yuzhi Chen",
        "Yuanchang Xie",
        "Lei Zhao",
        "Pan Liu",
        "Yajie Zou",
        "Chen Wang"
      ],
      "summary": "Multimodal trajectory prediction generates multiple plausible future trajectories to address vehicle motion uncertainty from intention ambiguity and execution variability. However, HD map-dependent models suffer from costly data acquisition, delayed updates, and vulnerability to corrupted inputs, causing prediction failures. Map-free approaches lack global context, with pairwise attention over-amplifying straight patterns while suppressing transitional patterns, resulting in motion-intention misalignment. This paper proposes GContextFormer, a plug-and-play encoder-decoder architecture with global context-aware hybrid attention and scaled additive aggregation achieving intention-aligned multimodal prediction without map reliance. The Motion-Aware Encoder builds scene-level intention prior via bounded scaled additive aggregation over mode-embedded trajectory tokens and refines per-mode representations under shared global context, mitigating inter-mode suppression and promoting intention alignment. The Hierarchical Interaction Decoder decomposes social reasoning into dual-pathway cross-attention: a standard pathway ensures uniform geometric coverage over agent-mode pairs while a neighbor-context-enhanced pathway emphasizes salient interactions, with gating module mediating their contributions to maintain coverage-focus balance. Experiments on eight highway-ramp scenarios from TOD-VT dataset show GContextFormer outperforms state-of-the-art baselines. Compared to existing transformer models, GContextFormer achieves greater robustness and concentrated improvements in high-curvature and transition zones via spatial distributions. Interpretability is achieved through motion mode distinctions and neighbor context modulation exposing reasoning attribution. The modular architecture supports extensibility toward cross-domain multimodal reasoning tasks. Source: https://fenghy-chen.github.io/sources/.",
      "published_utc": "2025-11-24T08:28:42Z",
      "updated_utc": "2025-11-24T08:28:42Z",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.MA",
        "cs.RO",
        "cs.SI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2511.18874v1",
      "abs_url": "http://arxiv.org/abs/2511.18874v1"
    },
    "2511.18248v2": {
      "arxiv_id": "2511.18248v2",
      "title": "Coherent Multi-Agent Trajectory Forecasting in Team Sports with CausalTraj",
      "authors": [
        "Wei Zhen Teoh"
      ],
      "summary": "Jointly forecasting trajectories of multiple interacting agents is a core challenge in sports analytics and other domains involving complex group dynamics. Accurate prediction enables realistic simulation and strategic understanding of gameplay evolution. Most existing models are evaluated solely on per-agent accuracy metrics (minADE, minFDE), which assess each agent independently on its best-of-k prediction. However these metrics overlook whether the model learns which predicted trajectories can jointly form a plausible multi-agent future. Many state-of-the-art models are designed and optimized primarily based on these metrics. As a result, they may underperform on joint predictions and also fail to generate coherent, interpretable multi-agent scenarios in team sports. We propose CausalTraj, a temporally causal, likelihood-based model that is built to generate jointly probable multi-agent trajectory forecasts. To better assess collective modeling capability, we emphasize joint metrics (minJADE, minJFDE) that measure joint accuracy across agents within the best generated scenario sample. Evaluated on the NBA SportVU, Basketball-U, and Football-U datasets, CausalTraj achieves competitive per-agent accuracy and the best recorded results on joint metrics, while yielding qualitatively coherent and realistic gameplay evolutions.",
      "published_utc": "2025-11-23T02:24:20Z",
      "updated_utc": "2025-12-14T09:14:14Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2511.18248v2",
      "abs_url": "http://arxiv.org/abs/2511.18248v2"
    },
    "2511.17941v1": {
      "arxiv_id": "2511.17941v1",
      "title": "V2X-RECT: An Efficient V2X Trajectory Prediction Framework via Redundant Interaction Filtering and Tracking Error Correction",
      "authors": [
        "Xiangyan Kong",
        "Xuecheng Wu",
        "Xiongwei Zhao",
        "Xiaodong Li",
        "Yunyun Shi",
        "Gang Wang",
        "Dingkang Yang",
        "Yang Liu",
        "Hong Chen",
        "Yulong Gao"
      ],
      "summary": "V2X prediction can alleviate perception incompleteness caused by limited line of sight through fusing trajectory data from infrastructure and vehicles, which is crucial to traffic safety and efficiency. However, in dense traffic scenarios, frequent identity switching of targets hinders cross-view association and fusion. Meanwhile, multi-source information tends to generate redundant interactions during the encoding stage, and traditional vehicle-centric encoding leads to large amounts of repetitive historical trajectory feature encoding, degrading real-time inference performance. To address these challenges, we propose V2X-RECT, a trajectory prediction framework designed for high-density environments. It enhances data association consistency, reduces redundant interactions, and reuses historical information to enable more efficient and accurate prediction. Specifically, we design a multi-source identity matching and correction module that leverages multi-view spatiotemporal relationships to achieve stable and consistent target association, mitigating the adverse effects of mismatches on trajectory encoding and cross-view feature fusion. Then we introduce traffic signal-guided interaction module, encoding trend of traffic light changes as features and exploiting their role in constraining spatiotemporal passage rights to accurately filter key interacting vehicles, while capturing the dynamic impact of signal changes on interaction patterns. Furthermore, a local spatiotemporal coordinate encoding enables reusable features of historical trajectories and map, supporting parallel decoding and significantly improving inference efficiency. Extensive experimental results across V2X-Seq and V2X-Traj datasets demonstrate that our V2X-RECT achieves significant improvements compared to SOTA methods, while also enhancing robustness and inference efficiency across diverse traffic densities.",
      "published_utc": "2025-11-22T06:50:47Z",
      "updated_utc": "2025-11-22T06:50:47Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2511.17941v1",
      "abs_url": "http://arxiv.org/abs/2511.17941v1"
    },
    "2511.17798v1": {
      "arxiv_id": "2511.17798v1",
      "title": "SM2ITH: Safe Mobile Manipulation with Interactive Human Prediction via Task-Hierarchical Bilevel Model Predictive Control",
      "authors": [
        "Francesco D'Orazio",
        "Sepehr Samavi",
        "Xintong Du",
        "Siqi Zhou",
        "Giuseppe Oriolo",
        "Angela P. Schoellig"
      ],
      "summary": "Mobile manipulators are designed to perform complex sequences of navigation and manipulation tasks in human-centered environments. While recent optimization-based methods such as Hierarchical Task Model Predictive Control (HTMPC) enable efficient multitask execution with strict task priorities, they have so far been applied mainly to static or structured scenarios. Extending these approaches to dynamic human-centered environments requires predictive models that capture how humans react to the actions of the robot. This work introduces Safe Mobile Manipulation with Interactive Human Prediction via Task-Hierarchical Bilevel Model Predictive Control (SM$^2$ITH), a unified framework that combines HTMPC with interactive human motion prediction through bilevel optimization that jointly accounts for robot and human dynamics. The framework is validated on two different mobile manipulators, the Stretch 3 and the Ridgeback-UR10, across three experimental settings: (i) delivery tasks with different navigation and manipulation priorities, (ii) sequential pick-and-place tasks with different human motion prediction models, and (iii) interactions involving adversarial human behavior. Our results highlight how interactive prediction enables safe and efficient coordination, outperforming baselines that rely on weighted objectives or open-loop human models.",
      "published_utc": "2025-11-21T21:42:59Z",
      "updated_utc": "2025-11-21T21:42:59Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2511.17798v1",
      "abs_url": "http://arxiv.org/abs/2511.17798v1"
    },
    "2511.17150v1": {
      "arxiv_id": "2511.17150v1",
      "title": "DiffRefiner: Coarse to Fine Trajectory Planning via Diffusion Refinement with Semantic Interaction for End to End Autonomous Driving",
      "authors": [
        "Liuhan Yin",
        "Runkun Ju",
        "Guodong Guo",
        "Erkang Cheng"
      ],
      "summary": "Unlike discriminative approaches in autonomous driving that predict a fixed set of candidate trajectories of the ego vehicle, generative methods, such as diffusion models, learn the underlying distribution of future motion, enabling more flexible trajectory prediction. However, since these methods typically rely on denoising human-crafted trajectory anchors or random noise, there remains significant room for improvement. In this paper, we propose DiffRefiner, a novel two-stage trajectory prediction framework. The first stage uses a transformer-based Proposal Decoder to generate coarse trajectory predictions by regressing from sensor inputs using predefined trajectory anchors. The second stage applies a Diffusion Refiner that iteratively denoises and refines these initial predictions. In this way, we enhance the performance of diffusion-based planning by incorporating a discriminative trajectory proposal module, which provides strong guidance for the generative refinement process. Furthermore, we design a fine-grained denoising decoder to enhance scene compliance, enabling more accurate trajectory prediction through enhanced alignment with the surrounding environment. Experimental results demonstrate that DiffRefiner achieves state-of-the-art performance, attaining 87.4 EPDMS on NAVSIM v2, and 87.1 DS along with 71.4 SR on Bench2Drive, thereby setting new records on both public benchmarks. The effectiveness of each component is validated via ablation studies as well.",
      "published_utc": "2025-11-21T11:16:00Z",
      "updated_utc": "2025-11-21T11:16:00Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2511.17150v1",
      "abs_url": "http://arxiv.org/abs/2511.17150v1"
    },
    "2511.17045v2": {
      "arxiv_id": "2511.17045v2",
      "title": "RacketVision: A Multiple Racket Sports Benchmark for Unified Ball and Racket Analysis",
      "authors": [
        "Linfeng Dong",
        "Yuchen Yang",
        "Hao Wu",
        "Wei Wang",
        "Yuenan Hou",
        "Zhihang Zhong",
        "Xiao Sun"
      ],
      "summary": "We introduce RacketVision, a novel dataset and benchmark for advancing computer vision in sports analytics, covering table tennis, tennis, and badminton. The dataset is the first to provide large-scale, fine-grained annotations for racket pose alongside traditional ball positions, enabling research into complex human-object interactions. It is designed to tackle three interconnected tasks: fine-grained ball tracking, articulated racket pose estimation, and predictive ball trajectory forecasting. Our evaluation of established baselines reveals a critical insight for multi-modal fusion: while naively concatenating racket pose features degrades performance, a CrossAttention mechanism is essential to unlock their value, leading to trajectory prediction results that surpass strong unimodal baselines. RacketVision provides a versatile resource and a strong starting point for future research in dynamic object tracking, conditional motion forecasting, and multimodal analysis in sports. Project page at https://github.com/OrcustD/RacketVision",
      "published_utc": "2025-11-21T08:44:33Z",
      "updated_utc": "2025-11-27T05:13:43Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2511.17045v2",
      "abs_url": "http://arxiv.org/abs/2511.17045v2"
    },
    "2511.16183v1": {
      "arxiv_id": "2511.16183v1",
      "title": "FOOTPASS: A Multi-Modal Multi-Agent Tactical Context Dataset for Play-by-Play Action Spotting in Soccer Broadcast Videos",
      "authors": [
        "Jeremie Ochin",
        "Raphael Chekroun",
        "Bogdan Stanciulescu",
        "Sotiris Manitsaris"
      ],
      "summary": "Soccer video understanding has motivated the creation of datasets for tasks such as temporal action localization, spatiotemporal action detection (STAD), or multiobject tracking (MOT). The annotation of structured sequences of events (who does what, when, and where) used for soccer analytics requires a holistic approach that integrates both STAD and MOT. However, current action recognition methods remain insufficient for constructing reliable play-by-play data and are typically used to assist rather than fully automate annotation. Parallel research has advanced tactical modeling, trajectory forecasting, and performance analysis, all grounded in game-state and play-by-play data. This motivates leveraging tactical knowledge as a prior to support computer-vision-based predictions, enabling more automated and reliable extraction of play-by-play data. We introduce Footovision Play-by-Play Action Spotting in Soccer Dataset (FOOTPASS), the first benchmark for play-by-play action spotting over entire soccer matches in a multi-modal, multi-agent tactical context. It enables the development of methods for player-centric action spotting that exploit both outputs from computer-vision tasks (e.g., tracking, identification) and prior knowledge of soccer, including its tactical regularities over long time horizons, to generate reliable play-by-play data streams. These streams form an essential input for data-driven sports analytics.",
      "published_utc": "2025-11-20T09:42:28Z",
      "updated_utc": "2025-11-20T09:42:28Z",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2511.16183v1",
      "abs_url": "http://arxiv.org/abs/2511.16183v1"
    },
    "2511.14237v1": {
      "arxiv_id": "2511.14237v1",
      "title": "Breaking the Passive Learning Trap: An Active Perception Strategy for Human Motion Prediction",
      "authors": [
        "Juncheng Hu",
        "Zijian Zhang",
        "Zeyu Wang",
        "Guoyu Wang",
        "Yingji Li",
        "Kedi Lyu"
      ],
      "summary": "Forecasting 3D human motion is an important embodiment of fine-grained understanding and cognition of human behavior by artificial agents. Current approaches excessively rely on implicit network modeling of spatiotemporal relationships and motion characteristics, falling into the passive learning trap that results in redundant and monotonous 3D coordinate information acquisition while lacking actively guided explicit learning mechanisms. To overcome these issues, we propose an Active Perceptual Strategy (APS) for human motion prediction, leveraging quotient space representations to explicitly encode motion properties while introducing auxiliary learning objectives to strengthen spatio-temporal modeling. Specifically, we first design a data perception module that projects poses into the quotient space, decoupling motion geometry from coordinate redundancy. By jointly encoding tangent vectors and Grassmann projections, this module simultaneously achieves geometric dimension reduction, semantic decoupling, and dynamic constraint enforcement for effective motion pose characterization. Furthermore, we introduce a network perception module that actively learns spatio-temporal dependencies through restorative learning. This module deliberately masks specific joints or injects noise to construct auxiliary supervision signals. A dedicated auxiliary learning network is designed to actively adapt and learn from perturbed information. Notably, APS is model agnostic and can be integrated with different prediction models to enhance active perceptual. The experimental results demonstrate that our method achieves the new state-of-the-art, outperforming existing methods by large margins: 16.3% on H3.6M, 13.9% on CMU Mocap, and 10.1% on 3DPW.",
      "published_utc": "2025-11-18T08:15:23Z",
      "updated_utc": "2025-11-18T08:15:23Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2511.14237v1",
      "abs_url": "http://arxiv.org/abs/2511.14237v1"
    },
    "2511.12878v3": {
      "arxiv_id": "2511.12878v3",
      "title": "Uni-Hand: Universal Hand Motion Forecasting in Egocentric Views",
      "authors": [
        "Junyi Ma",
        "Wentao Bao",
        "Jingyi Xu",
        "Guanzhong Sun",
        "Yu Zheng",
        "Erhang Zhang",
        "Xieyuanli Chen",
        "Hesheng Wang"
      ],
      "summary": "Forecasting how human hands move in egocentric views is critical for applications like augmented reality and human-robot policy transfer. Recently, several hand trajectory prediction (HTP) methods have been developed to generate future possible hand waypoints, which still suffer from insufficient prediction targets, inherent modality gaps, entangled hand-head motion, and limited validation in downstream tasks. To address these limitations, we present a universal hand motion forecasting framework considering multi-modal input, multi-dimensional and multi-target prediction patterns, and multi-task affordances for downstream applications. We harmonize multiple modalities by vision-language fusion, global context incorporation, and task-aware text embedding injection, to forecast hand waypoints in both 2D and 3D spaces. A novel dual-branch diffusion is proposed to concurrently predict human head and hand movements, capturing their motion synergy in egocentric vision. By introducing target indicators, the prediction model can forecast the specific joint waypoints of the wrist or the fingers, besides the widely studied hand center points. In addition, we enable Uni-Hand to additionally predict hand-object interaction states (contact/separation) to facilitate downstream tasks better. As the first work to incorporate downstream task evaluation in the literature, we build novel benchmarks to assess the real-world applicability of hand motion forecasting algorithms. The experimental results on multiple publicly available datasets and our newly proposed benchmarks demonstrate that Uni-Hand achieves the state-of-the-art performance in multi-dimensional and multi-target hand motion forecasting. Extensive validation in multiple downstream tasks also presents its impressive human-robot policy transfer to enable robotic manipulation, and effective feature enhancement for action anticipation/recognition.",
      "published_utc": "2025-11-17T02:14:13Z",
      "updated_utc": "2025-12-05T02:20:16Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2511.12878v3",
      "abs_url": "http://arxiv.org/abs/2511.12878v3"
    },
    "2511.12232v2": {
      "arxiv_id": "2511.12232v2",
      "title": "SocialNav-Map: Dynamic Mapping with Human Trajectory Prediction for Zero-Shot Social Navigation",
      "authors": [
        "Lingfeng Zhang",
        "Erjia Xiao",
        "Xiaoshuai Hao",
        "Haoxiang Fu",
        "Zeying Gong",
        "Long Chen",
        "Xiaojun Liang",
        "Renjing Xu",
        "Hangjun Ye",
        "Wenbo Ding"
      ],
      "summary": "Social navigation in densely populated dynamic environments poses a significant challenge for autonomous mobile robots, requiring advanced strategies for safe interaction. Existing reinforcement learning (RL)-based methods require over 2000+ hours of extensive training and often struggle to generalize to unfamiliar environments without additional fine-tuning, limiting their practical application in real-world scenarios. To address these limitations, we propose SocialNav-Map, a novel zero-shot social navigation framework that combines dynamic human trajectory prediction with occupancy mapping, enabling safe and efficient navigation without the need for environment-specific training. Specifically, SocialNav-Map first transforms the task goal position into the constructed map coordinate system. Subsequently, it creates a dynamic occupancy map that incorporates predicted human movements as dynamic obstacles. The framework employs two complementary methods for human trajectory prediction: history prediction and orientation prediction. By integrating these predicted trajectories into the occupancy map, the robot can proactively avoid potential collisions with humans while efficiently navigating to its destination. Extensive experiments on the Social-HM3D and Social-MP3D datasets demonstrate that SocialNav-Map significantly outperforms state-of-the-art (SOTA) RL-based methods, which require 2,396 GPU hours of training. Notably, it reduces human collision rates by over 10% without necessitating any training in novel environments. By eliminating the need for environment-specific training, SocialNav-Map achieves superior navigation performance, paving the way for the deployment of social navigation systems in real-world environments characterized by diverse human behaviors. The code is available at: https://github.com/linglingxiansen/SocialNav-Map.",
      "published_utc": "2025-11-15T14:20:05Z",
      "updated_utc": "2025-11-18T02:52:05Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2511.12232v2",
      "abs_url": "http://arxiv.org/abs/2511.12232v2"
    },
    "2511.11218v2": {
      "arxiv_id": "2511.11218v2",
      "title": "Humanoid Whole-Body Badminton via Multi-Stage Reinforcement Learning",
      "authors": [
        "Chenhao Liu",
        "Leyun Jiang",
        "Yibo Wang",
        "Kairan Yao",
        "Jinchen Fu",
        "Xiaoyu Ren"
      ],
      "summary": "Humanoid robots have demonstrated strong capabilities for interacting with static scenes across locomotion, manipulation, and more challenging loco-manipulation tasks. Yet the real world is dynamic, and quasi-static interactions are insufficient to cope with diverse environmental conditions. As a step toward more dynamic interaction scenarios, we present a reinforcement-learning-based training pipeline that produces a unified whole-body controller for humanoid badminton, enabling coordinated lower-body footwork and upper-body striking without motion priors or expert demonstrations. Training follows a three-stage curriculum: first footwork acquisition, then precision-guided racket swing generation, and finally task-focused refinement, yielding motions in which both legs and arms serve the hitting objective. For deployment, we incorporate an Extended Kalman Filter (EKF) to estimate and predict shuttlecock trajectories for target striking. We also introduce a prediction-free variant that dispenses with EKF and explicit trajectory prediction. To validate the framework, we conduct five sets of experiments in both simulation and the real world. In simulation, two robots sustain a rally of 21 consecutive hits. Moreover, the prediction-free variant achieves successful hits with comparable performance relative to the target-known policy. In real-world tests, both prediction and controller modules exhibit high accuracy, and on-court hitting achieves an outgoing shuttle speed up to 19.1 m/s with a mean return landing distance of 4 m. These experimental results show that our proposed training scheme can deliver highly dynamic while precise goal striking in badminton, and can be adapted to more dynamics-critical domains.",
      "published_utc": "2025-11-14T12:22:19Z",
      "updated_utc": "2025-12-09T11:59:44Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2511.11218v2",
      "abs_url": "http://arxiv.org/abs/2511.11218v2"
    },
    "2511.13753v1": {
      "arxiv_id": "2511.13753v1",
      "title": "Robustness of LLM-enabled vehicle trajectory prediction under data security threats",
      "authors": [
        "Feilong Wang",
        "Fuqiang Liu"
      ],
      "summary": "The integration of large language models (LLMs) into automated driving systems has opened new possibilities for reasoning and decision-making by transforming complex driving contexts into language-understandable representations. Recent studies demonstrate that fine-tuned LLMs can accurately predict vehicle trajectories and lane-change intentions by gathering and transforming data from surrounding vehicles. However, the robustness of such LLM-based prediction models for safety-critical driving systems remains unexplored, despite the increasing concerns about the trustworthiness of LLMs. This study addresses this gap by conducting a systematic vulnerability analysis of LLM-enabled vehicle trajectory prediction. We propose a one-feature differential evolution attack that perturbs a single kinematic feature of surrounding vehicles within the LLM's input prompts under a black-box setting. Experiments on the highD dataset reveal that even minor, physically plausible perturbations can significantly disrupt model outputs, underscoring the susceptibility of LLM-based predictors to adversarial manipulation. Further analyses reveal a trade-off between accuracy and robustness, examine the failure mechanism, and explore potential mitigation solutions. The findings provide the very first insights into adversarial vulnerabilities of LLM-driven automated vehicle models in the context of vehicular interactions and highlight the need for robustness-oriented design in future LLM-based intelligent transportation systems.",
      "published_utc": "2025-11-14T03:26:51Z",
      "updated_utc": "2025-11-14T03:26:51Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2511.13753v1",
      "abs_url": "http://arxiv.org/abs/2511.13753v1"
    },
    "2511.10411v1": {
      "arxiv_id": "2511.10411v1",
      "title": "LongComp: Long-Tail Compositional Zero-Shot Generalization for Robust Trajectory Prediction",
      "authors": [
        "Benjamin Stoler",
        "Jonathan Francis",
        "Jean Oh"
      ],
      "summary": "Methods for trajectory prediction in Autonomous Driving must contend with rare, safety-critical scenarios that make reliance on real-world data collection alone infeasible. To assess robustness under such conditions, we propose new long-tail evaluation settings that repartition datasets to create challenging out-of-distribution (OOD) test sets. We first introduce a safety-informed scenario factorization framework, which disentangles scenarios into discrete ego and social contexts. Building on analogies to compositional zero-shot image-labeling in Computer Vision, we then hold out novel context combinations to construct challenging closed-world and open-world settings. This process induces OOD performance gaps in future motion prediction of 5.0% and 14.7% in closed-world and open-world settings, respectively, relative to in-distribution performance for a state-of-the-art baseline. To improve generalization, we extend task-modular gating networks to operate within trajectory prediction models, and develop an auxiliary, difficulty-prediction head to refine internal representations. Our strategies jointly reduce the OOD performance gaps to 2.8% and 11.5% in the two settings, respectively, while still improving in-distribution performance.",
      "published_utc": "2025-11-13T15:33:09Z",
      "updated_utc": "2025-11-13T15:33:09Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2511.10411v1",
      "abs_url": "http://arxiv.org/abs/2511.10411v1"
    },
    "2511.10203v1": {
      "arxiv_id": "2511.10203v1",
      "title": "VISTA: A Vision and Intent-Aware Social Attention Framework for Multi-Agent Trajectory Prediction",
      "authors": [
        "Stephane Da Silva Martins",
        "Emanuel Aldea",
        "Sylvie Le Hégarat-Mascle"
      ],
      "summary": "Multi-agent trajectory prediction is crucial for autonomous systems operating in dense, interactive environments. Existing methods often fail to jointly capture agents' long-term goals and their fine-grained social interactions, which leads to unrealistic multi-agent futures. We propose VISTA, a recursive goal-conditioned transformer for multi-agent trajectory forecasting. VISTA combines (i) a cross-attention fusion module that integrates long-horizon intent with past motion, (ii) a social-token attention mechanism for flexible interaction modeling across agents, and (iii) pairwise attention maps that make social influence patterns interpretable at inference time. Our model turns single-agent goal-conditioned prediction into a coherent multi-agent forecasting framework. Beyond standard displacement metrics, we evaluate trajectory collision rates as a measure of joint realism. On the high-density MADRAS benchmark and on SDD, VISTA achieves state-of-the-art accuracy and substantially fewer collisions. On MADRAS, it reduces the average collision rate of strong baselines from 2.14 to 0.03 percent, and on SDD it attains zero collisions while improving ADE, FDE, and minFDE. These results show that VISTA generates socially compliant, goal-aware, and interpretable trajectories, making it promising for safety-critical autonomous systems.",
      "published_utc": "2025-11-13T11:17:01Z",
      "updated_utc": "2025-11-13T11:17:01Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2511.10203v1",
      "abs_url": "http://arxiv.org/abs/2511.10203v1"
    },
    "2510.22205v1": {
      "arxiv_id": "2510.22205v1",
      "title": "TrajGATFormer: A Graph-Based Transformer Approach for Worker and Obstacle Trajectory Prediction in Off-site Construction Environments",
      "authors": [
        "Mohammed Alduais",
        "Xinming Li",
        "Qipei Mei"
      ],
      "summary": "As the demand grows within the construction industry for processes that are not only faster but also safer and more efficient, offsite construction has emerged as a solution, though it brings new safety risks due to the close interaction between workers, machinery, and moving obstacles. Predicting the future trajectories of workers and taking into account social and environmental factors is a crucial step for developing collision-avoidance systems to mitigate such risks. Traditional methods often struggle to adapt to the dynamic and unpredictable nature of construction environments. Many rely on simplified assumptions or require hand-crafted features, limiting their ability to respond to complex, real-time interactions between workers and moving obstacles. While recent data-driven methods have improved the modeling of temporal patterns, they still face challenges in capturing long-term behavior and accounting for the spatial and social context crucial to collision risk assessment. To address these limitations, this paper proposes a framework integrating YOLOv10n and DeepSORT for precise detection and tracking, along with two novel trajectory prediction models: TrajGATFormer and TrajGATFormer-Obstacle. YOLOv10n serves as the backbone for object detection, accurately identifying workers and obstacles in diverse scenes, while DeepSORT efficiently tracks them over time with unique IDs for continuity. Both models employ a transformer encoder-decoder with Graph Attention Networks (GAT) to capture temporal and spatial interactions. TrajGATFormer predicts worker trajectories with an ADE of 1.25 m and FDE of 2.3 m over a 4.8 s horizon, while TrajGATFormer-Obstacle extends prediction to both workers and obstacles, achieving higher accuracy (ADE 1.15 m, FDE 2.2 m). Comparative analysis shows both models outperform traditional methods, reducing ADE and FDE by up to 35% and 38%, respectively.",
      "published_utc": "2025-10-25T08:08:10Z",
      "updated_utc": "2025-10-25T08:08:10Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2510.22205v1",
      "abs_url": "http://arxiv.org/abs/2510.22205v1"
    },
    "2510.19789v1": {
      "arxiv_id": "2510.19789v1",
      "title": "OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation",
      "authors": [
        "Guowei Xu",
        "Yuxuan Bian",
        "Ailing Zeng",
        "Mingyi Shi",
        "Shaoli Huang",
        "Wen Li",
        "Lixin Duan",
        "Qiang Xu"
      ],
      "summary": "This paper introduces OmniMotion-X, a versatile multimodal framework for whole-body human motion generation, leveraging an autoregressive diffusion transformer in a unified sequence-to-sequence manner. OmniMotion-X efficiently supports diverse multimodal tasks, including text-to-motion, music-to-dance, speech-to-gesture, and global spatial-temporal control scenarios (e.g., motion prediction, in-betweening, completion, and joint/trajectory-guided synthesis), as well as flexible combinations of these tasks. Specifically, we propose the use of reference motion as a novel conditioning signal, substantially enhancing the consistency of generated content, style, and temporal dynamics crucial for realistic animations. To handle multimodal conflicts, we introduce a progressive weak-to-strong mixed-condition training strategy. To enable high-quality multimodal training, we construct OmniMoCap-X, the largest unified multimodal motion dataset to date, integrating 28 publicly available MoCap sources across 10 distinct tasks, standardized to the SMPL-X format at 30 fps. To ensure detailed and consistent annotations, we render sequences into videos and use GPT-4o to automatically generate structured and hierarchical captions, capturing both low-level actions and high-level semantics. Extensive experimental evaluations confirm that OmniMotion-X significantly surpasses existing methods, demonstrating state-of-the-art performance across multiple multimodal tasks and enabling the interactive generation of realistic, coherent, and controllable long-duration motions.",
      "published_utc": "2025-10-22T17:25:33Z",
      "updated_utc": "2025-10-22T17:25:33Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2510.19789v1",
      "abs_url": "http://arxiv.org/abs/2510.19789v1"
    },
    "2510.14250v1": {
      "arxiv_id": "2510.14250v1",
      "title": "A Physics Prior-Guided Dual-Stream Attention Network for Motion Prediction of Elastic Bragg Breakwaters",
      "authors": [
        "Lianzi Jiang",
        "Jianxin Zhang",
        "Xinyu Han",
        "Huanhe Dong",
        "Xiangrong Wang"
      ],
      "summary": "Accurate motion response prediction for elastic Bragg breakwaters is critical for their structural safety and operational integrity in marine environments. However, conventional deep learning models often exhibit limited generalization capabilities when presented with unseen sea states. These deficiencies stem from the neglect of natural decay observed in marine systems and inadequate modeling of wave-structure interaction (WSI). To overcome these challenges, this study proposes a novel Physics Prior-Guided Dual-Stream Attention Network (PhysAttnNet). First, the decay bidirectional self-attention (DBSA) module incorporates a learnable temporal decay to assign higher weights to recent states, aiming to emulate the natural decay phenomenon. Meanwhile, the phase differences guided bidirectional cross-attention (PDG-BCA) module explicitly captures the bidirectional interaction and phase relationship between waves and the structure using a cosine-based bias within a bidirectional cross-computation paradigm. These streams are synergistically integrated through a global context fusion (GCF) module. Finally, PhysAttnNet is trained with a hybrid time-frequency loss that jointly minimizes time-domain prediction errors and frequency-domain spectral discrepancies. Comprehensive experiments on wave flume datasets demonstrate that PhysAttnNet significantly outperforms mainstream models. Furthermore,cross-scenario generalization tests validate the model's robustness and adaptability to unseen environments, highlighting its potential as a framework to develop predictive models for complex systems in ocean engineering.",
      "published_utc": "2025-10-16T03:06:44Z",
      "updated_utc": "2025-10-16T03:06:44Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2510.14250v1",
      "abs_url": "http://arxiv.org/abs/2510.14250v1"
    },
    "2510.10731v1": {
      "arxiv_id": "2510.10731v1",
      "title": "Controllable Generative Trajectory Prediction via Weak Preference Alignment",
      "authors": [
        "Yongxi Cao",
        "Julian F. Schumann",
        "Jens Kober",
        "Joni Pajarinen",
        "Arkady Zgonnikov"
      ],
      "summary": "Deep generative models such as conditional variational autoencoders (CVAEs) have shown great promise for predicting trajectories of surrounding agents in autonomous vehicle planning. State-of-the-art models have achieved remarkable accuracy in such prediction tasks. Besides accuracy, diversity is also crucial for safe planning because human behaviors are inherently uncertain and multimodal. However, existing methods generally lack a scheme to generate controllably diverse trajectories, which is arguably more useful than randomly diversified trajectories, to the end of safe planning. To address this, we propose PrefCVAE, an augmented CVAE framework that uses weakly labeled preference pairs to imbue latent variables with semantic attributes. Using average velocity as an example attribute, we demonstrate that PrefCVAE enables controllable, semantically meaningful predictions without degrading baseline accuracy. Our results show the effectiveness of preference supervision as a cost-effective way to enhance sampling-based generative models.",
      "published_utc": "2025-10-12T18:06:39Z",
      "updated_utc": "2025-10-12T18:06:39Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2510.10731v1",
      "abs_url": "http://arxiv.org/abs/2510.10731v1"
    },
    "2510.10086v1": {
      "arxiv_id": "2510.10086v1",
      "title": "Beyond ADE and FDE: A Comprehensive Evaluation Framework for Safety-Critical Prediction in Multi-Agent Autonomous Driving Scenarios",
      "authors": [
        "Feifei Liu",
        "Haozhe Wang",
        "Zejun Wei",
        "Qirong Lu",
        "Yiyang Wen",
        "Xiaoyu Tang",
        "Jingyan Jiang",
        "Zhijian He"
      ],
      "summary": "Current evaluation methods for autonomous driving prediction models rely heavily on simplistic metrics such as Average Displacement Error (ADE) and Final Displacement Error (FDE). While these metrics offer basic performance assessments, they fail to capture the nuanced behavior of prediction modules under complex, interactive, and safety-critical driving scenarios. For instance, existing benchmarks do not distinguish the influence of nearby versus distant agents, nor systematically test model robustness across varying multi-agent interactions. This paper addresses this critical gap by proposing a novel testing framework that evaluates prediction performance under diverse scene structures, saying, map context, agent density and spatial distribution. Through extensive empirical analysis, we quantify the differential impact of agent proximity on target trajectory prediction and identify scenario-specific failure cases that are not exposed by traditional metrics. Our findings highlight key vulnerabilities in current state-of-the-art prediction models and demonstrate the importance of scenario-aware evaluation. The proposed framework lays the groundwork for rigorous, safety-driven prediction validation, contributing significantly to the identification of failure-prone corner cases and the development of robust, certifiable prediction systems for autonomous vehicles.",
      "published_utc": "2025-10-11T07:56:58Z",
      "updated_utc": "2025-10-11T07:56:58Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2510.10086v1",
      "abs_url": "http://arxiv.org/abs/2510.10086v1"
    },
    "2510.04233v1": {
      "arxiv_id": "2510.04233v1",
      "title": "Physics-Inspired All-Pair Interaction Learning for 3D Dynamics Modeling",
      "authors": [
        "Kai Yang",
        "Yuqi Huang",
        "Junheng Tao",
        "Wanyu Wang",
        "Qitian Wu"
      ],
      "summary": "Modeling 3D dynamics is a fundamental problem in multi-body systems across scientific and engineering domains and has important practical implications in trajectory prediction and simulation. While recent GNN-based approaches have achieved strong performance by enforcing geometric symmetries, encoding high-order features or incorporating neural-ODE mechanics, they typically depend on explicitly observed structures and inherently fail to capture the unobserved interactions that are crucial to complex physical behaviors and dynamics mechanism. In this paper, we propose PAINET, a principled SE(3)-equivariant neural architecture for learning all-pair interactions in multi-body systems. The model comprises: (1) a novel physics-inspired attention network derived from the minimization trajectory of an energy function, and (2) a parallel decoder that preserves equivariance while enabling efficient inference. Empirical results on diverse real-world benchmarks, including human motion capture, molecular dynamics, and large-scale protein simulations, show that PAINET consistently outperforms recently proposed models, yielding 4.7% to 41.5% error reductions in 3D dynamics prediction with comparable computation costs in terms of time and memory.",
      "published_utc": "2025-10-05T14:48:26Z",
      "updated_utc": "2025-10-05T14:48:26Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2510.04233v1",
      "abs_url": "http://arxiv.org/abs/2510.04233v1"
    },
    "2510.03776v1": {
      "arxiv_id": "2510.03776v1",
      "title": "Trajectory prediction for heterogeneous agents: A performance analysis on small and imbalanced datasets",
      "authors": [
        "Tiago Rodrigues de Almeida",
        "Yufei Zhu",
        "Andrey Rudenko",
        "Tomasz P. Kucner",
        "Johannes A. Stork",
        "Martin Magnusson",
        "Achim J. Lilienthal"
      ],
      "summary": "Robots and other intelligent systems navigating in complex dynamic environments should predict future actions and intentions of surrounding agents to reach their goals efficiently and avoid collisions. The dynamics of those agents strongly depends on their tasks, roles, or observable labels. Class-conditioned motion prediction is thus an appealing way to reduce forecast uncertainty and get more accurate predictions for heterogeneous agents. However, this is hardly explored in the prior art, especially for mobile robots and in limited data applications. In this paper, we analyse different class-conditioned trajectory prediction methods on two datasets. We propose a set of conditional pattern-based and efficient deep learning-based baselines, and evaluate their performance on robotics and outdoors datasets (THÖR-MAGNI and Stanford Drone Dataset). Our experiments show that all methods improve accuracy in most of the settings when considering class labels. More importantly, we observe that there are significant differences when learning from imbalanced datasets, or in new environments where sufficient data is not available. In particular, we find that deep learning methods perform better on balanced datasets, but in applications with limited data, e.g., cold start of a robot in a new environment, or imbalanced classes, pattern-based methods may be preferable.",
      "published_utc": "2025-10-04T11:02:21Z",
      "updated_utc": "2025-10-04T11:02:21Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2510.03776v1",
      "abs_url": "http://arxiv.org/abs/2510.03776v1"
    },
    "2510.03031v1": {
      "arxiv_id": "2510.03031v1",
      "title": "Long-Term Human Motion Prediction Using Spatio-Temporal Maps of Dynamics",
      "authors": [
        "Yufei Zhu",
        "Andrey Rudenko",
        "Tomasz P. Kucner",
        "Achim J. Lilienthal",
        "Martin Magnusson"
      ],
      "summary": "Long-term human motion prediction (LHMP) is important for the safe and efficient operation of autonomous robots and vehicles in environments shared with humans. Accurate predictions are important for applications including motion planning, tracking, human-robot interaction, and safety monitoring. In this paper, we exploit Maps of Dynamics (MoDs), which encode spatial or spatio-temporal motion patterns as environment features, to achieve LHMP for horizons of up to 60 seconds. We propose an MoD-informed LHMP framework that supports various types of MoDs and includes a ranking method to output the most likely predicted trajectory, improving practical utility in robotics. Further, a time-conditioned MoD is introduced to capture motion patterns that vary across different times of day. We evaluate MoD-LHMP instantiated with three types of MoDs. Experiments on two real-world datasets show that MoD-informed method outperforms learning-based ones, with up to 50\\% improvement in average displacement error, and the time-conditioned variant achieves the highest accuracy overall. Project code is available at https://github.com/test-bai-cpu/LHMP-with-MoDs.git",
      "published_utc": "2025-10-03T14:12:55Z",
      "updated_utc": "2025-10-03T14:12:55Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2510.03031v1",
      "abs_url": "http://arxiv.org/abs/2510.03031v1"
    },
    "2510.02627v1": {
      "arxiv_id": "2510.02627v1",
      "title": "A Trajectory Generator for High-Density Traffic and Diverse Agent-Interaction Scenarios",
      "authors": [
        "Ruining Yang",
        "Yi Xu",
        "Yixiao Chen",
        "Yun Fu",
        "Lili Su"
      ],
      "summary": "Accurate trajectory prediction is fundamental to autonomous driving, as it underpins safe motion planning and collision avoidance in complex environments. However, existing benchmark datasets suffer from a pronounced long-tail distribution problem, with most samples drawn from low-density scenarios and simple straight-driving behaviors. This underrepresentation of high-density scenarios and safety critical maneuvers such as lane changes, overtaking and turning is an obstacle to model generalization and leads to overly optimistic evaluations. To address these challenges, we propose a novel trajectory generation framework that simultaneously enhances scenarios density and enriches behavioral diversity. Specifically, our approach converts continuous road environments into a structured grid representation that supports fine-grained path planning, explicit conflict detection, and multi-agent coordination. Built upon this representation, we introduce behavior-aware generation mechanisms that combine rule-based decision triggers with Frenet-based trajectory smoothing and dynamic feasibility constraints. This design allows us to synthesize realistic high-density scenarios and rare behaviors with complex interactions that are often missing in real data. Extensive experiments on the large-scale Argoverse 1 and Argoverse 2 datasets demonstrate that our method significantly improves both agent density and behavior diversity, while preserving motion realism and scenario-level safety. Our synthetic data also benefits downstream trajectory prediction models and enhances performance in challenging high-density scenarios.",
      "published_utc": "2025-10-03T00:12:18Z",
      "updated_utc": "2025-10-03T00:12:18Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2510.02627v1",
      "abs_url": "http://arxiv.org/abs/2510.02627v1"
    },
    "2510.00405v1": {
      "arxiv_id": "2510.00405v1",
      "title": "EgoTraj-Bench: Towards Robust Trajectory Prediction Under Ego-view Noisy Observations",
      "authors": [
        "Jiayi Liu",
        "Jiaming Zhou",
        "Ke Ye",
        "Kun-Yu Lin",
        "Allan Wang",
        "Junwei Liang"
      ],
      "summary": "Reliable trajectory prediction from an ego-centric perspective is crucial for robotic navigation in human-centric environments. However, existing methods typically assume idealized observation histories, failing to account for the perceptual artifacts inherent in first-person vision, such as occlusions, ID switches, and tracking drift. This discrepancy between training assumptions and deployment reality severely limits model robustness. To bridge this gap, we introduce EgoTraj-Bench, the first real-world benchmark that grounds noisy, first-person visual histories in clean, bird's-eye-view future trajectories, enabling robust learning under realistic perceptual constraints. Building on this benchmark, we propose BiFlow, a dual-stream flow matching model that concurrently denoises historical observations and forecasts future motion by leveraging a shared latent representation. To better model agent intent, BiFlow incorporates our EgoAnchor mechanism, which conditions the prediction decoder on distilled historical features via feature modulation. Extensive experiments show that BiFlow achieves state-of-the-art performance, reducing minADE and minFDE by 10-15% on average and demonstrating superior robustness. We anticipate that our benchmark and model will provide a critical foundation for developing trajectory forecasting systems truly resilient to the challenges of real-world, ego-centric perception.",
      "published_utc": "2025-10-01T01:30:13Z",
      "updated_utc": "2025-10-01T01:30:13Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2510.00405v1",
      "abs_url": "http://arxiv.org/abs/2510.00405v1"
    },
    "2510.00060v2": {
      "arxiv_id": "2510.00060v2",
      "title": "Less is More: Lean yet Powerful Vision-Language Model for Autonomous Driving",
      "authors": [
        "Sheng Yang",
        "Tong Zhan",
        "Guancheng Chen",
        "Yanfeng Lu",
        "Jian Wang"
      ],
      "summary": "In this work, we reconceptualize autonomous driving as a generalized language and formulate the trajectory planning task as next waypoint prediction. We introduce Max-V1, a novel framework for one-stage end-to-end autonomous driving. Our framework presents a single-pass generation paradigm that aligns with the inherent sequentiality of driving. This approach leverages the generative capacity of the VLM (Vision-Language Model) to enable end-to-end trajectory prediction directly from front-view camera input. The efficacy of this method is underpinned by a principled supervision strategy derived from statistical modeling. This provides a well-defined learning objective, which makes the framework highly amenable to master complex driving policies through imitation learning from large-scale expert demonstrations. Empirically, our method achieves the state-of-the-art performance on the nuScenes dataset, delivers an overall improvement of over 30% compared to prior baselines. Furthermore, it exhibits superior generalization performance on cross-domain datasets acquired from diverse vehicles, demonstrating notable potential for cross-vehicle robustness and adaptability. Due to these empirical strengths, this work introduces a model enabling fundamental driving behaviors, laying the foundation for the development of more capable self-driving agents. Code will be available upon publication.",
      "published_utc": "2025-09-29T05:14:18Z",
      "updated_utc": "2025-10-03T15:30:05Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2510.00060v2",
      "abs_url": "http://arxiv.org/abs/2510.00060v2"
    },
    "2509.24209v1": {
      "arxiv_id": "2509.24209v1",
      "title": "Forge4D: Feed-Forward 4D Human Reconstruction and Interpolation from Uncalibrated Sparse-view Videos",
      "authors": [
        "Yingdong Hu",
        "Yisheng He",
        "Jinnan Chen",
        "Weihao Yuan",
        "Kejie Qiu",
        "Zehong Lin",
        "Siyu Zhu",
        "Zilong Dong",
        "Jun Zhang"
      ],
      "summary": "Instant reconstruction of dynamic 3D humans from uncalibrated sparse-view videos is critical for numerous downstream applications. Existing methods, however, are either limited by the slow reconstruction speeds or incapable of generating novel-time representations. To address these challenges, we propose Forge4D, a feed-forward 4D human reconstruction and interpolation model that efficiently reconstructs temporally aligned representations from uncalibrated sparse-view videos, enabling both novel view and novel time synthesis. Our model simplifies the 4D reconstruction and interpolation problem as a joint task of streaming 3D Gaussian reconstruction and dense motion prediction. For the task of streaming 3D Gaussian reconstruction, we first reconstruct static 3D Gaussians from uncalibrated sparse-view images and then introduce learnable state tokens to enforce temporal consistency in a memory-friendly manner by interactively updating shared information across different timestamps. For novel time synthesis, we design a novel motion prediction module to predict dense motions for each 3D Gaussian between two adjacent frames, coupled with an occlusion-aware Gaussian fusion process to interpolate 3D Gaussians at arbitrary timestamps. To overcome the lack of the ground truth for dense motion supervision, we formulate dense motion prediction as a dense point matching task and introduce a self-supervised retargeting loss to optimize this module. An additional occlusion-aware optical flow loss is introduced to ensure motion consistency with plausible human movement, providing stronger regularization. Extensive experiments demonstrate the effectiveness of our model on both in-domain and out-of-domain datasets. Project page and code at: https://zhenliuzju.github.io/huyingdong/Forge4D.",
      "published_utc": "2025-09-29T02:47:14Z",
      "updated_utc": "2025-09-29T02:47:14Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2509.24209v1",
      "abs_url": "http://arxiv.org/abs/2509.24209v1"
    },
    "2509.17850v1": {
      "arxiv_id": "2509.17850v1",
      "title": "SocialTraj: Two-Stage Socially-Aware Trajectory Prediction for Autonomous Driving via Conditional Diffusion Model",
      "authors": [
        "Xiao Zhou",
        "Zengqi Peng",
        "Jun Ma"
      ],
      "summary": "Accurate trajectory prediction of surrounding vehicles (SVs) is crucial for autonomous driving systems to avoid misguided decisions and potential accidents. However, achieving reliable predictions in highly dynamic and complex traffic scenarios remains a significant challenge. One of the key impediments lies in the limited effectiveness of current approaches to capture the multi-modal behaviors of drivers, which leads to predicted trajectories that deviate from actual future motions. To address this issue, we propose SocialTraj, a novel trajectory prediction framework integrating social psychology principles through social value orientation (SVO). By utilizing Bayesian inverse reinforcement learning (IRL) to estimate the SVO of SVs, we obtain the critical social context to infer the future interaction trend. To ensure modal consistency in predicted behaviors, the estimated SVOs of SVs are embedded into a conditional denoising diffusion model that aligns generated trajectories with historical driving styles. Additionally, the planned future trajectory of the ego vehicle (EV) is explicitly incorporated to enhance interaction modeling. Extensive experiments on NGSIM and HighD datasets demonstrate that SocialTraj is capable of adapting to highly dynamic and interactive scenarios while generating socially compliant and behaviorally consistent trajectory predictions, outperforming existing baselines. Ablation studies demonstrate that dynamic SVO estimation and explicit ego-planning components notably improve prediction accuracy and substantially reduce inference time.",
      "published_utc": "2025-09-22T14:42:51Z",
      "updated_utc": "2025-09-22T14:42:51Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2509.17850v1",
      "abs_url": "http://arxiv.org/abs/2509.17850v1"
    },
    "2509.17080v1": {
      "arxiv_id": "2509.17080v1",
      "title": "CoPlanner: An Interactive Motion Planner with Contingency-Aware Diffusion for Autonomous Driving",
      "authors": [
        "Ruiguo Zhong",
        "Ruoyu Yao",
        "Pei Liu",
        "Xiaolong Chen",
        "Rui Yang",
        "Jun Ma"
      ],
      "summary": "Accurate trajectory prediction and motion planning are crucial for autonomous driving systems to navigate safely in complex, interactive environments characterized by multimodal uncertainties. However, current generation-then-evaluation frameworks typically construct multiple plausible trajectory hypotheses but ultimately adopt a single most likely outcome, leading to overconfident decisions and a lack of fallback strategies that are vital for safety in rare but critical scenarios. Moreover, the usual decoupling of prediction and planning modules could result in socially inconsistent or unrealistic joint trajectories, especially in highly interactive traffic. To address these challenges, we propose a contingency-aware diffusion planner (CoPlanner), a unified framework that jointly models multi-agent interactive trajectory generation and contingency-aware motion planning. Specifically, the pivot-conditioned diffusion mechanism anchors trajectory sampling on a validated, shared short-term segment to preserve temporal consistency, while stochastically generating diverse long-horizon branches that capture multimodal motion evolutions. In parallel, we design a contingency-aware multi-scenario scoring strategy that evaluates candidate ego trajectories across multiple plausible long-horizon evolution scenarios, balancing safety, progress, and comfort. This integrated design preserves feasible fallback options and enhances robustness under uncertainty, leading to more realistic interaction-aware planning. Extensive closed-loop experiments on the nuPlan benchmark demonstrate that CoPlanner consistently surpasses state-of-the-art methods on both Val14 and Test14 datasets, achieving significant improvements in safety and comfort under both reactive and non-reactive settings. Code and model will be made publicly available upon acceptance.",
      "published_utc": "2025-09-21T13:54:26Z",
      "updated_utc": "2025-09-21T13:54:26Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2509.17080v1",
      "abs_url": "http://arxiv.org/abs/2509.17080v1"
    },
    "2509.16095v1": {
      "arxiv_id": "2509.16095v1",
      "title": "AdaSports-Traj: Role- and Domain-Aware Adaptation for Multi-Agent Trajectory Modeling in Sports",
      "authors": [
        "Yi Xu",
        "Yun Fu"
      ],
      "summary": "Trajectory prediction in multi-agent sports scenarios is inherently challenging due to the structural heterogeneity across agent roles (e.g., players vs. ball) and dynamic distribution gaps across different sports domains. Existing unified frameworks often fail to capture these structured distributional shifts, resulting in suboptimal generalization across roles and domains. We propose AdaSports-Traj, an adaptive trajectory modeling framework that explicitly addresses both intra-domain and inter-domain distribution discrepancies in sports. At its core, AdaSports-Traj incorporates a Role- and Domain-Aware Adapter to conditionally adjust latent representations based on agent identity and domain context. Additionally, we introduce a Hierarchical Contrastive Learning objective, which separately supervises role-sensitive and domain-aware representations to encourage disentangled latent structures without introducing optimization conflict. Experiments on three diverse sports datasets, Basketball-U, Football-U, and Soccer-U, demonstrate the effectiveness of our adaptive design, achieving strong performance in both unified and cross-domain trajectory prediction settings.",
      "published_utc": "2025-09-19T15:38:27Z",
      "updated_utc": "2025-09-19T15:38:27Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2509.16095v1",
      "abs_url": "http://arxiv.org/abs/2509.16095v1"
    },
    "2509.15984v1": {
      "arxiv_id": "2509.15984v1",
      "title": "CoPAD : Multi-source Trajectory Fusion and Cooperative Trajectory Prediction with Anchor-oriented Decoder in V2X Scenarios",
      "authors": [
        "Kangyu Wu",
        "Jiaqi Qiao",
        "Ya Zhang"
      ],
      "summary": "Recently, data-driven trajectory prediction methods have achieved remarkable results, significantly advancing the development of autonomous driving. However, the instability of single-vehicle perception introduces certain limitations to trajectory prediction. In this paper, a novel lightweight framework for cooperative trajectory prediction, CoPAD, is proposed. This framework incorporates a fusion module based on the Hungarian algorithm and Kalman filtering, along with the Past Time Attention (PTA) module, mode attention module and anchor-oriented decoder (AoD). It effectively performs early fusion on multi-source trajectory data from vehicles and road infrastructure, enabling the trajectories with high completeness and accuracy. The PTA module can efficiently capture potential interaction information among historical trajectories, and the mode attention module is proposed to enrich the diversity of predictions. Additionally, the decoder based on sparse anchors is designed to generate the final complete trajectories. Extensive experiments show that CoPAD achieves the state-of-the-art performance on the DAIR-V2X-Seq dataset, validating the effectiveness of the model in cooperative trajectory prediction in V2X scenarios.",
      "published_utc": "2025-09-19T13:50:49Z",
      "updated_utc": "2025-09-19T13:50:49Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.MA",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2509.15984v1",
      "abs_url": "http://arxiv.org/abs/2509.15984v1"
    },
    "2509.15513v1": {
      "arxiv_id": "2509.15513v1",
      "title": "KoopCast: Trajectory Forecasting via Koopman Operators",
      "authors": [
        "Jungjin Lee",
        "Jaeuk Shin",
        "Gihwan Kim",
        "Joonho Han",
        "Insoon Yang"
      ],
      "summary": "We present KoopCast, a lightweight yet efficient model for trajectory forecasting in general dynamic environments. Our approach leverages Koopman operator theory, which enables a linear representation of nonlinear dynamics by lifting trajectories into a higher-dimensional space. The framework follows a two-stage design: first, a probabilistic neural goal estimator predicts plausible long-term targets, specifying where to go; second, a Koopman operator-based refinement module incorporates intention and history into a nonlinear feature space, enabling linear prediction that dictates how to go. This dual structure not only ensures strong predictive accuracy but also inherits the favorable properties of linear operators while faithfully capturing nonlinear dynamics. As a result, our model offers three key advantages: (i) competitive accuracy, (ii) interpretability grounded in Koopman spectral theory, and (iii) low-latency deployment. We validate these benefits on ETH/UCY, the Waymo Open Motion Dataset, and nuScenes, which feature rich multi-agent interactions and map-constrained nonlinear motion. Across benchmarks, KoopCast consistently delivers high predictive accuracy together with mode-level interpretability and practical efficiency.",
      "published_utc": "2025-09-19T01:27:53Z",
      "updated_utc": "2025-09-19T01:27:53Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.RO",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2509.15513v1",
      "abs_url": "http://arxiv.org/abs/2509.15513v1"
    },
    "2509.14801v1": {
      "arxiv_id": "2509.14801v1",
      "title": "STEP: Structured Training and Evaluation Platform for benchmarking trajectory prediction models",
      "authors": [
        "Julian F. Schumann",
        "Anna Mészáros",
        "Jens Kober",
        "Arkady Zgonnikov"
      ],
      "summary": "While trajectory prediction plays a critical role in enabling safe and effective path-planning in automated vehicles, standardized practices for evaluating such models remain underdeveloped. Recent efforts have aimed to unify dataset formats and model interfaces for easier comparisons, yet existing frameworks often fall short in supporting heterogeneous traffic scenarios, joint prediction models, or user documentation. In this work, we introduce STEP -- a new benchmarking framework that addresses these limitations by providing a unified interface for multiple datasets, enforcing consistent training and evaluation conditions, and supporting a wide range of prediction models. We demonstrate the capabilities of STEP in a number of experiments which reveal 1) the limitations of widely-used testing procedures, 2) the importance of joint modeling of agents for better predictions of interactions, and 3) the vulnerability of current state-of-the-art models against both distribution shifts and targeted attacks by adversarial agents. With STEP, we aim to shift the focus from the ``leaderboard'' approach to deeper insights about model behavior and generalization in complex multi-agent settings.",
      "published_utc": "2025-09-18T09:56:16Z",
      "updated_utc": "2025-09-18T09:56:16Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2509.14801v1",
      "abs_url": "http://arxiv.org/abs/2509.14801v1"
    },
    "2509.12151v1": {
      "arxiv_id": "2509.12151v1",
      "title": "Learning Contact Dynamics for Control with Action-conditioned Face Interaction Graph Networks",
      "authors": [
        "Zongyao Yi",
        "Joachim Hertzberg",
        "Martin Atzmueller"
      ],
      "summary": "We present a learnable physics simulator that provides accurate motion and force-torque prediction of robot end effectors in contact-rich manipulation. The proposed model extends the state-of-the-art GNN-based simulator (FIGNet) with novel node and edge types, enabling action-conditional predictions for control and state estimation tasks. In simulation, the MPC agent using our model matches the performance of the same controller with the ground truth dynamics model in a challenging peg-in-hole task, while in the real-world experiment, our model achieves a 50% improvement in motion prediction accuracy and 3$\\times$ increase in force-torque prediction precision over the baseline physics simulator. Source code and data are publicly available.",
      "published_utc": "2025-09-15T17:15:31Z",
      "updated_utc": "2025-09-15T17:15:31Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2509.12151v1",
      "abs_url": "http://arxiv.org/abs/2509.12151v1"
    },
    "2509.10426v2": {
      "arxiv_id": "2509.10426v2",
      "title": "DECAMP: Towards Scene-Consistent Multi-Agent Motion Prediction with Disentangled Context-Aware Pre-Training",
      "authors": [
        "Jianxin Shi",
        "Zengqi Peng",
        "Xiaolong Chen",
        "Tianyu Wo",
        "Jun Ma"
      ],
      "summary": "Trajectory prediction is a critical component of autonomous driving, essential for ensuring both safety and efficiency on the road. However, traditional approaches often struggle with the scarcity of labeled data and exhibit suboptimal performance in multi-agent prediction scenarios. To address these challenges, we introduce a disentangled context-aware pre-training framework for multi-agent motion prediction, named DECAMP. Unlike existing methods that entangle representation learning with pretext tasks, our framework decouples behavior pattern learning from latent feature reconstruction, prioritizing interpretable dynamics and thereby enhancing scene representation for downstream prediction. Additionally, our framework incorporates context-aware representation learning alongside collaborative spatial-motion pretext tasks, which enables joint optimization of structural and intentional reasoning while capturing the underlying dynamic intentions. Our experiments on the Argoverse 2 benchmark showcase the superior performance of our method, and the results attained underscore its effectiveness in multi-agent motion forecasting. To the best of our knowledge, this is the first context autoencoder framework for multi-agent motion forecasting in autonomous driving. The code and models will be made publicly available.",
      "published_utc": "2025-09-12T17:29:02Z",
      "updated_utc": "2025-09-17T06:22:52Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.MA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2509.10426v2",
      "abs_url": "http://arxiv.org/abs/2509.10426v2"
    },
    "2509.10096v1": {
      "arxiv_id": "2509.10096v1",
      "title": "HHI-Assist: A Dataset and Benchmark of Human-Human Interaction in Physical Assistance Scenario",
      "authors": [
        "Saeed Saadatnejad",
        "Reyhaneh Hosseininejad",
        "Jose Barreiros",
        "Katherine M. Tsui",
        "Alexandre Alahi"
      ],
      "summary": "The increasing labor shortage and aging population underline the need for assistive robots to support human care recipients. To enable safe and responsive assistance, robots require accurate human motion prediction in physical interaction scenarios. However, this remains a challenging task due to the variability of assistive settings and the complexity of coupled dynamics in physical interactions. In this work, we address these challenges through two key contributions: (1) HHI-Assist, a dataset comprising motion capture clips of human-human interactions in assistive tasks; and (2) a conditional Transformer-based denoising diffusion model for predicting the poses of interacting agents. Our model effectively captures the coupled dynamics between caregivers and care receivers, demonstrating improvements over baselines and strong generalization to unseen scenarios. By advancing interaction-aware motion prediction and introducing a new dataset, our work has the potential to significantly enhance robotic assistance policies. The dataset and code are available at: https://sites.google.com/view/hhi-assist/home",
      "published_utc": "2025-09-12T09:38:17Z",
      "updated_utc": "2025-09-12T09:38:17Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2509.10096v1",
      "abs_url": "http://arxiv.org/abs/2509.10096v1"
    },
    "2509.09210v1": {
      "arxiv_id": "2509.09210v1",
      "title": "ProgD: Progressive Multi-scale Decoding with Dynamic Graphs for Joint Multi-agent Motion Forecasting",
      "authors": [
        "Xing Gao",
        "Zherui Huang",
        "Weiyao Lin",
        "Xiao Sun"
      ],
      "summary": "Accurate motion prediction of surrounding agents is crucial for the safe planning of autonomous vehicles. Recent advancements have extended prediction techniques from individual agents to joint predictions of multiple interacting agents, with various strategies to address complex interactions within future motions of agents. However, these methods overlook the evolving nature of these interactions. To address this limitation, we propose a novel progressive multi-scale decoding strategy, termed ProgD, with the help of dynamic heterogeneous graph-based scenario modeling. In particular, to explicitly and comprehensively capture the evolving social interactions in future scenarios, given their inherent uncertainty, we design a progressive modeling of scenarios with dynamic heterogeneous graphs. With the unfolding of such dynamic heterogeneous graphs, a factorized architecture is designed to process the spatio-temporal dependencies within future scenarios and progressively eliminate uncertainty in future motions of multiple agents. Furthermore, a multi-scale decoding procedure is incorporated to improve on the future scenario modeling and consistent prediction of agents' future motion. The proposed ProgD achieves state-of-the-art performance on the INTERACTION multi-agent prediction benchmark, ranking $1^{st}$, and the Argoverse 2 multi-world forecasting benchmark.",
      "published_utc": "2025-09-11T07:36:54Z",
      "updated_utc": "2025-09-11T07:36:54Z",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2509.09210v1",
      "abs_url": "http://arxiv.org/abs/2509.09210v1"
    },
    "2509.01836v1": {
      "arxiv_id": "2509.01836v1",
      "title": "Multi-vessel Interaction-Aware Trajectory Prediction and Collision Risk Assessment",
      "authors": [
        "Md Mahbub Alam",
        "Jose F. Rodrigues-Jr",
        "Gabriel Spadon"
      ],
      "summary": "Accurate vessel trajectory prediction is essential for enhancing situational awareness and preventing collisions. Still, existing data-driven models are constrained mainly to single-vessel forecasting, overlooking vessel interactions, navigation rules, and explicit collision risk assessment. We present a transformer-based framework for multi-vessel trajectory prediction with integrated collision risk analysis. For a given target vessel, the framework identifies nearby vessels. It jointly predicts their future trajectories through parallel streams encoding kinematic and derived physical features, causal convolutions for temporal locality, spatial transformations for positional encoding, and hybrid positional embeddings that capture both local motion patterns and long-range dependencies. Evaluated on large-scale real-world AIS data using joint multi-vessel metrics, the model demonstrates superior forecasting capabilities beyond traditional single-vessel displacement errors. By simulating interactions among predicted trajectories, the framework further quantifies potential collision risks, offering actionable insights to strengthen maritime safety and decision support.",
      "published_utc": "2025-09-01T23:38:01Z",
      "updated_utc": "2025-09-01T23:38:01Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2509.01836v1",
      "abs_url": "http://arxiv.org/abs/2509.01836v1"
    },
    "2508.21043v2": {
      "arxiv_id": "2508.21043v2",
      "title": "HITTER: A HumanoId Table TEnnis Robot via Hierarchical Planning and Learning",
      "authors": [
        "Zhi Su",
        "Bike Zhang",
        "Nima Rahmanian",
        "Yuman Gao",
        "Qiayuan Liao",
        "Caitlin Regan",
        "Koushil Sreenath",
        "S. Shankar Sastry"
      ],
      "summary": "Humanoid robots have recently achieved impressive progress in locomotion and whole-body control, yet they remain constrained in tasks that demand rapid interaction with dynamic environments through manipulation. Table tennis exemplifies such a challenge: with ball speeds exceeding 5 m/s, players must perceive, predict, and act within sub-second reaction times, requiring both agility and precision. To address this, we present a hierarchical framework for humanoid table tennis that integrates a model-based planner for ball trajectory prediction and racket target planning with a reinforcement learning-based whole-body controller. The planner determines striking position, velocity and timing, while the controller generates coordinated arm and leg motions that mimic human strikes and maintain stability and agility across consecutive rallies. Moreover, to encourage natural movements, human motion references are incorporated during training. We validate our system on a general-purpose humanoid robot, achieving up to 106 consecutive shots with a human opponent and sustained exchanges against another humanoid. These results demonstrate real-world humanoid table tennis with sub-second reactive control, marking a step toward agile and interactive humanoid behaviors.",
      "published_utc": "2025-08-28T17:49:12Z",
      "updated_utc": "2025-09-04T13:32:15Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2508.21043v2",
      "abs_url": "http://arxiv.org/abs/2508.21043v2"
    },
    "2508.20812v1": {
      "arxiv_id": "2508.20812v1",
      "title": "Uncertainty Aware-Predictive Control Barrier Functions: Safer Human Robot Interaction through Probabilistic Motion Forecasting",
      "authors": [
        "Lorenzo Busellato",
        "Federico Cunico",
        "Diego Dall'Alba",
        "Marco Emporio",
        "Andrea Giachetti",
        "Riccardo Muradore",
        "Marco Cristani"
      ],
      "summary": "To enable flexible, high-throughput automation in settings where people and robots share workspaces, collaborative robotic cells must reconcile stringent safety guarantees with the need for responsive and effective behavior. A dynamic obstacle is the stochastic, task-dependent variability of human motion: when robots fall back on purely reactive or worst-case envelopes, they brake unnecessarily, stall task progress, and tamper with the fluidity that true Human-Robot Interaction demands. In recent years, learning-based human-motion prediction has rapidly advanced, although most approaches produce worst-case scenario forecasts that often do not treat prediction uncertainty in a well-structured way, resulting in over-conservative planning algorithms, limiting their flexibility. We introduce Uncertainty-Aware Predictive Control Barrier Functions (UA-PCBFs), a unified framework that fuses probabilistic human hand motion forecasting with the formal safety guarantees of Control Barrier Functions. In contrast to other variants, our framework allows for dynamic adjustment of the safety margin thanks to the human motion uncertainty estimation provided by a forecasting module. Thanks to uncertainty estimation, UA-PCBFs empower collaborative robots with a deeper understanding of future human states, facilitating more fluid and intelligent interactions through informed motion planning. We validate UA-PCBFs through comprehensive real-world experiments with an increasing level of realism, including automated setups (to perform exactly repeatable motions) with a robotic hand and direct human-robot interactions (to validate promptness, usability, and human confidence). Relative to state-of-the-art HRI architectures, UA-PCBFs show better performance in task-critical metrics, significantly reducing the number of violations of the robot's safe space during interaction with respect to the state-of-the-art.",
      "published_utc": "2025-08-28T14:11:26Z",
      "updated_utc": "2025-08-28T14:11:26Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2508.20812v1",
      "abs_url": "http://arxiv.org/abs/2508.20812v1"
    },
    "2509.09696v2": {
      "arxiv_id": "2509.09696v2",
      "title": "DCHO: A Decomposition-Composition Framework for Predicting Higher-Order Brain Connectivity to Enhance Diverse Downstream Applications",
      "authors": [
        "Weibin Li",
        "Wendu Li",
        "Quanying Liu"
      ],
      "summary": "Higher-order brain connectivity (HOBC), which captures interactions among three or more brain regions, provides richer organizational information than traditional pairwise functional connectivity (FC). Recent studies have begun to infer latent HOBC from noninvasive imaging data, but they mainly focus on static analyses, limiting their applicability in dynamic prediction tasks. To address this gap, we propose DCHO, a unified approach for modeling and forecasting the temporal evolution of HOBC based on a Decomposition-Composition framework, which is applicable to both non-predictive tasks (state classification) and predictive tasks (brain dynamics forecasting). DCHO adopts a decomposition-composition strategy that reformulates the prediction task into two manageable subproblems: HOBC inference and latent trajectory prediction. In the inference stage, we propose a dual-view encoder to extract multiscale topological features and a latent combinatorial learner to capture high-level HOBC information. In the forecasting stage, we introduce a latent-space prediction loss to enhance the modeling of temporal trajectories. Extensive experiments on multiple neuroimaging datasets demonstrate that DCHO achieves superior performance in both non-predictive tasks (state classification) and predictive tasks (brain dynamics forecasting), significantly outperforming existing methods.",
      "published_utc": "2025-08-27T13:01:35Z",
      "updated_utc": "2025-12-29T04:12:35Z",
      "primary_category": "q-bio.NC",
      "categories": [
        "q-bio.NC",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2509.09696v2",
      "abs_url": "http://arxiv.org/abs/2509.09696v2"
    },
    "2601.02082v1": {
      "arxiv_id": "2601.02082v1",
      "title": "Realistic adversarial scenario generation via human-like pedestrian model for autonomous vehicle control parameter optimisation",
      "authors": [
        "Yueyang Wang",
        "Mehmet Dogar",
        "Gustav Markkula"
      ],
      "summary": "Autonomous vehicles (AVs) are rapidly advancing and are expected to play a central role in future mobility. Ensuring their safe deployment requires reliable interaction with other road users, not least pedestrians. Direct testing on public roads is costly and unsafe for rare but critical interactions, making simulation a practical alternative. Within simulation-based testing, adversarial scenarios are widely used to probe safety limits, but many prioritise difficulty over realism, producing exaggerated behaviours which may result in AV controllers that are overly conservative. We propose an alternative method, instead using a cognitively inspired pedestrian model featuring both inter-individual and intra-individual variability to generate behaviourally plausible adversarial scenarios. We provide a proof of concept demonstration of this method's potential for AV control optimisation, in closed-loop testing and tuning of an AV controller. Our results show that replacing the rule-based CARLA pedestrian with the human-like model yields more realistic gap acceptance patterns and smoother vehicle decelerations. Unsafe interactions occur only for certain pedestrian individuals and conditions, underscoring the importance of human variability in AV testing. Adversarial scenarios generated by this model can be used to optimise AV control towards safer and more efficient behaviour. Overall, this work illustrates how incorporating human-like road user models into simulation-based adversarial testing can enhance the credibility of AV evaluation and provide a practical basis to behaviourally informed controller optimisation.",
      "published_utc": "2026-01-05T13:10:32Z",
      "updated_utc": "2026-01-05T13:10:32Z",
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2601.02082v1",
      "abs_url": "http://arxiv.org/abs/2601.02082v1"
    },
    "2601.01989v1": {
      "arxiv_id": "2601.01989v1",
      "title": "VIT-Ped: Visionary Intention Transformer for Pedestrian Behavior Analysis",
      "authors": [
        "Aly R. Elkammar",
        "Karim M. Gamaleldin",
        "Catherine M. Elias"
      ],
      "summary": "Pedestrian Intention prediction is one of the key technologies in the transition from level 3 to level 4 autonomous driving. To understand pedestrian crossing behaviour, several elements and features should be taken into consideration to make the roads of tomorrow safer for everybody. We introduce a transformer / video vision transformer based algorithm of different sizes which uses different data modalities .We evaluated our algorithms on popular pedestrian behaviour dataset, JAAD, and have reached SOTA performance and passed the SOTA in metrics like Accuracy, AUC and F1-score. The advantages brought by different model design choices are investigated via extensive ablation studies.",
      "published_utc": "2026-01-05T10:48:12Z",
      "updated_utc": "2026-01-05T10:48:12Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2601.01989v1",
      "abs_url": "http://arxiv.org/abs/2601.01989v1"
    },
    "2512.24129v1": {
      "arxiv_id": "2512.24129v1",
      "title": "ROBOPOL: Social Robotics Meets Vehicular Communications for Cooperative Automated Driving",
      "authors": [
        "Manuel Bied",
        "John Arockiasamy",
        "Andy Comeca",
        "Maximilian Schrapel",
        "Victoria Yang",
        "Alexey Rolich",
        "Barbara Bruno",
        "Maike Schwammberger",
        "Dieter Fiems",
        "Alexey Vinel"
      ],
      "summary": "On the way towards full autonomy, sharing roads between automated vehicles and human actors in so-called mixed traffic is unavoidable. Moreover, even if all vehicles on the road were autonomous, pedestrians would still be crossing the streets. We propose social robots as moderators between autonomous vehicles and vulnerable road users (VRU). To this end, we identify four enablers requiring integration: (1) advanced perception, allowing the robot to see the environment; (2) vehicular communications allowing connected vehicles to share intentions and the robot to send guiding commands; (3) social human-robot interaction allowing the robot to effectively communicate with VRUs and drivers; (4) formal specification allowing the robot to understand traffic and plan accordingly. This paper presents an overview of the key enablers and report on a first proof-of-concept integration of the first three enablers envisioning a social robot advising pedestrians in scenarios with a cooperative automated e-bike.",
      "published_utc": "2025-12-30T10:30:17Z",
      "updated_utc": "2025-12-30T10:30:17Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2512.24129v1",
      "abs_url": "http://arxiv.org/abs/2512.24129v1"
    },
    "2511.20020v1": {
      "arxiv_id": "2511.20020v1",
      "title": "ACIT: Attention-Guided Cross-Modal Interaction Transformer for Pedestrian Crossing Intention Prediction",
      "authors": [
        "Yuanzhe Li",
        "Steffen Müller"
      ],
      "summary": "Predicting pedestrian crossing intention is crucial for autonomous vehicles to prevent pedestrian-related collisions. However, effectively extracting and integrating complementary cues from different types of data remains one of the major challenges. This paper proposes an attention-guided cross-modal interaction Transformer (ACIT) for pedestrian crossing intention prediction. ACIT leverages six visual and motion modalities, which are grouped into three interaction pairs: (1) Global semantic map and global optical flow, (2) Local RGB image and local optical flow, and (3) Ego-vehicle speed and pedestrian's bounding box. Within each visual interaction pair, a dual-path attention mechanism enhances salient regions within the primary modality through intra-modal self-attention and facilitates deep interactions with the auxiliary modality (i.e., optical flow) via optical flow-guided attention. Within the motion interaction pair, cross-modal attention is employed to model the cross-modal dynamics, enabling the effective extraction of complementary motion features. Beyond pairwise interactions, a multi-modal feature fusion module further facilitates cross-modal interactions at each time step. Furthermore, a Transformer-based temporal feature aggregation module is introduced to capture sequential dependencies. Experimental results demonstrate that ACIT outperforms state-of-the-art methods, achieving accuracy rates of 70% and 89% on the JAADbeh and JAADall datasets, respectively. Extensive ablation studies are further conducted to investigate the contribution of different modules of ACIT.",
      "published_utc": "2025-11-25T07:41:11Z",
      "updated_utc": "2025-11-25T07:41:11Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2511.20020v1",
      "abs_url": "http://arxiv.org/abs/2511.20020v1"
    },
    "2511.20011v1": {
      "arxiv_id": "2511.20011v1",
      "title": "Multi-Context Fusion Transformer for Pedestrian Crossing Intention Prediction in Urban Environments",
      "authors": [
        "Yuanzhe Li",
        "Hang Zhong",
        "Steffen Müller"
      ],
      "summary": "Pedestrian crossing intention prediction is essential for autonomous vehicles to improve pedestrian safety and reduce traffic accidents. However, accurate pedestrian intention prediction in urban environments remains challenging due to the multitude of factors affecting pedestrian behavior. In this paper, we propose a multi-context fusion Transformer (MFT) that leverages diverse numerical contextual attributes across four key dimensions, encompassing pedestrian behavior context, environmental context, pedestrian localization context and vehicle motion context, to enable accurate pedestrian intention prediction. MFT employs a progressive fusion strategy, where mutual intra-context attention enables reciprocal interactions within each context, thereby facilitating feature sequence fusion and yielding a context token as a context-specific representation. This is followed by mutual cross-context attention, which integrates features across contexts with a global CLS token serving as a compact multi-context representation. Finally, guided intra-context attention refines context tokens within each context through directed interactions, while guided cross-context attention strengthens the global CLS token to promote multi-context fusion via guided information propagation, yielding deeper and more efficient integration. Experimental results validate the superiority of MFT over state-of-the-art methods, achieving accuracy rates of 73%, 93%, and 90% on the JAADbeh, JAADall, and PIE datasets, respectively. Extensive ablation studies are further conducted to investigate the effectiveness of the network architecture and contribution of different input context. Our code is open-source: https://github.com/ZhongHang0307/Multi-Context-Fusion-Transformer.",
      "published_utc": "2025-11-25T07:24:49Z",
      "updated_utc": "2025-11-25T07:24:49Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2511.20011v1",
      "abs_url": "http://arxiv.org/abs/2511.20011v1"
    },
    "2511.20008v1": {
      "arxiv_id": "2511.20008v1",
      "title": "Pedestrian Crossing Intention Prediction Using Multimodal Fusion Network",
      "authors": [
        "Yuanzhe Li",
        "Steffen Müller"
      ],
      "summary": "Pedestrian crossing intention prediction is essential for the deployment of autonomous vehicles (AVs) in urban environments. Ideal prediction provides AVs with critical environmental cues, thereby reducing the risk of pedestrian-related collisions. However, the prediction task is challenging due to the diverse nature of pedestrian behavior and its dependence on multiple contextual factors. This paper proposes a multimodal fusion network that leverages seven modality features from both visual and motion branches, aiming to effectively extract and integrate complementary cues across different modalities. Specifically, motion and visual features are extracted from the raw inputs using multiple Transformer-based extraction modules. Depth-guided attention module leverages depth information to guide attention towards salient regions in another modality through comprehensive spatial feature interactions. To account for the varying importance of different modalities and frames, modality attention and temporal attention are designed to selectively emphasize informative modalities and effectively capture temporal dependencies. Extensive experiments on the JAAD dataset validate the effectiveness of the proposed network, achieving superior performance compared to the baseline methods.",
      "published_utc": "2025-11-25T07:18:12Z",
      "updated_utc": "2025-11-25T07:18:12Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2511.20008v1",
      "abs_url": "http://arxiv.org/abs/2511.20008v1"
    },
    "2511.19109v1": {
      "arxiv_id": "2511.19109v1",
      "title": "HABIT: Human Action Benchmark for Interactive Traffic in CARLA",
      "authors": [
        "Mohan Ramesh",
        "Mark Azer",
        "Fabian B. Flohr"
      ],
      "summary": "Current autonomous driving (AD) simulations are critically limited by their inadequate representation of realistic and diverse human behavior, which is essential for ensuring safety and reliability. Existing benchmarks often simplify pedestrian interactions, failing to capture complex, dynamic intentions and varied responses critical for robust system deployment. To overcome this, we introduce HABIT (Human Action Benchmark for Interactive Traffic), a high-fidelity simulation benchmark. HABIT integrates real-world human motion, sourced from mocap and videos, into CARLA (Car Learning to Act, a full autonomous driving simulator) via a modular, extensible, and physically consistent motion retargeting pipeline. From an initial pool of approximately 30,000 retargeted motions, we curate 4,730 traffic-compatible pedestrian motions, standardized in SMPL format for physically consistent trajectories. HABIT seamlessly integrates with CARLA's Leaderboard, enabling automated scenario generation and rigorous agent evaluation. Our safety metrics, including Abbreviated Injury Scale (AIS) and False Positive Braking Rate (FPBR), reveal critical failure modes in state-of-the-art AD agents missed by prior evaluations. Evaluating three state-of-the-art autonomous driving agents, InterFuser, TransFuser, and BEVDriver, demonstrates how HABIT exposes planner weaknesses that remain hidden in scripted simulations. Despite achieving close or equal to zero collisions per kilometer on the CARLA Leaderboard, the autonomous agents perform notably worse on HABIT, with up to 7.43 collisions/km and a 12.94% AIS 3+ injury risk, and they brake unnecessarily in up to 33% of cases. All components are publicly released to support reproducible, pedestrian-aware AI research.",
      "published_utc": "2025-11-24T13:43:39Z",
      "updated_utc": "2025-11-24T13:43:39Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2511.19109v1",
      "abs_url": "http://arxiv.org/abs/2511.19109v1"
    },
    "2511.00858v1": {
      "arxiv_id": "2511.00858v1",
      "title": "Occlusion-Aware Diffusion Model for Pedestrian Intention Prediction",
      "authors": [
        "Yu Liu",
        "Zhijie Liu",
        "Zedong Yang",
        "You-Fu Li",
        "He Kong"
      ],
      "summary": "Predicting pedestrian crossing intentions is crucial for the navigation of mobile robots and intelligent vehicles. Although recent deep learning-based models have shown significant success in forecasting intentions, few consider incomplete observation under occlusion scenarios. To tackle this challenge, we propose an Occlusion-Aware Diffusion Model (ODM) that reconstructs occluded motion patterns and leverages them to guide future intention prediction. During the denoising stage, we introduce an occlusion-aware diffusion transformer architecture to estimate noise features associated with occluded patterns, thereby enhancing the model's ability to capture contextual relationships in occluded semantic scenarios. Furthermore, an occlusion mask-guided reverse process is introduced to effectively utilize observation information, reducing the accumulation of prediction errors and enhancing the accuracy of reconstructed motion features. The performance of the proposed method under various occlusion scenarios is comprehensively evaluated and compared with existing methods on popular benchmarks, namely PIE and JAAD. Extensive experimental results demonstrate that the proposed method achieves more robust performance than existing methods in the literature.",
      "published_utc": "2025-11-02T08:49:07Z",
      "updated_utc": "2025-11-02T08:49:07Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2511.00858v1",
      "abs_url": "http://arxiv.org/abs/2511.00858v1"
    },
    "2510.20158v1": {
      "arxiv_id": "2510.20158v1",
      "title": "Monocular Visual 8D Pose Estimation for Articulated Bicycles and Cyclists",
      "authors": [
        "Eduardo R. Corral-Soto",
        "Yang Liu",
        "Yuan Ren",
        "Bai Dongfeng",
        "Liu Bingbing"
      ],
      "summary": "In Autonomous Driving, cyclists belong to the safety-critical class of Vulnerable Road Users (VRU), and accurate estimation of their pose is critical for cyclist crossing intention classification, behavior prediction, and collision avoidance. Unlike rigid objects, articulated bicycles are composed of movable rigid parts linked by joints and constrained by a kinematic structure. 6D pose methods can estimate the 3D rotation and translation of rigid bicycles, but 6D becomes insufficient when the steering/pedals angles of the bicycle vary. That is because: 1) varying the articulated pose of the bicycle causes its 3D bounding box to vary as well, and 2) the 3D box orientation is not necessarily aligned to the orientation of the steering which determines the actual intended travel direction. In this work, we introduce a method for category-level 8D pose estimation for articulated bicycles and cyclists from a single RGB image. Besides being able to estimate the 3D translation and rotation of a bicycle from a single image, our method also estimates the rotations of its steering handles and pedals with respect to the bicycle body frame. These two new parameters enable the estimation of a more fine-grained bicycle pose state and travel direction. Our proposed model jointly estimates the 8D pose and the 3D Keypoints of articulated bicycles, and trains with a mix of synthetic and real image data to generalize on real images. We include an evaluation section where we evaluate the accuracy of our estimated 8D pose parameters, and our method shows promising results by achieving competitive scores when compared against state-of-the-art category-level 6D pose estimators that use rigid canonical object templates for matching.",
      "published_utc": "2025-10-23T03:17:22Z",
      "updated_utc": "2025-10-23T03:17:22Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2510.20158v1",
      "abs_url": "http://arxiv.org/abs/2510.20158v1"
    },
    "2510.15673v1": {
      "arxiv_id": "2510.15673v1",
      "title": "Valeo Near-Field: a novel dataset for pedestrian intent detection",
      "authors": [
        "Antonyo Musabini",
        "Rachid Benmokhtar",
        "Jagdish Bhanushali",
        "Victor Galizzi",
        "Bertrand Luvison",
        "Xavier Perrotton"
      ],
      "summary": "This paper presents a novel dataset aimed at detecting pedestrians' intentions as they approach an ego-vehicle. The dataset comprises synchronized multi-modal data, including fisheye camera feeds, lidar laser scans, ultrasonic sensor readings, and motion capture-based 3D body poses, collected across diverse real-world scenarios. Key contributions include detailed annotations of 3D body joint positions synchronized with fisheye camera images, as well as accurate 3D pedestrian positions extracted from lidar data, facilitating robust benchmarking for perception algorithms. We release a portion of the dataset along with a comprehensive benchmark suite, featuring evaluation metrics for accuracy, efficiency, and scalability on embedded systems. By addressing real-world challenges such as sensor occlusions, dynamic environments, and hardware constraints, this dataset offers a unique resource for developing and evaluating state-of-the-art algorithms in pedestrian detection, 3D pose estimation and 4D trajectory and intention prediction. Additionally, we provide baseline performance metrics using custom neural network architectures and suggest future research directions to encourage the adoption and enhancement of the dataset. This work aims to serve as a foundation for researchers seeking to advance the capabilities of intelligent vehicles in near-field scenarios.",
      "published_utc": "2025-10-17T14:02:54Z",
      "updated_utc": "2025-10-17T14:02:54Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2510.15673v1",
      "abs_url": "http://arxiv.org/abs/2510.15673v1"
    },
    "2509.14303v1": {
      "arxiv_id": "2509.14303v1",
      "title": "FlowDrive: Energy Flow Field for End-to-End Autonomous Driving",
      "authors": [
        "Hao Jiang",
        "Zhipeng Zhang",
        "Yu Gao",
        "Zhigang Sun",
        "Yiru Wang",
        "Yuwen Heng",
        "Shuo Wang",
        "Jinhao Chai",
        "Zhuo Chen",
        "Hao Zhao",
        "Hao Sun",
        "Xi Zhang",
        "Anqing Jiang",
        "Chuan Hu"
      ],
      "summary": "Recent advances in end-to-end autonomous driving leverage multi-view images to construct BEV representations for motion planning. In motion planning, autonomous vehicles need considering both hard constraints imposed by geometrically occupied obstacles (e.g., vehicles, pedestrians) and soft, rule-based semantics with no explicit geometry (e.g., lane boundaries, traffic priors). However, existing end-to-end frameworks typically rely on BEV features learned in an implicit manner, lacking explicit modeling of risk and guidance priors for safe and interpretable planning. To address this, we propose FlowDrive, a novel framework that introduces physically interpretable energy-based flow fields-including risk potential and lane attraction fields-to encode semantic priors and safety cues into the BEV space. These flow-aware features enable adaptive refinement of anchor trajectories and serve as interpretable guidance for trajectory generation. Moreover, FlowDrive decouples motion intent prediction from trajectory denoising via a conditional diffusion planner with feature-level gating, alleviating task interference and enhancing multimodal diversity. Experiments on the NAVSIM v2 benchmark demonstrate that FlowDrive achieves state-of-the-art performance with an EPDMS of 86.3, surpassing prior baselines in both safety and planning quality. The project is available at https://astrixdrive.github.io/FlowDrive.github.io/.",
      "published_utc": "2025-09-17T13:51:33Z",
      "updated_utc": "2025-09-17T13:51:33Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2509.14303v1",
      "abs_url": "http://arxiv.org/abs/2509.14303v1"
    },
    "2509.04117v1": {
      "arxiv_id": "2509.04117v1",
      "title": "DVS-PedX: Synthetic-and-Real Event-Based Pedestrian Dataset",
      "authors": [
        "Mustafa Sakhai",
        "Kaung Sithu",
        "Min Khant Soe Oke",
        "Maciej Wielgosz"
      ],
      "summary": "Event cameras like Dynamic Vision Sensors (DVS) report micro-timed brightness changes instead of full frames, offering low latency, high dynamic range, and motion robustness. DVS-PedX (Dynamic Vision Sensor Pedestrian eXploration) is a neuromorphic dataset designed for pedestrian detection and crossing-intention analysis in normal and adverse weather conditions across two complementary sources: (1) synthetic event streams generated in the CARLA simulator for controlled \"approach-cross\" scenes under varied weather and lighting; and (2) real-world JAAD dash-cam videos converted to event streams using the v2e tool, preserving natural behaviors and backgrounds. Each sequence includes paired RGB frames, per-frame DVS \"event frames\" (33 ms accumulations), and frame-level labels (crossing vs. not crossing). We also provide raw AEDAT 2.0/AEDAT 4.0 event files and AVI DVS video files and metadata for flexible re-processing. Baseline spiking neural networks (SNNs) using SpikingJelly illustrate dataset usability and reveal a sim-to-real gap, motivating domain adaptation and multimodal fusion. DVS-PedX aims to accelerate research in event-based pedestrian safety, intention prediction, and neuromorphic perception.",
      "published_utc": "2025-09-04T11:30:29Z",
      "updated_utc": "2025-09-04T11:30:29Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2509.04117v1",
      "abs_url": "http://arxiv.org/abs/2509.04117v1"
    },
    "2508.21690v1": {
      "arxiv_id": "2508.21690v1",
      "title": "Can a mobile robot learn from a pedestrian model to prevent the sidewalk salsa?",
      "authors": [
        "Olger Siebinga",
        "David Abbink"
      ],
      "summary": "Pedestrians approaching each other on a sidewalk sometimes end up in an awkward interaction known as the \"sidewalk salsa\": they both (repeatedly) deviate to the same side to avoid a collision. This provides an interesting use case to study interactions between pedestrians and mobile robots because, in the vast majority of cases, this phenomenon is avoided through a negotiation based on implicit communication. Understanding how it goes wrong and how pedestrians end up in the sidewalk salsa will therefore provide insight into the implicit communication. This understanding can be used to design safe and acceptable robotic behaviour. In a previous attempt to gain this understanding, a model of pedestrian behaviour based on the Communication-Enabled Interaction (CEI) framework was developed that can replicate the sidewalk salsa. However, it is unclear how to leverage this model in robotic planning and decision-making since it violates the assumptions of game theory, a much-used framework in planning and decision-making. Here, we present a proof-of-concept for an approach where a Reinforcement Learning (RL) agent leverages the model to learn how to interact with pedestrians. The results show that a basic RL agent successfully learned to interact with the CEI model. Furthermore, a risk-averse RL agent that had access to the perceived risk of the CEI model learned how to effectively communicate its intention through its motion and thereby substantially lowered the perceived risk, and displayed effort by the modelled pedestrian. These results show this is a promising approach and encourage further exploration.",
      "published_utc": "2025-08-29T14:56:48Z",
      "updated_utc": "2025-08-29T14:56:48Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2508.21690v1",
      "abs_url": "http://arxiv.org/abs/2508.21690v1"
    },
    "2508.19866v1": {
      "arxiv_id": "2508.19866v1",
      "title": "TrajFusionNet: Pedestrian Crossing Intention Prediction via Fusion of Sequential and Visual Trajectory Representations",
      "authors": [
        "François G. Landry",
        "Moulay A. Akhloufi"
      ],
      "summary": "With the introduction of vehicles with autonomous capabilities on public roads, predicting pedestrian crossing intention has emerged as an active area of research. The task of predicting pedestrian crossing intention involves determining whether pedestrians in the scene are likely to cross the road or not. In this work, we propose TrajFusionNet, a novel transformer-based model that combines future pedestrian trajectory and vehicle speed predictions as priors for predicting crossing intention. TrajFusionNet comprises two branches: a Sequence Attention Module (SAM) and a Visual Attention Module (VAM). The SAM branch learns from a sequential representation of the observed and predicted pedestrian trajectory and vehicle speed. Complementarily, the VAM branch enables learning from a visual representation of the predicted pedestrian trajectory by overlaying predicted pedestrian bounding boxes onto scene images. By utilizing a small number of lightweight modalities, TrajFusionNet achieves the lowest total inference time (including model runtime and data preprocessing) among current state-of-the-art approaches. In terms of performance, it achieves state-of-the-art results across the three most commonly used datasets for pedestrian crossing intention prediction.",
      "published_utc": "2025-08-27T13:29:15Z",
      "updated_utc": "2025-08-27T13:29:15Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2508.19866v1",
      "abs_url": "http://arxiv.org/abs/2508.19866v1"
    },
    "2508.15336v1": {
      "arxiv_id": "2508.15336v1",
      "title": "Predicting Road Crossing Behaviour using Pose Detection and Sequence Modelling",
      "authors": [
        "Subhasis Dasgupta",
        "Preetam Saha",
        "Agniva Roy",
        "Jaydip Sen"
      ],
      "summary": "The world is constantly moving towards AI based systems and autonomous vehicles are now reality in different parts of the world. These vehicles require sensors and cameras to detect objects and maneuver according to that. It becomes important to for such vehicles to also predict from a distant if a person is about to cross a road or not. The current study focused on predicting the intent of crossing the road by pedestrians in an experimental setup. The study involved working with deep learning models to predict poses and sequence modelling for temporal predictions. The study analysed three different sequence modelling to understand the prediction behaviour and it was found out that GRU was better in predicting the intent compared to LSTM model but 1D CNN was the best model in terms of speed. The study involved video analysis, and the output of pose detection model was integrated later on to sequence modelling techniques for an end-to-end deep learning framework for predicting road crossing intents.",
      "published_utc": "2025-08-21T08:08:50Z",
      "updated_utc": "2025-08-21T08:08:50Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2508.15336v1",
      "abs_url": "http://arxiv.org/abs/2508.15336v1"
    },
    "2507.21161v1": {
      "arxiv_id": "2507.21161v1",
      "title": "Seeing Beyond Frames: Zero-Shot Pedestrian Intention Prediction with Raw Temporal Video and Multimodal Cues",
      "authors": [
        "Pallavi Zambare",
        "Venkata Nikhil Thanikella",
        "Ying Liu"
      ],
      "summary": "Pedestrian intention prediction is essential for autonomous driving in complex urban environments. Conventional approaches depend on supervised learning over frame sequences and require extensive retraining to adapt to new scenarios. Here, we introduce BF-PIP (Beyond Frames Pedestrian Intention Prediction), a zero-shot approach built upon Gemini 2.5 Pro. It infers crossing intentions directly from short, continuous video clips enriched with structured JAAD metadata. In contrast to GPT-4V based methods that operate on discrete frames, BF-PIP processes uninterrupted temporal clips. It also incorporates bounding-box annotations and ego-vehicle speed via specialized multimodal prompts. Without any additional training, BF-PIP achieves 73% prediction accuracy, outperforming a GPT-4V baseline by 18 %. These findings illustrate that combining temporal video inputs with contextual cues enhances spatiotemporal perception and improves intent inference under ambiguous conditions. This approach paves the way for agile, retraining-free perception module in intelligent transportation system.",
      "published_utc": "2025-07-25T07:23:11Z",
      "updated_utc": "2025-07-25T07:23:11Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2507.21161v1",
      "abs_url": "http://arxiv.org/abs/2507.21161v1"
    },
    "2507.12433v1": {
      "arxiv_id": "2507.12433v1",
      "title": "Traffic-Aware Pedestrian Intention Prediction",
      "authors": [
        "Fahimeh Orvati Nia",
        "Hai Lin"
      ],
      "summary": "Accurate pedestrian intention estimation is crucial for the safe navigation of autonomous vehicles (AVs) and hence attracts a lot of research attention. However, current models often fail to adequately consider dynamic traffic signals and contextual scene information, which are critical for real-world applications. This paper presents a Traffic-Aware Spatio-Temporal Graph Convolutional Network (TA-STGCN) that integrates traffic signs and their states (Red, Yellow, Green) into pedestrian intention prediction. Our approach introduces the integration of dynamic traffic signal states and bounding box size as key features, allowing the model to capture both spatial and temporal dependencies in complex urban environments. The model surpasses existing methods in accuracy. Specifically, TA-STGCN achieves a 4.75% higher accuracy compared to the baseline model on the PIE dataset, demonstrating its effectiveness in improving pedestrian intention prediction.",
      "published_utc": "2025-07-16T17:20:36Z",
      "updated_utc": "2025-07-16T17:20:36Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2507.12433v1",
      "abs_url": "http://arxiv.org/abs/2507.12433v1"
    },
    "2507.04141v1": {
      "arxiv_id": "2507.04141v1",
      "title": "Pedestrian Intention Prediction via Vision-Language Foundation Models",
      "authors": [
        "Mohsen Azarmi",
        "Mahdi Rezaei",
        "He Wang"
      ],
      "summary": "Prediction of pedestrian crossing intention is a critical function in autonomous vehicles. Conventional vision-based methods of crossing intention prediction often struggle with generalizability, context understanding, and causal reasoning. This study explores the potential of vision-language foundation models (VLFMs) for predicting pedestrian crossing intentions by integrating multimodal data through hierarchical prompt templates. The methodology incorporates contextual information, including visual frames, physical cues observations, and ego-vehicle dynamics, into systematically refined prompts to guide VLFMs effectively in intention prediction. Experiments were conducted on three common datasets-JAAD, PIE, and FU-PIP. Results demonstrate that incorporating vehicle speed, its variations over time, and time-conscious prompts significantly enhances the prediction accuracy up to 19.8%. Additionally, optimised prompts generated via an automatic prompt engineering framework yielded 12.5% further accuracy gains. These findings highlight the superior performance of VLFMs compared to conventional vision-based models, offering enhanced generalisation and contextual understanding for autonomous driving applications.",
      "published_utc": "2025-07-05T19:39:00Z",
      "updated_utc": "2025-07-05T19:39:00Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.ET",
        "cs.LG",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2507.04141v1",
      "abs_url": "http://arxiv.org/abs/2507.04141v1"
    },
    "2506.17590v2": {
      "arxiv_id": "2506.17590v2",
      "title": "DRAMA-X: A Fine-grained Intent Prediction and Risk Reasoning Benchmark For Driving",
      "authors": [
        "Mihir Godbole",
        "Xiangbo Gao",
        "Zhengzhong Tu"
      ],
      "summary": "Understanding the short-term motion of vulnerable road users (VRUs) like pedestrians and cyclists is critical for safe autonomous driving, especially in urban scenarios with ambiguous or high-risk behaviors. While vision-language models (VLMs) have enabled open-vocabulary perception, their utility for fine-grained intent reasoning remains underexplored. Notably, no existing benchmark evaluates multi-class intent prediction in safety-critical situations, To address this gap, we introduce DRAMA-X, a fine-grained benchmark constructed from the DRAMA dataset via an automated annotation pipeline. DRAMA-X contains 5,686 accident-prone frames labeled with object bounding boxes, a nine-class directional intent taxonomy, binary risk scores, expert-generated action suggestions for the ego vehicle, and descriptive motion summaries. These annotations enable a structured evaluation of four interrelated tasks central to autonomous decision-making: object detection, intent prediction, risk assessment, and action suggestion. As a reference baseline, we propose SGG-Intent, a lightweight, training-free framework that mirrors the ego vehicle's reasoning pipeline. It sequentially generates a scene graph from visual input using VLM-backed detectors, infers intent, assesses risk, and recommends an action using a compositional reasoning stage powered by a large language model. We evaluate a range of recent VLMs, comparing performance across all four DRAMA-X tasks. Our experiments demonstrate that scene-graph-based reasoning enhances intent prediction and risk assessment, especially when contextual cues are explicitly modeled.",
      "published_utc": "2025-06-21T05:01:42Z",
      "updated_utc": "2025-08-09T06:21:14Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.17590v2",
      "abs_url": "http://arxiv.org/abs/2506.17590v2"
    },
    "2506.05883v1": {
      "arxiv_id": "2506.05883v1",
      "title": "HMVLM: Multistage Reasoning-Enhanced Vision-Language Model for Long-Tailed Driving Scenarios",
      "authors": [
        "Daming Wang",
        "Yuhao Song",
        "Zijian He",
        "Kangliang Chen",
        "Xing Pan",
        "Lu Deng",
        "Weihao Gu"
      ],
      "summary": "We present HaoMo Vision-Language Model (HMVLM), an end-to-end driving framework that implements the slow branch of a cognitively inspired fast-slow architecture. A fast controller outputs low-level steering, throttle, and brake commands, while a slow planner-a large vision-language model-generates high-level intents such as \"yield to pedestrian\" or \"merge after the truck\" without compromising latency. HMVLM introduces three upgrades: (1) selective five-view prompting with an embedded 4s history of ego kinematics, (2) multi-stage chain-of-thought (CoT) prompting that enforces a Scene Understanding -> Driving Decision -> Trajectory Inference reasoning flow, and (3) spline-based trajectory post-processing that removes late-stage jitter and sharp turns. Trained on the Waymo Open Dataset, these upgrades enable HMVLM to achieve a Rater Feedback Score (RFS) of 7.7367, securing 2nd place in the 2025 Waymo Vision-based End-to-End (E2E) Driving Challenge and surpassing the public baseline by 2.77%.",
      "published_utc": "2025-06-06T08:51:06Z",
      "updated_utc": "2025-06-06T08:51:06Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.05883v1",
      "abs_url": "http://arxiv.org/abs/2506.05883v1"
    },
    "2504.06292v1": {
      "arxiv_id": "2504.06292v1",
      "title": "Temporal-contextual Event Learning for Pedestrian Crossing Intent Prediction",
      "authors": [
        "Hongbin Liang",
        "Hezhe Qiao",
        "Wei Huang",
        "Qizhou Wang",
        "Mingsheng Shang",
        "Lin Chen"
      ],
      "summary": "Ensuring the safety of vulnerable road users through accurate prediction of pedestrian crossing intention (PCI) plays a crucial role in the context of autonomous and assisted driving. Analyzing the set of observation video frames in ego-view has been widely used in most PCI prediction methods to forecast the cross intent. However, they struggle to capture the critical events related to pedestrian behaviour along the temporal dimension due to the high redundancy of the video frames, which results in the sub-optimal performance of PCI prediction. Our research addresses the challenge by introducing a novel approach called \\underline{T}emporal-\\underline{c}ontextual Event \\underline{L}earning (TCL). The TCL is composed of the Temporal Merging Module (TMM), which aims to manage the redundancy by clustering the observed video frames into multiple key temporal events. Then, the Contextual Attention Block (CAB) is employed to adaptively aggregate multiple event features along with visual and non-visual data. By synthesizing the temporal feature extraction and contextual attention on the key information across the critical events, TCL can learn expressive representation for the PCI prediction. Extensive experiments are carried out on three widely adopted datasets, including PIE, JAAD-beh, and JAAD-all. The results show that TCL substantially surpasses the state-of-the-art methods. Our code can be accessed at https://github.com/dadaguailhb/TCL.",
      "published_utc": "2025-04-04T10:44:24Z",
      "updated_utc": "2025-04-04T10:44:24Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.06292v1",
      "abs_url": "http://arxiv.org/abs/2504.06292v1"
    },
    "2503.08437v1": {
      "arxiv_id": "2503.08437v1",
      "title": "ICPR 2024 Competition on Rider Intention Prediction",
      "authors": [
        "Shankar Gangisetty",
        "Abdul Wasi",
        "Shyam Nandan Rai",
        "C. V. Jawahar",
        "Sajay Raj",
        "Manish Prajapati",
        "Ayesha Choudhary",
        "Aaryadev Chandra",
        "Dev Chandan",
        "Shireen Chand",
        "Suvaditya Mukherjee"
      ],
      "summary": "The recent surge in the vehicle market has led to an alarming increase in road accidents. This underscores the critical importance of enhancing road safety measures, particularly for vulnerable road users like motorcyclists. Hence, we introduce the rider intention prediction (RIP) competition that aims to address challenges in rider safety by proactively predicting maneuvers before they occur, thereby strengthening rider safety. This capability enables the riders to react to the potential incorrect maneuvers flagged by advanced driver assistance systems (ADAS). We collect a new dataset, namely, rider action anticipation dataset (RAAD) for the competition consisting of two tasks: single-view RIP and multi-view RIP. The dataset incorporates a spectrum of traffic conditions and challenging navigational maneuvers on roads with varying lighting conditions. For the competition, we received seventy-five registrations and five team submissions for inference of which we compared the methods of the top three performing teams on both the RIP tasks: one state-space model (Mamba2) and two learning-based approaches (SVM and CNN-LSTM). The results indicate that the state-space model outperformed the other methods across the entire dataset, providing a balanced performance across maneuver classes. The SVM-based RIP method showed the second-best performance when using random sampling and SMOTE. However, the CNN-LSTM method underperformed, primarily due to class imbalance issues, particularly struggling with minority classes. This paper details the proposed RAAD dataset and provides a summary of the submissions for the RIP 2024 competition.",
      "published_utc": "2025-03-11T13:50:37Z",
      "updated_utc": "2025-03-11T13:50:37Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.08437v1",
      "abs_url": "http://arxiv.org/abs/2503.08437v1"
    },
    "2412.02447v2": {
      "arxiv_id": "2412.02447v2",
      "title": "Resonance: Learning to Predict Social-Aware Pedestrian Trajectories as Co-Vibrations",
      "authors": [
        "Conghao Wong",
        "Ziqian Zou",
        "Beihao Xia",
        "Xinge You"
      ],
      "summary": "Learning to forecast trajectories of intelligent agents has caught much more attention recently. However, it remains a challenge to accurately account for agents' intentions and social behaviors when forecasting, and in particular, to simulate the unique randomness within each of those components in an explainable and decoupled way. Inspired by vibration systems and their resonance properties, we propose the Resonance (short for Re) model to encode and forecast pedestrian trajectories in the form of ``co-vibrations''. It decomposes trajectory modifications and randomnesses into multiple vibration portions to simulate agents' reactions to each single cause, and forecasts trajectories as the superposition of these independent vibrations separately. Also, benefiting from such vibrations and their spectral properties, representations of social interactions can be learned by emulating the resonance phenomena, further enhancing its explainability. Experiments on multiple datasets have verified its usefulness both quantitatively and qualitatively.",
      "published_utc": "2024-12-03T13:31:29Z",
      "updated_utc": "2025-03-10T01:37:08Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.02447v2",
      "abs_url": "http://arxiv.org/abs/2412.02447v2"
    },
    "2411.13302v1": {
      "arxiv_id": "2411.13302v1",
      "title": "Can Reasons Help Improve Pedestrian Intent Estimation? A Cross-Modal Approach",
      "authors": [
        "Vaishnavi Khindkar",
        "Vineeth Balasubramanian",
        "Chetan Arora",
        "Anbumani Subramanian",
        "C. V. Jawahar"
      ],
      "summary": "With the increased importance of autonomous navigation systems has come an increasing need to protect the safety of Vulnerable Road Users (VRUs) such as pedestrians. Predicting pedestrian intent is one such challenging task, where prior work predicts the binary cross/no-cross intention with a fusion of visual and motion features. However, there has been no effort so far to hedge such predictions with human-understandable reasons. We address this issue by introducing a novel problem setting of exploring the intuitive reasoning behind a pedestrian's intent. In particular, we show that predicting the 'WHY' can be very useful in understanding the 'WHAT'. To this end, we propose a novel, reason-enriched PIE++ dataset consisting of multi-label textual explanations/reasons for pedestrian intent. We also introduce a novel multi-task learning framework called MINDREAD, which leverages a cross-modal representation learning framework for predicting pedestrian intent as well as the reason behind the intent. Our comprehensive experiments show significant improvement of 5.6% and 7% in accuracy and F1-score for the task of intent prediction on the PIE++ dataset using MINDREAD. We also achieved a 4.4% improvement in accuracy on a commonly used JAAD dataset. Extensive evaluation using quantitative/qualitative metrics and user studies shows the effectiveness of our approach.",
      "published_utc": "2024-11-20T13:15:04Z",
      "updated_utc": "2024-11-20T13:15:04Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.13302v1",
      "abs_url": "http://arxiv.org/abs/2411.13302v1"
    },
    "2410.13039v2": {
      "arxiv_id": "2410.13039v2",
      "title": "A low complexity contextual stacked ensemble-learning approach for pedestrian intent prediction",
      "authors": [
        "Chia-Yen Chiang",
        "Yasmin Fathy",
        "Gregory Slabaugh",
        "Mona Jaber"
      ],
      "summary": "Walking as a form of active travel is essential in promoting sustainable transport. It is thus crucial to accurately predict pedestrian crossing intention and avoid collisions, especially with the advent of autonomous and advanced driver-assisted vehicles. Current research leverages computer vision and machine learning advances to predict near-misses; however, this often requires high computation power to yield reliable results. In contrast, this work proposes a low-complexity ensemble-learning approach that employs contextual data for predicting the pedestrian's intent for crossing. The pedestrian is first detected, and their image is then compressed using skeleton-ization, and contextual information is added into a stacked ensemble-learning approach. Our experiments on different datasets achieve similar pedestrian intent prediction performance as the state-of-the-art approaches with 99.7% reduction in computational complexity. Our source code and trained models will be released upon paper acceptance",
      "published_utc": "2024-10-16T21:02:24Z",
      "updated_utc": "2025-09-01T14:32:54Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.13039v2",
      "abs_url": "http://arxiv.org/abs/2410.13039v2"
    },
    "2410.06400v1": {
      "arxiv_id": "2410.06400v1",
      "title": "Reliable Heading Tracking for Pedestrian Road Crossing Prediction Using Commodity Devices",
      "authors": [
        "Yucheng Yang",
        "Jingjie Li",
        "Kassem Fawaz"
      ],
      "summary": "Pedestrian heading tracking enables applications in pedestrian navigation, traffic safety, and accessibility. Previous works, using inertial sensor fusion or machine learning, are limited in that they assume the phone is fixed in specific orientations, hindering their generalizability. We propose a new heading tracking algorithm, the Orientation-Heading Alignment (OHA), which leverages a key insight: people tend to carry smartphones in certain ways due to habits, such as swinging them while walking. For each smartphone attitude during this motion, OHA maps the smartphone orientation to the pedestrian heading and learns such mappings efficiently from coarse headings and smartphone orientations. To anchor our algorithm in a practical scenario, we apply OHA to a challenging task: predicting when pedestrians are about to cross the road to improve road user safety. In particular, using 755 hours of walking data collected since 2020 from 60 individuals, we develop a lightweight model that operates in real-time on commodity devices to predict road crossings. Our evaluation shows that OHA achieves 3.4 times smaller heading errors across nine scenarios than existing methods. Furthermore, OHA enables the early and accurate detection of pedestrian crossing behavior, issuing crossing alerts 0.35 seconds, on average, before pedestrians enter the road range.",
      "published_utc": "2024-10-08T22:12:18Z",
      "updated_utc": "2024-10-08T22:12:18Z",
      "primary_category": "eess.SP",
      "categories": [
        "eess.SP",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.06400v1",
      "abs_url": "http://arxiv.org/abs/2410.06400v1"
    },
    "2409.20223v2": {
      "arxiv_id": "2409.20223v2",
      "title": "GTransPDM: A Graph-embedded Transformer with Positional Decoupling for Pedestrian Crossing Intention Prediction",
      "authors": [
        "Chen Xie",
        "Ciyun Lin",
        "Xiaoyu Zheng",
        "Bowen Gong",
        "Antonio M. López"
      ],
      "summary": "Understanding and predicting pedestrian crossing behavioral intention is crucial for the driving safety of autonomous vehicles. Nonetheless, challenges emerge when using promising images or environmental context masks to extract various factors for time-series network modeling, causing pre-processing errors or a loss of efficiency. Typically, pedestrian positions captured by onboard cameras are often distorted and do not accurately reflect their actual movements. To address these issues, GTransPDM -- a Graph-embedded Transformer with a Position Decoupling Module -- was developed for pedestrian crossing intention prediction by leveraging multi-modal features. First, a positional decoupling module was proposed to decompose pedestrian lateral motion and encode depth cues in the image view. Then, a graph-embedded Transformer was designed to capture the spatio-temporal dynamics of human pose skeletons, integrating essential factors such as position, skeleton, and ego-vehicle motion. Experimental results indicate that the proposed method achieves 92% accuracy on the PIE dataset and 87% accuracy on the JAAD dataset, with a processing speed of 0.05ms. It outperforms the state-of-the-art in comparison.",
      "published_utc": "2024-09-30T12:02:17Z",
      "updated_utc": "2025-05-09T18:51:10Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.20223v2",
      "abs_url": "http://arxiv.org/abs/2409.20223v2"
    },
    "2409.07645v1": {
      "arxiv_id": "2409.07645v1",
      "title": "Feature Importance in Pedestrian Intention Prediction: A Context-Aware Review",
      "authors": [
        "Mohsen Azarmi",
        "Mahdi Rezaei",
        "He Wang",
        "Ali Arabian"
      ],
      "summary": "Recent advancements in predicting pedestrian crossing intentions for Autonomous Vehicles using Computer Vision and Deep Neural Networks are promising. However, the black-box nature of DNNs poses challenges in understanding how the model works and how input features contribute to final predictions. This lack of interpretability delimits the trust in model performance and hinders informed decisions on feature selection, representation, and model optimisation; thereby affecting the efficacy of future research in the field. To address this, we introduce Context-aware Permutation Feature Importance (CAPFI), a novel approach tailored for pedestrian intention prediction. CAPFI enables more interpretability and reliable assessments of feature importance by leveraging subdivided scenario contexts, mitigating the randomness of feature values through targeted shuffling. This aims to reduce variance and prevent biased estimations in importance scores during permutations. We divide the Pedestrian Intention Estimation (PIE) dataset into 16 comparable context sets, measure the baseline performance of five distinct neural network architectures for intention prediction in each context, and assess input feature importance using CAPFI. We observed nuanced differences among models across various contextual characteristics. The research reveals the critical role of pedestrian bounding boxes and ego-vehicle speed in predicting pedestrian intentions, and potential prediction biases due to the speed feature through cross-context permutation evaluation. We propose an alternative feature representation by considering proximity change rate for rendering dynamic pedestrian-vehicle locomotion, thereby enhancing the contributions of input features to intention prediction. These findings underscore the importance of contextual features and their diversity to develop accurate and robust intent-predictive models.",
      "published_utc": "2024-09-11T22:13:01Z",
      "updated_utc": "2024-09-11T22:13:01Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.07645v1",
      "abs_url": "http://arxiv.org/abs/2409.07645v1"
    },
    "2409.06707v1": {
      "arxiv_id": "2409.06707v1",
      "title": "Gating Syn-to-Real Knowledge for Pedestrian Crossing Prediction in Safe Driving",
      "authors": [
        "Jie Bai",
        "Jianwu Fang",
        "Yisheng Lv",
        "Chen Lv",
        "Jianru Xue",
        "Zhengguo Li"
      ],
      "summary": "Pedestrian Crossing Prediction (PCP) in driving scenes plays a critical role in ensuring the safe operation of intelligent vehicles. Due to the limited observations of pedestrian crossing behaviors in typical situations, recent studies have begun to leverage synthetic data with flexible variation to boost prediction performance, employing domain adaptation frameworks. However, different domain knowledge has distinct cross-domain distribution gaps, which necessitates suitable domain knowledge adaption ways for PCP tasks. In this work, we propose a Gated Syn-to-Real Knowledge transfer approach for PCP (Gated-S2R-PCP), which has two aims: 1) designing the suitable domain adaptation ways for different kinds of crossing-domain knowledge, and 2) transferring suitable knowledge for specific situations with gated knowledge fusion. Specifically, we design a framework that contains three domain adaption methods including style transfer, distribution approximation, and knowledge distillation for various information, such as visual, semantic, depth, location, etc. A Learnable Gated Unit (LGU) is employed to fuse suitable cross-domain knowledge to boost pedestrian crossing prediction. We construct a new synthetic benchmark S2R-PCP-3181 with 3181 sequences (489,740 frames) which contains the pedestrian locations, RGB frames, semantic images, and depth images. With the synthetic S2R-PCP-3181, we transfer the knowledge to two real challenging datasets of PIE and JAAD, and superior PCP performance is obtained to the state-of-the-art methods.",
      "published_utc": "2024-08-24T07:42:26Z",
      "updated_utc": "2024-08-24T07:42:26Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.06707v1",
      "abs_url": "http://arxiv.org/abs/2409.06707v1"
    },
    "2407.18451v1": {
      "arxiv_id": "2407.18451v1",
      "title": "Gaussian Lane Keeping: A Robust Prediction Baseline",
      "authors": [
        "David Isele",
        "Piyush Gupta",
        "Xinyi Liu",
        "Sangjae Bae"
      ],
      "summary": "Predicting agents' behavior for vehicles and pedestrians is challenging due to a myriad of factors including the uncertainty attached to different intentions, inter-agent interactions, traffic (environment) rules, individual inclinations, and agent dynamics. Consequently, a plethora of neural network-driven prediction models have been introduced in the literature to encompass these intricacies to accurately predict the agent behavior. Nevertheless, many of these approaches falter when confronted with scenarios beyond their training datasets, and lack interpretability, raising concerns about their suitability for real-world applications such as autonomous driving. Moreover, these models frequently demand additional training, substantial computational resources, or specific input features necessitating extensive implementation endeavors. In response, we propose Gaussian Lane Keeping (GLK), a robust prediction method for autonomous vehicles that can provide a solid baseline for comparison when developing new algorithms and a sanity check for real-world deployment. We provide several extensions to the GLK model, evaluate it on the CitySim dataset, and show that it outperforms the neural-network based predictions.",
      "published_utc": "2024-07-26T01:14:37Z",
      "updated_utc": "2024-07-26T01:14:37Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.18451v1",
      "abs_url": "http://arxiv.org/abs/2407.18451v1"
    },
    "2407.00446v1": {
      "arxiv_id": "2407.00446v1",
      "title": "Diving Deeper Into Pedestrian Behavior Understanding: Intention Estimation, Action Prediction, and Event Risk Assessment",
      "authors": [
        "Amir Rasouli",
        "Iuliia Kotseruba"
      ],
      "summary": "In this paper, we delve into the pedestrian behavior understanding problem from the perspective of three different tasks: intention estimation, action prediction, and event risk assessment. We first define the tasks and discuss how these tasks are represented and annotated in two widely used pedestrian datasets, JAAD and PIE. We then propose a new benchmark based on these definitions, available annotations, and three new classes of metrics, each designed to assess different aspects of the model performance. We apply the new evaluation approach to examine four SOTA prediction models on each task and compare their performance w.r.t. metrics and input modalities. In particular, we analyze the differences between intention estimation and action prediction tasks by considering various scenarios and contextual factors. Lastly, we examine model agreement across these two tasks to show their complementary role. The proposed benchmark reveals new facts about the role of different data modalities, the tasks, and relevant data properties. We conclude by elaborating on our findings and proposing future research directions.",
      "published_utc": "2024-06-29T14:03:54Z",
      "updated_utc": "2024-06-29T14:03:54Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.00446v1",
      "abs_url": "http://arxiv.org/abs/2407.00446v1"
    },
    "2406.16817v1": {
      "arxiv_id": "2406.16817v1",
      "title": "GPT-4V Explorations: Mining Autonomous Driving",
      "authors": [
        "Zixuan Li"
      ],
      "summary": "This paper explores the application of the GPT-4V(ision) large visual language model to autonomous driving in mining environments, where traditional systems often falter in understanding intentions and making accurate decisions during emergencies. GPT-4V introduces capabilities for visual question answering and complex scene comprehension, addressing challenges in these specialized settings.Our evaluation focuses on its proficiency in scene understanding, reasoning, and driving functions, with specific tests on its ability to recognize and interpret elements such as pedestrians, various vehicles, and traffic devices. While GPT-4V showed robust comprehension and decision-making skills, it faced difficulties in accurately identifying specific vehicle types and managing dynamic interactions. Despite these challenges, its effective navigation and strategic decision-making demonstrate its potential as a reliable agent for autonomous driving in the complex conditions of mining environments, highlighting its adaptability and operational viability in industrial settings.",
      "published_utc": "2024-06-24T17:26:06Z",
      "updated_utc": "2024-06-24T17:26:06Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.16817v1",
      "abs_url": "http://arxiv.org/abs/2406.16817v1"
    },
    "2406.00473v1": {
      "arxiv_id": "2406.00473v1",
      "title": "Pedestrian intention prediction in Adverse Weather Conditions with Spiking Neural Networks and Dynamic Vision Sensors",
      "authors": [
        "Mustafa Sakhai",
        "Szymon Mazurek",
        "Jakub Caputa",
        "Jan K. Argasiński",
        "Maciej Wielgosz"
      ],
      "summary": "This study examines the effectiveness of Spiking Neural Networks (SNNs) paired with Dynamic Vision Sensors (DVS) to improve pedestrian detection in adverse weather, a significant challenge for autonomous vehicles. Utilizing the high temporal resolution and low latency of DVS, which excels in dynamic, low-light, and high-contrast environments, we assess the efficiency of SNNs compared to traditional Convolutional Neural Networks (CNNs). Our experiments involved testing across diverse weather scenarios using a custom dataset from the CARLA simulator, mirroring real-world variability. SNN models, enhanced with Temporally Effective Batch Normalization, were trained and benchmarked against state-of-the-art CNNs to demonstrate superior accuracy and computational efficiency in complex conditions such as rain and fog. The results indicate that SNNs, integrated with DVS, significantly reduce computational overhead and improve detection accuracy in challenging conditions compared to CNNs. This highlights the potential of DVS combined with bio-inspired SNN processing to enhance autonomous vehicle perception and decision-making systems, advancing intelligent transportation systems' safety features in varying operational environments. Additionally, our research indicates that SNNs perform more efficiently in handling long perception windows and prediction tasks, rather than simple pedestrian detection.",
      "published_utc": "2024-06-01T15:58:24Z",
      "updated_utc": "2024-06-01T15:58:24Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.00473v1",
      "abs_url": "http://arxiv.org/abs/2406.00473v1"
    },
    "2405.19202v5": {
      "arxiv_id": "2405.19202v5",
      "title": "Vulnerable Road User Detection and Safety Enhancement: A Comprehensive Survey",
      "authors": [
        "Renato M. Silva",
        "Gregorio F. Azevedo",
        "Matheus V. V. Berto",
        "Jean R. Rocha",
        "Eduardo C. Fidelis",
        "Matheus V. Nogueira",
        "Pedro H. Lisboa",
        "Tiago A. Almeida"
      ],
      "summary": "Traffic incidents involving vulnerable road users (VRUs) constitute a significant proportion of global road accidents. Advances in traffic communication ecosystems, coupled with sophisticated signal processing and machine learning techniques, have facilitated the utilization of data from diverse sensors. Despite these advancements and the availability of extensive datasets, substantial progress is required to mitigate traffic casualties. This paper provides a comprehensive survey of state-of-the-art technologies and methodologies to enhance the safety of VRUs. The study investigates the communication networks between vehicles and VRUs, emphasizing the integration of advanced sensors and the availability of relevant datasets. It explores preprocessing techniques and data fusion methods to enhance sensor data quality. Furthermore, our study assesses critical simulation environments essential for developing and testing VRU safety systems. Our research also highlights recent advances in VRU detection and classification algorithms, addressing challenges such as variable environmental conditions. Additionally, we cover cutting-edge research in predicting VRU intentions and behaviors, which is mandatory for proactive collision avoidance strategies. Through this survey, we aim to provide a comprehensive understanding of the current landscape of VRU safety technologies, identifying areas of progress and areas needing further research and development.",
      "published_utc": "2024-05-29T15:42:10Z",
      "updated_utc": "2025-06-27T21:39:35Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.19202v5",
      "abs_url": "http://arxiv.org/abs/2405.19202v5"
    },
    "2405.13969v1": {
      "arxiv_id": "2405.13969v1",
      "title": "Uncertainty-Aware DRL for Autonomous Vehicle Crowd Navigation in Shared Space",
      "authors": [
        "Mahsa Golchoubian",
        "Moojan Ghafurian",
        "Kerstin Dautenhahn",
        "Nasser Lashgarian Azad"
      ],
      "summary": "Safe, socially compliant, and efficient navigation of low-speed autonomous vehicles (AVs) in pedestrian-rich environments necessitates considering pedestrians' future positions and interactions with the vehicle and others. Despite the inevitable uncertainties associated with pedestrians' predicted trajectories due to their unobserved states (e.g., intent), existing deep reinforcement learning (DRL) algorithms for crowd navigation often neglect these uncertainties when using predicted trajectories to guide policy learning. This omission limits the usability of predictions when diverging from ground truth. This work introduces an integrated prediction and planning approach that incorporates the uncertainties of predicted pedestrian states in the training of a model-free DRL algorithm. A novel reward function encourages the AV to respect pedestrians' personal space, decrease speed during close approaches, and minimize the collision probability with their predicted paths. Unlike previous DRL methods, our model, designed for AV operation in crowded spaces, is trained in a novel simulation environment that reflects realistic pedestrian behaviour in a shared space with vehicles. Results show a 40% decrease in collision rate and a 15% increase in minimum distance to pedestrians compared to the state of the art model that does not account for prediction uncertainty. Additionally, the approach outperforms model predictive control methods that incorporate the same prediction uncertainties in terms of both performance and computational time, while producing trajectories closer to human drivers in similar scenarios.",
      "published_utc": "2024-05-22T20:09:21Z",
      "updated_utc": "2024-05-22T20:09:21Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.13969v1",
      "abs_url": "http://arxiv.org/abs/2405.13969v1"
    },
    "2404.17031v2": {
      "arxiv_id": "2404.17031v2",
      "title": "Motor Focus: Fast Ego-Motion Prediction for Assistive Visual Navigation",
      "authors": [
        "Hao Wang",
        "Jiayou Qin",
        "Xiwen Chen",
        "Ashish Bastola",
        "John Suchanek",
        "Zihao Gong",
        "Abolfazl Razi"
      ],
      "summary": "Assistive visual navigation systems for visually impaired individuals have become increasingly popular thanks to the rise of mobile computing. Most of these devices work by translating visual information into voice commands. In complex scenarios where multiple objects are present, it is imperative to prioritize object detection and provide immediate notifications for key entities in specific directions. This brings the need for identifying the observer's motion direction (ego-motion) by merely processing visual information, which is the key contribution of this paper. Specifically, we introduce Motor Focus, a lightweight image-based framework that predicts the ego-motion - the humans (and humanoid machines) movement intentions based on their visual feeds, while filtering out camera motion without any camera calibration. To this end, we implement an optical flow-based pixel-wise temporal analysis method to compensate for the camera motion with a Gaussian aggregation to smooth out the movement prediction area. Subsequently, to evaluate the performance, we collect a dataset including 50 clips of pedestrian scenes in 5 different scenarios. We tested this framework with classical feature detectors such as SIFT and ORB to show the comparison. Our framework demonstrates its superiority in speed (> 40FPS), accuracy (MAE = 60pixels), and robustness (SNR = 23dB), confirming its potential to enhance the usability of vision-based assistive navigation tools in complex environments.",
      "published_utc": "2024-04-25T20:45:39Z",
      "updated_utc": "2024-10-12T21:08:05Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.17031v2",
      "abs_url": "http://arxiv.org/abs/2404.17031v2"
    },
    "2404.09574v1": {
      "arxiv_id": "2404.09574v1",
      "title": "Predicting and Analyzing Pedestrian Crossing Behavior at Unsignalized Crossings",
      "authors": [
        "Chi Zhang",
        "Janis Sprenger",
        "Zhongjun Ni",
        "Christian Berger"
      ],
      "summary": "Understanding and predicting pedestrian crossing behavior is essential for enhancing automated driving and improving driving safety. Predicting gap selection behavior and the use of zebra crossing enables driving systems to proactively respond and prevent potential conflicts. This task is particularly challenging at unsignalized crossings due to the ambiguous right of way, requiring pedestrians to constantly interact with vehicles and other pedestrians. This study addresses these challenges by utilizing simulator data to investigate scenarios involving multiple vehicles and pedestrians. We propose and evaluate machine learning models to predict gap selection in non-zebra scenarios and zebra crossing usage in zebra scenarios. We investigate and discuss how pedestrians' behaviors are influenced by various factors, including pedestrian waiting time, walking speed, the number of unused gaps, the largest missed gap, and the influence of other pedestrians. This research contributes to the evolution of intelligent vehicles by providing predictive models and valuable insights into pedestrian crossing behavior.",
      "published_utc": "2024-04-15T08:36:40Z",
      "updated_utc": "2024-04-15T08:36:40Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.09574v1",
      "abs_url": "http://arxiv.org/abs/2404.09574v1"
    },
    "2403.06774v1": {
      "arxiv_id": "2403.06774v1",
      "title": "From Agent Autonomy to Casual Collaboration: A Design Investigation on Help-Seeking Urban Robots",
      "authors": [
        "Xinyan Yu",
        "Marius Hoggenmueller",
        "Martin Tomitsch"
      ],
      "summary": "As intelligent agents transition from controlled to uncontrolled environments, they face challenges that sometimes exceed their operational capabilities. In many scenarios, they rely on assistance from bystanders to overcome those challenges. Using robots that get stuck in urban settings as an example, we investigate how agents can prompt bystanders into providing assistance. We conducted four focus group sessions with 17 participants that involved bodystorming, where participants assumed the role of robots and bystander pedestrians in role-playing activities. Generating insights from both assumed robot and bystander perspectives, we were able to identify potential non-verbal help-seeking strategies (i.e., addressing bystanders, cueing intentions, and displaying emotions) and factors shaping the assistive behaviours of bystanders. Drawing on these findings, we offer design considerations for help-seeking urban robots and other agents operating in uncontrolled environments to foster casual collaboration, encompass expressiveness, align with agent social categories, and curate appropriate incentives.",
      "published_utc": "2024-03-04T06:42:13Z",
      "updated_utc": "2024-03-04T06:42:13Z",
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.06774v1",
      "abs_url": "http://arxiv.org/abs/2403.06774v1"
    },
    "2402.19002v2": {
      "arxiv_id": "2402.19002v2",
      "title": "GoalNet: Goal Areas Oriented Pedestrian Trajectory Prediction",
      "authors": [
        "Amar Fadillah",
        "Ching-Lin Lee",
        "Zhi-Xuan Wang",
        "Kuan-Ting Lai"
      ],
      "summary": "Predicting the future trajectories of pedestrians on the road is an important task for autonomous driving. The pedestrian trajectory prediction is affected by scene paths, pedestrian's intentions and decision-making, which is a multi-modal problem. Most recent studies use past trajectories to predict a variety of potential future trajectory distributions, which do not account for the scene context and pedestrian targets. Instead of predicting the future trajectory directly, we propose to use scene context and observed trajectory to predict the goal points first, and then reuse the goal points to predict the future trajectories. By leveraging the information from scene context and observed trajectory, the uncertainty can be limited to a few target areas, which represent the \"goals\" of the pedestrians. In this paper, we propose GoalNet, a new trajectory prediction neural network based on the goal areas of a pedestrian. Our network can predict both pedestrian's trajectories and bounding boxes. The overall model is efficient and modular, and its outputs can be changed according to the usage scenario. Experimental results show that GoalNet significantly improves the previous state-of-the-art performance by 48.7% on the JAAD and 40.8% on the PIE dataset.",
      "published_utc": "2024-02-29T09:53:19Z",
      "updated_utc": "2025-07-11T16:39:37Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.19002v2",
      "abs_url": "http://arxiv.org/abs/2402.19002v2"
    },
    "2402.12810v3": {
      "arxiv_id": "2402.12810v3",
      "title": "PIP-Net: Pedestrian Intention Prediction in the Wild",
      "authors": [
        "Mohsen Azarmi",
        "Mahdi Rezaei",
        "He Wang"
      ],
      "summary": "Accurate pedestrian intention prediction (PIP) by Autonomous Vehicles (AVs) is one of the current research challenges in this field. In this article, we introduce PIP-Net, a novel framework designed to predict pedestrian crossing intentions by AVs in real-world urban scenarios. We offer two variants of PIP-Net designed for different camera mounts and setups. Leveraging both kinematic data and spatial features from the driving scene, the proposed model employs a recurrent and temporal attention-based solution, outperforming state-of-the-art performance. To enhance the visual representation of road users and their proximity to the ego vehicle, we introduce a categorical depth feature map, combined with a local motion flow feature, providing rich insights into the scene dynamics. Additionally, we explore the impact of expanding the camera's field of view, from one to three cameras surrounding the ego vehicle, leading to an enhancement in the model's contextual perception. Depending on the traffic scenario and road environment, the model excels in predicting pedestrian crossing intentions up to 4 seconds in advance, which is a breakthrough in current research studies in pedestrian intention prediction. Finally, for the first time, we present the Urban-PIP dataset, a customised pedestrian intention prediction dataset, with multi-camera annotations in real-world automated driving scenarios.",
      "published_utc": "2024-02-20T08:28:45Z",
      "updated_utc": "2025-07-06T19:28:52Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.NE",
        "eess.IV",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.12810v3",
      "abs_url": "http://arxiv.org/abs/2402.12810v3"
    },
    "2512.18211v1": {
      "arxiv_id": "2512.18211v1",
      "title": "LLaViDA: A Large Language Vision Driving Assistant for Explicit Reasoning and Enhanced Trajectory Planning",
      "authors": [
        "Yudong Liu",
        "Spencer Hallyburton",
        "Jiwoo Kim",
        "Yueqian Lin",
        "Yiming Li",
        "Qinsi Wang",
        "Hui Ye",
        "Jingwei Sun",
        "Miroslav Pajic",
        "Yiran Chen",
        "Hai Li"
      ],
      "summary": "Trajectory planning is a fundamental yet challenging component of autonomous driving. End-to-end planners frequently falter under adverse weather, unpredictable human behavior, or complex road layouts, primarily because they lack strong generalization or few-shot capabilities beyond their training data. We propose LLaViDA, a Large Language Vision Driving Assistant that leverages a Vision-Language Model (VLM) for object motion prediction, semantic grounding, and chain-of-thought reasoning for trajectory planning in autonomous driving. A two-stage training pipeline--supervised fine-tuning followed by Trajectory Preference Optimization (TPO)--enhances scene understanding and trajectory planning by injecting regression-based supervision, produces a powerful \"VLM Trajectory Planner for Autonomous Driving.\" On the NuScenes benchmark, LLaViDA surpasses state-of-the-art end-to-end and other recent VLM/LLM-based baselines in open-loop trajectory planning task, achieving an average L2 trajectory error of 0.31 m and a collision rate of 0.10% on the NuScenes test set. The code for this paper is available at GitHub.",
      "published_utc": "2025-12-20T04:38:35Z",
      "updated_utc": "2025-12-20T04:38:35Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2512.18211v1",
      "abs_url": "http://arxiv.org/abs/2512.18211v1"
    },
    "2512.16784v1": {
      "arxiv_id": "2512.16784v1",
      "title": "R3ST: A Synthetic 3D Dataset With Realistic Trajectories",
      "authors": [
        "Simone Teglia",
        "Claudia Melis Tonti",
        "Francesco Pro",
        "Leonardo Russo",
        "Andrea Alfarano",
        "Leonardo Pentassuglia",
        "Irene Amerini"
      ],
      "summary": "Datasets are essential to train and evaluate computer vision models used for traffic analysis and to enhance road safety. Existing real datasets fit real-world scenarios, capturing authentic road object behaviors, however, they typically lack precise ground-truth annotations. In contrast, synthetic datasets play a crucial role, allowing for the annotation of a large number of frames without additional costs or extra time. However, a general drawback of synthetic datasets is the lack of realistic vehicle motion, since trajectories are generated using AI models or rule-based systems. In this work, we introduce R3ST (Realistic 3D Synthetic Trajectories), a synthetic dataset that overcomes this limitation by generating a synthetic 3D environment and integrating real-world trajectories derived from SinD, a bird's-eye-view dataset recorded from drone footage. The proposed dataset closes the gap between synthetic data and realistic trajectories, advancing the research in trajectory forecasting of road vehicles, offering both accurate multimodal ground-truth annotations and authentic human-driven vehicle trajectories.",
      "published_utc": "2025-12-18T17:18:45Z",
      "updated_utc": "2025-12-18T17:18:45Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2512.16784v1",
      "abs_url": "http://arxiv.org/abs/2512.16784v1"
    },
    "2512.10056v1": {
      "arxiv_id": "2512.10056v1",
      "title": "Mitigating Exposure Bias in Risk-Aware Time Series Forecasting with Soft Tokens",
      "authors": [
        "Alireza Namazi",
        "Amirreza Dolatpour Fathkouhi",
        "Heman Shakeri"
      ],
      "summary": "Autoregressive forecasting is central to predictive control in diabetes and hemodynamic management, where different operating zones carry different clinical risks. Standard models trained with teacher forcing suffer from exposure bias, yielding unstable multi-step forecasts for closed-loop use. We introduce Soft-Token Trajectory Forecasting (SoTra), which propagates continuous probability distributions (``soft tokens'') to mitigate exposure bias and learn calibrated, uncertainty-aware trajectories. A risk-aware decoding module then minimizes expected clinical harm. In glucose forecasting, SoTra reduces average zone-based risk by 18\\%; in blood-pressure forecasting, it lowers effective clinical risk by approximately 15\\%. These improvements support its use in safety-critical predictive control.",
      "published_utc": "2025-12-10T20:25:45Z",
      "updated_utc": "2025-12-10T20:25:45Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2512.10056v1",
      "abs_url": "http://arxiv.org/abs/2512.10056v1"
    },
    "2512.06190v1": {
      "arxiv_id": "2512.06190v1",
      "title": "Multi-Modal Zero-Shot Prediction of Color Trajectories in Food Drying",
      "authors": [
        "Shichen Li",
        "Ahmadreza Eslaminia",
        "Chenhui Shao"
      ],
      "summary": "Food drying is widely used to reduce moisture content, ensure safety, and extend shelf life. Color evolution of food samples is an important indicator of product quality in food drying. Although existing studies have examined color changes under different drying conditions, current approaches primarily rely on low-dimensional color features and cannot fully capture the complex, dynamic color trajectories of food samples. Moreover, existing modeling approaches lack the ability to generalize to unseen process conditions. To address these limitations, we develop a novel multi-modal color-trajectory prediction method that integrates high-dimensional temporal color information with drying process parameters to enable accurate and data-efficient color trajectory prediction. Under unseen drying conditions, the model attains RMSEs of 2.12 for cookie drying and 1.29 for apple drying, reducing errors by over 90% compared with baseline models. These experimental results demonstrate the model's superior accuracy, robustness, and broad applicability.",
      "published_utc": "2025-12-05T22:17:25Z",
      "updated_utc": "2025-12-05T22:17:25Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2512.06190v1",
      "abs_url": "http://arxiv.org/abs/2512.06190v1"
    },
    "2512.05682v1": {
      "arxiv_id": "2512.05682v1",
      "title": "Scenario-aware Uncertainty Quantification for Trajectory Prediction with Statistical Guarantees",
      "authors": [
        "Yiming Shu",
        "Jiahui Xu",
        "Linghuan Kong",
        "Fangni Zhang",
        "Guodong Yin",
        "Chen Sun"
      ],
      "summary": "Reliable uncertainty quantification in trajectory prediction is crucial for safety-critical autonomous driving systems, yet existing deep learning predictors lack uncertainty-aware frameworks adaptable to heterogeneous real-world scenarios. To bridge this gap, we propose a novel scenario-aware uncertainty quantification framework to provide the predicted trajectories with prediction intervals and reliability assessment. To begin with, predicted trajectories from the trained predictor and their ground truth are projected onto the map-derived reference routes within the Frenet coordinate system. We then employ CopulaCPTS as the conformal calibration method to generate temporal prediction intervals for distinct scenarios as the uncertainty measure. Building upon this, within the proposed trajectory reliability discriminator (TRD), mean error and calibrated confidence intervals are synergistically analyzed to establish reliability models for different scenarios. Subsequently, the risk-aware discriminator leverages a joint risk model that integrates longitudinal and lateral prediction intervals within the Frenet coordinate to identify critical points. This enables segmentation of trajectories into reliable and unreliable segments, holding the advantage of informing downstream planning modules with actionable reliability results. We evaluated our framework using the real-world nuPlan dataset, demonstrating its effectiveness in scenario-aware uncertainty quantification and reliability assessment across diverse driving contexts.",
      "published_utc": "2025-12-05T12:54:39Z",
      "updated_utc": "2025-12-05T12:54:39Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2512.05682v1",
      "abs_url": "http://arxiv.org/abs/2512.05682v1"
    },
    "2511.18170v1": {
      "arxiv_id": "2511.18170v1",
      "title": "Time-aware Motion Planning in Dynamic Environments with Conformal Prediction",
      "authors": [
        "Kaier Liang",
        "Licheng Luo",
        "Yixuan Wang",
        "Mingyu Cai",
        "Cristian Ioan Vasile"
      ],
      "summary": "Safe navigation in dynamic environments remains challenging due to uncertain obstacle behaviors and the lack of formal prediction guarantees. We propose two motion planning frameworks that leverage conformal prediction (CP): a global planner that integrates Safe Interval Path Planning (SIPP) for uncertainty-aware trajectory generation, and a local planner that performs online reactive planning. The global planner offers distribution-free safety guarantees for long-horizon navigation, while the local planner mitigates inaccuracies in obstacle trajectory predictions through adaptive CP, enabling robust and responsive motion in dynamic environments. To further enhance trajectory feasibility, we introduce an adaptive quantile mechanism in the CP-based uncertainty quantification. Instead of using a fixed confidence level, the quantile is automatically tuned to the optimal value that preserves trajectory feasibility, allowing the planner to adaptively tighten safety margins in regions with higher uncertainty. We validate the proposed framework through numerical experiments conducted in dynamic and cluttered environments. The project page is available at https://time-aware-planning.github.io",
      "published_utc": "2025-11-22T19:51:10Z",
      "updated_utc": "2025-11-22T19:51:10Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2511.18170v1",
      "abs_url": "http://arxiv.org/abs/2511.18170v1"
    },
    "2511.00126v1": {
      "arxiv_id": "2511.00126v1",
      "title": "Dynamic Model Selection for Trajectory Prediction via Pairwise Ranking and Meta-Features",
      "authors": [
        "Lu Bowen"
      ],
      "summary": "Recent deep trajectory predictors (e.g., Jiang et al., 2023; Zhou et al., 2022) have achieved strong average accuracy but remain unreliable in complex long-tail driving scenarios. These limitations reveal the weakness of the prevailing \"one-model-fits-all\" paradigm, particularly in safety-critical urban contexts where simpler physics-based models can occasionally outperform advanced networks (Kalman, 1960). To bridge this gap, we propose a dynamic multi-expert gating framework that adaptively selects the most reliable trajectory predictor among a physics-informed LSTM, a Transformer, and a fine-tuned GameFormer on a per-sample basis. Our method leverages internal model signals (meta-features) such as stability and uncertainty (Gal and Ghahramani, 2016), which we demonstrate to be substantially more informative than geometric scene descriptors. To the best of our knowledge, this is the first work to formulate trajectory expert selection as a pairwise-ranking problem over internal model signals (Burges et al., 2005), directly optimizing decision quality without requiring post-hoc calibration. Evaluated on the nuPlan-mini dataset (Caesar et al., 2021) with 1,287 samples, our LLM-enhanced tri-expert gate achieves a Final Displacement Error (FDE) of 2.567 m, representing a 9.5 percent reduction over GameFormer (2.835 m), and realizes 57.8 percent of the oracle performance bound. In open-loop simulations, after trajectory horizon alignment, the same configuration reduces FDE on left-turn scenarios by approximately 10 percent, demonstrating consistent improvements across both offline validation and open-loop evaluation. These results indicate that adaptive hybrid systems enhance trajectory reliability in safety-critical autonomous driving, providing a practical pathway beyond static single-model paradigms.",
      "published_utc": "2025-10-31T10:01:01Z",
      "updated_utc": "2025-10-31T10:01:01Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2511.00126v1",
      "abs_url": "http://arxiv.org/abs/2511.00126v1"
    },
    "2510.22789v1": {
      "arxiv_id": "2510.22789v1",
      "title": "Learning Neural Observer-Predictor Models for Limb-level Sampling-based Locomotion Planning",
      "authors": [
        "Abhijeet M. Kulkarni",
        "Ioannis Poulakakis",
        "Guoquan Huang"
      ],
      "summary": "Accurate full-body motion prediction is essential for the safe, autonomous navigation of legged robots, enabling critical capabilities like limb-level collision checking in cluttered environments. Simplified kinematic models often fail to capture the complex, closed-loop dynamics of the robot and its low-level controller, limiting their predictions to simple planar motion. To address this, we present a learning-based observer-predictor framework that accurately predicts this motion. Our method features a neural observer with provable UUB guarantees that provides a reliable latent state estimate from a history of proprioceptive measurements. This stable estimate initializes a computationally efficient predictor, designed for the rapid, parallel evaluation of thousands of potential trajectories required by modern sampling-based planners. We validated the system by integrating our neural predictor into an MPPI-based planner on a Vision 60 quadruped. Hardware experiments successfully demonstrated effective, limb-aware motion planning in a challenging, narrow passage and over small objects, highlighting our system's ability to provide a robust foundation for high-performance, collision-aware planning on dynamic robotic platforms.",
      "published_utc": "2025-10-26T18:46:46Z",
      "updated_utc": "2025-10-26T18:46:46Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2510.22789v1",
      "abs_url": "http://arxiv.org/abs/2510.22789v1"
    },
    "2510.17191v2": {
      "arxiv_id": "2510.17191v2",
      "title": "SimpleVSF: VLM-Scoring Fusion for Trajectory Prediction of End-to-End Autonomous Driving",
      "authors": [
        "Peiru Zheng",
        "Yun Zhao",
        "Zhan Gong",
        "Hong Zhu",
        "Shaohua Wu"
      ],
      "summary": "End-to-end autonomous driving has emerged as a promising paradigm for achieving robust and intelligent driving policies. However, existing end-to-end methods still face significant challenges, such as suboptimal decision-making in complex scenarios. In this paper,we propose SimpleVSF (Simple VLM-Scoring Fusion), a novel framework that enhances end-to-end planning by leveraging the cognitive capabilities of Vision-Language Models (VLMs) and advanced trajectory fusion techniques. We utilize the conventional scorers and the novel VLM-enhanced scorers. And we leverage a robust weight fusioner for quantitative aggregation and a powerful VLM-based fusioner for qualitative, context-aware decision-making. As the leading approach in the ICCV 2025 NAVSIM v2 End-to-End Driving Challenge, our SimpleVSF framework demonstrates state-of-the-art performance, achieving a superior balance between safety, comfort, and efficiency.",
      "published_utc": "2025-10-20T06:09:57Z",
      "updated_utc": "2025-10-28T00:58:56Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2510.17191v2",
      "abs_url": "http://arxiv.org/abs/2510.17191v2"
    },
    "2510.13461v1": {
      "arxiv_id": "2510.13461v1",
      "title": "Physics-Informed Neural Network Modeling of Vehicle Collision Dynamics in Precision Immobilization Technique Maneuvers",
      "authors": [
        "Yangye Jiang",
        "Jiachen Wang",
        "Daofei Li"
      ],
      "summary": "Accurate prediction of vehicle collision dynamics is crucial for advanced safety systems and post-impact control applications, yet existing methods face inherent trade-offs among computational efficiency, prediction accuracy, and data requirements. This paper proposes a dual Physics-Informed Neural Network framework addressing these challenges through two complementary networks. The first network integrates Gaussian Mixture Models with PINN architecture to learn impact force distributions from finite element analysis data while enforcing momentum conservation and energy consistency constraints. The second network employs an adaptive PINN with dynamic constraint weighting to predict post-collision vehicle dynamics, featuring an adaptive physics guard layer that prevents unrealistic predictions whil e preserving data-driven learning capabilities. The framework incorporates uncertainty quantification through time-varying parameters and enables rapid adaptation via fine-tuning strategies. Validation demonstrates significant improvements: the impact force model achieves relative errors below 15.0% for force prediction on finite element analysis (FEA) datasets, while the vehicle dynamics model reduces average trajectory prediction error by 63.6% compared to traditional four-degree-of-freedom models in scaled vehicle experiments. The integrated system maintains millisecond-level computational efficiency suitable for real-time applications while providing probabilistic confidence bounds essential for safety-critical control. Comprehensive validation through FEA simulation, dynamic modeling, and scaled vehicle experiments confirms the framework's effectiveness for Precision Immobilization Technique scenarios and general collision dynamics prediction.",
      "published_utc": "2025-10-15T12:08:55Z",
      "updated_utc": "2025-10-15T12:08:55Z",
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2510.13461v1",
      "abs_url": "http://arxiv.org/abs/2510.13461v1"
    },
    "2509.07464v1": {
      "arxiv_id": "2509.07464v1",
      "title": "Safe and Non-Conservative Contingency Planning for Autonomous Vehicles via Online Learning-Based Reachable Set Barriers",
      "authors": [
        "Rui Yang",
        "Lei Zheng",
        "Shuzhi Sam Ge",
        "Jun Ma"
      ],
      "summary": "Autonomous vehicles must navigate dynamically uncertain environments while balancing the safety and driving efficiency. This challenge is exacerbated by the unpredictable nature of surrounding human-driven vehicles (HVs) and perception inaccuracies, which require planners to adapt to evolving uncertainties while maintaining safe trajectories. Overly conservative planners degrade driving efficiency, while deterministic approaches may encounter serious issues and risks of failure when faced with sudden and unexpected maneuvers. To address these issues, we propose a real-time contingency trajectory optimization framework in this paper. By employing event-triggered online learning of HV control-intent sets, our method dynamically quantifies multi-modal HV uncertainties and refines the forward reachable set (FRS) incrementally. Crucially, we enforce invariant safety through FRS-based barrier constraints that ensure safety without reliance on accurate trajectory prediction of HVs. These constraints are embedded in contingency trajectory optimization and solved efficiently through consensus alternative direction method of multipliers (ADMM). The system continuously adapts to the uncertainties in HV behaviors, preserving feasibility and safety without resorting to excessive conservatism. High-fidelity simulations on highway and urban scenarios, as well as a series of real-world experiments demonstrate significant improvements in driving efficiency and passenger comfort while maintaining safety under uncertainty. The project page is available at https://pathetiue.github.io/frscp.github.io/.",
      "published_utc": "2025-09-09T07:43:10Z",
      "updated_utc": "2025-09-09T07:43:10Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2509.07464v1",
      "abs_url": "http://arxiv.org/abs/2509.07464v1"
    },
    "2509.01294v1": {
      "arxiv_id": "2509.01294v1",
      "title": "Metamorphic Testing of Multimodal Human Trajectory Prediction",
      "authors": [
        "Helge Spieker",
        "Nadjib Lazaar",
        "Arnaud Gotlieb",
        "Nassim Belmecheri"
      ],
      "summary": "Context: Predicting human trajectories is crucial for the safety and reliability of autonomous systems, such as automated vehicles and mobile robots. However, rigorously testing the underlying multimodal Human Trajectory Prediction (HTP) models, which typically use multiple input sources (e.g., trajectory history and environment maps) and produce stochastic outputs (multiple possible future paths), presents significant challenges. The primary difficulty lies in the absence of a definitive test oracle, as numerous future trajectories might be plausible for any given scenario. Objectives: This research presents the application of Metamorphic Testing (MT) as a systematic methodology for testing multimodal HTP systems. We address the oracle problem through metamorphic relations (MRs) adapted for the complexities and stochastic nature of HTP. Methods: We present five MRs, targeting transformations of both historical trajectory data and semantic segmentation maps used as an environmental context. These MRs encompass: 1) label-preserving geometric transformations (mirroring, rotation, rescaling) applied to both trajectory and map inputs, where outputs are expected to transform correspondingly. 2) Map-altering transformations (changing semantic class labels, introducing obstacles) with predictable changes in trajectory distributions. We propose probabilistic violation criteria based on distance metrics between probability distributions, such as the Wasserstein or Hellinger distance. Conclusion: This study introduces tool, a MT framework for the oracle-less testing of multimodal, stochastic HTP systems. It allows for assessment of model robustness against input transformations and contextual changes without reliance on ground-truth trajectories.",
      "published_utc": "2025-09-01T09:30:35Z",
      "updated_utc": "2025-09-01T09:30:35Z",
      "primary_category": "cs.SE",
      "categories": [
        "cs.SE",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2509.01294v1",
      "abs_url": "http://arxiv.org/abs/2509.01294v1"
    },
    "2508.21559v1": {
      "arxiv_id": "2508.21559v1",
      "title": "Limitations of Physics-Informed Neural Networks: a Study on Smart Grid Surrogation",
      "authors": [
        "Julen Cestero",
        "Carmine Delle Femine",
        "Kenji S. Muro",
        "Marco Quartulli",
        "Marcello Restelli"
      ],
      "summary": "Physics-Informed Neural Networks (PINNs) present a transformative approach for smart grid modeling by integrating physical laws directly into learning frameworks, addressing critical challenges of data scarcity and physical consistency in conventional data-driven methods. This paper evaluates PINNs' capabilities as surrogate models for smart grid dynamics, comparing their performance against XGBoost, Random Forest, and Linear Regression across three key experiments: interpolation, cross-validation, and episodic trajectory prediction. By training PINNs exclusively through physics-based loss functions (enforcing power balance, operational constraints, and grid stability) we demonstrate their superior generalization, outperforming data-driven models in error reduction. Notably, PINNs maintain comparatively lower MAE in dynamic grid operations, reliably capturing state transitions in both random and expert-driven control scenarios, while traditional models exhibit erratic performance. Despite slight degradation in extreme operational regimes, PINNs consistently enforce physical feasibility, proving vital for safety-critical applications. Our results contribute to establishing PINNs as a paradigm-shifting tool for smart grid surrogation, bridging data-driven flexibility with first-principles rigor. This work advances real-time grid control and scalable digital twins, emphasizing the necessity of physics-aware architectures in mission-critical energy systems.",
      "published_utc": "2025-08-29T12:15:32Z",
      "updated_utc": "2025-08-29T12:15:32Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2508.21559v1",
      "abs_url": "http://arxiv.org/abs/2508.21559v1"
    },
    "2508.14198v1": {
      "arxiv_id": "2508.14198v1",
      "title": "Reliability comparison of vessel trajectory prediction models via Probability of Detection",
      "authors": [
        "Zahra Rastin",
        "Kathrin Donandt",
        "Dirk Söffker"
      ],
      "summary": "This contribution addresses vessel trajectory prediction (VTP), focusing on the evaluation of different deep learning-based approaches. The objective is to assess model performance in diverse traffic complexities and compare the reliability of the approaches. While previous VTP models overlook the specific traffic situation complexity and lack reliability assessments, this research uses a probability of detection analysis to quantify model reliability in varying traffic scenarios, thus going beyond common error distribution analyses. All models are evaluated on test samples categorized according to their traffic situation during the prediction horizon, with performance metrics and reliability estimates obtained for each category. The results of this comprehensive evaluation provide a deeper understanding of the strengths and weaknesses of the different prediction approaches, along with their reliability in terms of the prediction horizon lengths for which safe forecasts can be guaranteed. These findings can inform the development of more reliable vessel trajectory prediction approaches, enhancing safety and efficiency in future inland waterways navigation.",
      "published_utc": "2025-08-19T18:43:56Z",
      "updated_utc": "2025-08-19T18:43:56Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CE",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2508.14198v1",
      "abs_url": "http://arxiv.org/abs/2508.14198v1"
    },
    "2508.12456v1": {
      "arxiv_id": "2508.12456v1",
      "title": "Autonomous Oil Spill Response Through Liquid Neural Trajectory Modeling and Coordinated Marine Robotics",
      "authors": [
        "Hadas C. Kuzmenko",
        "David Ehevich",
        "Oren Gal"
      ],
      "summary": "Marine oil spills pose grave environmental and economic risks, threatening marine ecosystems, coastlines, and dependent industries. Predicting and managing oil spill trajectories is highly complex, due to the interplay of physical, chemical, and environmental factors such as wind, currents, and temperature, which makes timely and effective response challenging. Accurate real-time trajectory forecasting and coordinated mitigation are vital for minimizing the impact of these disasters. This study introduces an integrated framework combining a multi-agent swarm robotics system built on the MOOS-IvP platform with Liquid Time-Constant Neural Networks (LTCNs). The proposed system fuses adaptive machine learning with autonomous marine robotics, enabling real-time prediction, dynamic tracking, and rapid response to evolving oil spills. By leveraging LTCNs--well-suited for modeling complex, time-dependent processes--the framework achieves real-time, high-accuracy forecasts of spill movement. Swarm intelligence enables decentralized, scalable, and resilient decision-making among robot agents, enhancing collective monitoring and containment efforts. Our approach was validated using data from the Deepwater Horizon spill, where the LTC-RK4 model achieved 0.96 spatial accuracy, surpassing LSTM approaches by 23%. The integration of advanced neural modeling with autonomous, coordinated robotics demonstrates substantial improvements in prediction precision, flexibility, and operational scalability. Ultimately, this research advances the state-of-the-art for sustainable, autonomous oil spill management and environmental protection by enhancing both trajectory prediction and response coordination.",
      "published_utc": "2025-08-17T18:03:23Z",
      "updated_utc": "2025-08-17T18:03:23Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2508.12456v1",
      "abs_url": "http://arxiv.org/abs/2508.12456v1"
    },
    "2508.10567v1": {
      "arxiv_id": "2508.10567v1",
      "title": "SpaRC-AD: A Baseline for Radar-Camera Fusion in End-to-End Autonomous Driving",
      "authors": [
        "Philipp Wolters",
        "Johannes Gilg",
        "Torben Teepe",
        "Gerhard Rigoll"
      ],
      "summary": "End-to-end autonomous driving systems promise stronger performance through unified optimization of perception, motion forecasting, and planning. However, vision-based approaches face fundamental limitations in adverse weather conditions, partial occlusions, and precise velocity estimation - critical challenges in safety-sensitive scenarios where accurate motion understanding and long-horizon trajectory prediction are essential for collision avoidance. To address these limitations, we propose SpaRC-AD, a query-based end-to-end camera-radar fusion framework for planning-oriented autonomous driving. Through sparse 3D feature alignment, and doppler-based velocity estimation, we achieve strong 3D scene representations for refinement of agent anchors, map polylines and motion modelling. Our method achieves strong improvements over the state-of-the-art vision-only baselines across multiple autonomous driving tasks, including 3D detection (+4.8% mAP), multi-object tracking (+8.3% AMOTA), online mapping (+1.8% mAP), motion prediction (-4.0% mADE), and trajectory planning (-0.1m L2 and -9% TPC). We achieve both spatial coherence and temporal consistency on multiple challenging benchmarks, including real-world open-loop nuScenes, long-horizon T-nuScenes, and closed-loop simulator Bench2Drive. We show the effectiveness of radar-based fusion in safety-critical scenarios where accurate motion understanding and long-horizon trajectory prediction are essential for collision avoidance. The source code of all experiments is available at https://phi-wol.github.io/sparcad/",
      "published_utc": "2025-08-14T12:02:41Z",
      "updated_utc": "2025-08-14T12:02:41Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2508.10567v1",
      "abs_url": "http://arxiv.org/abs/2508.10567v1"
    },
    "2508.10427v2": {
      "arxiv_id": "2508.10427v2",
      "title": "STRIDE-QA: Visual Question Answering Dataset for Spatiotemporal Reasoning in Urban Driving Scenes",
      "authors": [
        "Keishi Ishihara",
        "Kento Sasaki",
        "Tsubasa Takahashi",
        "Daiki Shiono",
        "Yu Yamaguchi"
      ],
      "summary": "Vision-Language Models (VLMs) have been applied to autonomous driving to support decision-making in complex real-world scenarios. However, their training on static, web-sourced image-text pairs fundamentally limits the precise spatiotemporal reasoning required to understand and predict dynamic traffic scenes. We address this critical gap with STRIDE-QA, a large-scale visual question answering (VQA) dataset for physically grounded reasoning from an ego-centric perspective. Constructed from 100 hours of multi-sensor driving data in Tokyo, capturing diverse and challenging conditions, STRIDE-QA is the largest VQA dataset for spatiotemporal reasoning in urban driving, offering 16 million QA pairs over 285K frames. Grounded by dense, automatically generated annotations including 3D bounding boxes, segmentation masks, and multi-object tracks, the dataset uniquely supports both object-centric and ego-centric reasoning through three novel QA tasks that require spatial localization and temporal prediction. Our benchmarks demonstrate that existing VLMs struggle significantly, achieving near-zero scores on prediction consistency. In contrast, VLMs fine-tuned on STRIDE-QA exhibit dramatic performance gains, achieving 55% success in spatial localization and 28% consistency in future motion prediction, compared to near-zero scores from general-purpose VLMs. Therefore, STRIDE-QA establishes a comprehensive foundation for developing more reliable VLMs for safety-critical autonomous systems.",
      "published_utc": "2025-08-14T07:57:06Z",
      "updated_utc": "2025-10-14T06:54:59Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2508.10427v2",
      "abs_url": "http://arxiv.org/abs/2508.10427v2"
    },
    "2508.07668v1": {
      "arxiv_id": "2508.07668v1",
      "title": "AIS-LLM: A Unified Framework for Maritime Trajectory Prediction, Anomaly Detection, and Collision Risk Assessment with Explainable Forecasting",
      "authors": [
        "Hyobin Park",
        "Jinwook Jung",
        "Minseok Seo",
        "Hyunsoo Choi",
        "Deukjae Cho",
        "Sekil Park",
        "Dong-Geol Choi"
      ],
      "summary": "With the increase in maritime traffic and the mandatory implementation of the Automatic Identification System (AIS), the importance and diversity of maritime traffic analysis tasks based on AIS data, such as vessel trajectory prediction, anomaly detection, and collision risk assessment, is rapidly growing. However, existing approaches tend to address these tasks individually, making it difficult to holistically consider complex maritime situations. To address this limitation, we propose a novel framework, AIS-LLM, which integrates time-series AIS data with a large language model (LLM). AIS-LLM consists of a Time-Series Encoder for processing AIS sequences, an LLM-based Prompt Encoder, a Cross-Modality Alignment Module for semantic alignment between time-series data and textual prompts, and an LLM-based Multi-Task Decoder. This architecture enables the simultaneous execution of three key tasks: trajectory prediction, anomaly detection, and risk assessment of vessel collisions within a single end-to-end system. Experimental results demonstrate that AIS-LLM outperforms existing methods across individual tasks, validating its effectiveness. Furthermore, by integratively analyzing task outputs to generate situation summaries and briefings, AIS-LLM presents the potential for more intelligent and efficient maritime traffic management.",
      "published_utc": "2025-08-11T06:39:45Z",
      "updated_utc": "2025-08-11T06:39:45Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2508.07668v1",
      "abs_url": "http://arxiv.org/abs/2508.07668v1"
    },
    "2508.05616v1": {
      "arxiv_id": "2508.05616v1",
      "title": "TrajEvo: Trajectory Prediction Heuristics Design via LLM-driven Evolution",
      "authors": [
        "Zhikai Zhao",
        "Chuanbo Hua",
        "Federico Berto",
        "Kanghoon Lee",
        "Zihan Ma",
        "Jiachen Li",
        "Jinkyoo Park"
      ],
      "summary": "Trajectory prediction is a critical task in modeling human behavior, especially in safety-critical domains such as social robotics and autonomous vehicle navigation. Traditional heuristics based on handcrafted rules often lack accuracy and generalizability. Although deep learning approaches offer improved performance, they typically suffer from high computational cost, limited explainability, and, importantly, poor generalization to out-of-distribution (OOD) scenarios. In this paper, we introduce TrajEvo, a framework that leverages Large Language Models (LLMs) to automatically design trajectory prediction heuristics. TrajEvo employs an evolutionary algorithm to generate and refine prediction heuristics from past trajectory data. We propose two key innovations: Cross-Generation Elite Sampling to encourage population diversity, and a Statistics Feedback Loop that enables the LLM to analyze and improve alternative predictions. Our evaluations demonstrate that TrajEvo outperforms existing heuristic methods across multiple real-world datasets, and notably surpasses both heuristic and deep learning methods in generalizing to an unseen OOD real-world dataset. TrajEvo marks a promising step toward the automated design of fast, explainable, and generalizable trajectory prediction heuristics. We release our source code to facilitate future research at https://github.com/ai4co/trajevo.",
      "published_utc": "2025-08-07T17:55:10Z",
      "updated_utc": "2025-08-07T17:55:10Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2508.05616v1",
      "abs_url": "http://arxiv.org/abs/2508.05616v1"
    },
    "2508.04564v1": {
      "arxiv_id": "2508.04564v1",
      "title": "Drone Detection with Event Cameras",
      "authors": [
        "Gabriele Magrini",
        "Lorenzo Berlincioni",
        "Luca Cultrera",
        "Federico Becattini",
        "Pietro Pala"
      ],
      "summary": "The diffusion of drones presents significant security and safety challenges. Traditional surveillance systems, particularly conventional frame-based cameras, struggle to reliably detect these targets due to their small size, high agility, and the resulting motion blur and poor performance in challenging lighting conditions. This paper surveys the emerging field of event-based vision as a robust solution to these problems. Event cameras virtually eliminate motion blur and enable consistent detection in extreme lighting. Their sparse, asynchronous output suppresses static backgrounds, enabling low-latency focus on motion cues. We review the state-of-the-art in event-based drone detection, from data representation methods to advanced processing pipelines using spiking neural networks. The discussion extends beyond simple detection to cover more sophisticated tasks such as real-time tracking, trajectory forecasting, and unique identification through propeller signature analysis. By examining current methodologies, available datasets, and the distinct advantages of the technology, this work demonstrates that event-based vision provides a powerful foundation for the next generation of reliable, low-latency, and efficient counter-UAV systems.",
      "published_utc": "2025-08-06T15:49:33Z",
      "updated_utc": "2025-08-06T15:49:33Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2508.04564v1",
      "abs_url": "http://arxiv.org/abs/2508.04564v1"
    },
    "2508.06544v1": {
      "arxiv_id": "2508.06544v1",
      "title": "Historical Prediction Attention Mechanism based Trajectory Forecasting for Proactive Work Zone Safety in a Digital Twin Environment",
      "authors": [
        "Minhaj Uddin Ahmad",
        "Mizanur Rahman",
        "Alican Sevim",
        "David Bodoh",
        "Sakib Khan",
        "Li Zhao",
        "Nathan Huynh",
        "Eren Erman Ozguven"
      ],
      "summary": "Proactive safety systems aim to mitigate risks by anticipating potential conflicts between vehicles and enabling early intervention to prevent work zone-related crashes. This study presents an infrastructure-enabled proactive work zone safety warning system that leverages a Digital Twin environment, integrating real-time multi-sensor data, detailed High-Definition (HD) maps, and a historical prediction attention mechanism-based trajectory prediction model. Using a co-simulation environment that combines Simulation of Urban MObility (SUMO) and CAR Learning to Act (CARLA) simulators, along with Lanelet2 HD maps and the Historical Prediction Network (HPNet) model, we demonstrate effective trajectory prediction and early warning generation for vehicle interactions in freeway work zones. To evaluate the accuracy of predicted trajectories, we use two standard metrics: Joint Average Displacement Error (ADE) and Joint Final Displacement Error (FDE). Specifically, the infrastructure-enabled HPNet model demonstrates superior performance on the work-zone datasets generated from the co-simulation environment, achieving a minimum Joint FDE of 0.3228 meters and a minimum Joint ADE of 0.1327 meters, lower than the benchmarks on the Argoverse (minJointFDE: 1.0986 m, minJointADE: 0.7612 m) and Interaction (minJointFDE: 0.8231 m, minJointADE: 0.2548 m) datasets. In addition, our proactive safety warning generation application, utilizing vehicle bounding boxes and probabilistic conflict modeling, demonstrates its capability to issue alerts for potential vehicle conflicts.",
      "published_utc": "2025-08-05T16:52:31Z",
      "updated_utc": "2025-08-05T16:52:31Z",
      "primary_category": "cs.OH",
      "categories": [
        "cs.OH",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2508.06544v1",
      "abs_url": "http://arxiv.org/abs/2508.06544v1"
    },
    "2507.15266v1": {
      "arxiv_id": "2507.15266v1",
      "title": "VLM-UDMC: VLM-Enhanced Unified Decision-Making and Motion Control for Urban Autonomous Driving",
      "authors": [
        "Haichao Liu",
        "Haoren Guo",
        "Pei Liu",
        "Benshan Ma",
        "Yuxiang Zhang",
        "Jun Ma",
        "Tong Heng Lee"
      ],
      "summary": "Scene understanding and risk-aware attentions are crucial for human drivers to make safe and effective driving decisions. To imitate this cognitive ability in urban autonomous driving while ensuring the transparency and interpretability, we propose a vision-language model (VLM)-enhanced unified decision-making and motion control framework, named VLM-UDMC. This framework incorporates scene reasoning and risk-aware insights into an upper-level slow system, which dynamically reconfigures the optimal motion planning for the downstream fast system. The reconfiguration is based on real-time environmental changes, which are encoded through context-aware potential functions. More specifically, the upper-level slow system employs a two-step reasoning policy with Retrieval-Augmented Generation (RAG), leveraging foundation models to process multimodal inputs and retrieve contextual knowledge, thereby generating risk-aware insights. Meanwhile, a lightweight multi-kernel decomposed LSTM provides real-time trajectory predictions for heterogeneous traffic participants by extracting smoother trend representations for short-horizon trajectory prediction. The effectiveness of the proposed VLM-UDMC framework is verified via both simulations and real-world experiments with a full-size autonomous vehicle. It is demonstrated that the presented VLM-UDMC effectively leverages scene understanding and attention decomposition for rational driving decisions, thus improving the overall urban driving performance. Our open-source project is available at https://github.com/henryhcliu/vlmudmc.git.",
      "published_utc": "2025-07-21T06:06:27Z",
      "updated_utc": "2025-07-21T06:06:27Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2507.15266v1",
      "abs_url": "http://arxiv.org/abs/2507.15266v1"
    },
    "2507.13073v1": {
      "arxiv_id": "2507.13073v1",
      "title": "Dual LiDAR-Based Traffic Movement Count Estimation at a Signalized Intersection: Deployment, Data Collection, and Preliminary Analysis",
      "authors": [
        "Saswat Priyadarshi Nayak",
        "Guoyuan Wu",
        "Kanok Boriboonsomsin",
        "Matthew Barth"
      ],
      "summary": "Traffic Movement Count (TMC) at intersections is crucial for optimizing signal timings, assessing the performance of existing traffic control measures, and proposing efficient lane configurations to minimize delays, reduce congestion, and promote safety. Traditionally, methods such as manual counting, loop detectors, pneumatic road tubes, and camera-based recognition have been used for TMC estimation. Although generally reliable, camera-based TMC estimation is prone to inaccuracies under poor lighting conditions during harsh weather and nighttime. In contrast, Light Detection and Ranging (LiDAR) technology is gaining popularity in recent times due to reduced costs and its expanding use in 3D object detection, tracking, and related applications. This paper presents the authors' endeavor to develop, deploy and evaluate a dual-LiDAR system at an intersection in the city of Rialto, California, for TMC estimation. The 3D bounding box detections from the two LiDARs are used to classify vehicle counts based on traffic directions, vehicle movements, and vehicle classes. This work discusses the estimated TMC results and provides insights into the observed trends and irregularities. Potential improvements are also discussed that could enhance not only TMC estimation, but also trajectory forecasting and intent prediction at intersections.",
      "published_utc": "2025-07-17T12:42:28Z",
      "updated_utc": "2025-07-17T12:42:28Z",
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2507.13073v1",
      "abs_url": "http://arxiv.org/abs/2507.13073v1"
    },
    "2507.12463v1": {
      "arxiv_id": "2507.12463v1",
      "title": "MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior Understanding",
      "authors": [
        "Renjie Li",
        "Ruijie Ye",
        "Mingyang Wu",
        "Hao Frank Yang",
        "Zhiwen Fan",
        "Hezhen Hu",
        "Zhengzhong Tu"
      ],
      "summary": "Humans are integral components of the transportation ecosystem, and understanding their behaviors is crucial to facilitating the development of safe driving systems. Although recent progress has explored various aspects of human behavior$\\unicode{x2014}$such as motion, trajectories, and intention$\\unicode{x2014}$a comprehensive benchmark for evaluating human behavior understanding in autonomous driving remains unavailable. In this work, we propose $\\textbf{MMHU}$, a large-scale benchmark for human behavior analysis featuring rich annotations, such as human motion and trajectories, text description for human motions, human intention, and critical behavior labels relevant to driving safety. Our dataset encompasses 57k human motion clips and 1.73M frames gathered from diverse sources, including established driving datasets such as Waymo, in-the-wild videos from YouTube, and self-collected data. A human-in-the-loop annotation pipeline is developed to generate rich behavior captions. We provide a thorough dataset analysis and benchmark multiple tasks$\\unicode{x2014}$ranging from motion prediction to motion generation and human behavior question answering$\\unicode{x2014}$thereby offering a broad evaluation suite. Project page : https://MMHU-Benchmark.github.io.",
      "published_utc": "2025-07-16T17:59:30Z",
      "updated_utc": "2025-07-16T17:59:30Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2507.12463v1",
      "abs_url": "http://arxiv.org/abs/2507.12463v1"
    },
    "2507.12083v1": {
      "arxiv_id": "2507.12083v1",
      "title": "Foresight in Motion: Reinforcing Trajectory Prediction with Reward Heuristics",
      "authors": [
        "Muleilan Pei",
        "Shaoshuai Shi",
        "Xuesong Chen",
        "Xu Liu",
        "Shaojie Shen"
      ],
      "summary": "Motion forecasting for on-road traffic agents presents both a significant challenge and a critical necessity for ensuring safety in autonomous driving systems. In contrast to most existing data-driven approaches that directly predict future trajectories, we rethink this task from a planning perspective, advocating a \"First Reasoning, Then Forecasting\" strategy that explicitly incorporates behavior intentions as spatial guidance for trajectory prediction. To achieve this, we introduce an interpretable, reward-driven intention reasoner grounded in a novel query-centric Inverse Reinforcement Learning (IRL) scheme. Our method first encodes traffic agents and scene elements into a unified vectorized representation, then aggregates contextual features through a query-centric paradigm. This enables the derivation of a reward distribution, a compact yet informative representation of the target agent's behavior within the given scene context via IRL. Guided by this reward heuristic, we perform policy rollouts to reason about multiple plausible intentions, providing valuable priors for subsequent trajectory generation. Finally, we develop a hierarchical DETR-like decoder integrated with bidirectional selective state space models to produce accurate future trajectories along with their associated probabilities. Extensive experiments on the large-scale Argoverse and nuScenes motion forecasting datasets demonstrate that our approach significantly enhances trajectory prediction confidence, achieving highly competitive performance relative to state-of-the-art methods.",
      "published_utc": "2025-07-16T09:46:17Z",
      "updated_utc": "2025-07-16T09:46:17Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2507.12083v1",
      "abs_url": "http://arxiv.org/abs/2507.12083v1"
    },
    "2507.08563v2": {
      "arxiv_id": "2507.08563v2",
      "title": "STRAP: Spatial-Temporal Risk-Attentive Vehicle Trajectory Prediction for Autonomous Driving",
      "authors": [
        "Xinyi Ning",
        "Zilin Bian",
        "Dachuan Zuo",
        "Semiha Ergan"
      ],
      "summary": "Accurate vehicle trajectory prediction is essential for ensuring safety and efficiency in fully autonomous driving systems. While existing methods primarily focus on modeling observed motion patterns and interactions with other vehicles, they often neglect the potential risks posed by the uncertain or aggressive behaviors of surrounding vehicles. In this paper, we propose a novel spatial-temporal risk-attentive trajectory prediction framework that incorporates a risk potential field to assess perceived risks arising from behaviors of nearby vehicles. The framework leverages a spatial-temporal encoder and a risk-attentive feature fusion decoder to embed the risk potential field into the extracted spatial-temporal feature representations for trajectory prediction. A risk-scaled loss function is further designed to improve the prediction accuracy of high-risk scenarios, such as short relative spacing. Experiments on the widely used NGSIM and HighD datasets demonstrate that our method reduces average prediction errors by 4.8% and 31.2% respectively compared to state-of-the-art approaches, especially in high-risk scenarios. The proposed framework provides interpretable, risk-aware predictions, contributing to more robust decision-making for autonomous driving systems.",
      "published_utc": "2025-07-11T13:05:35Z",
      "updated_utc": "2025-07-14T08:04:31Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2507.08563v2",
      "abs_url": "http://arxiv.org/abs/2507.08563v2"
    },
    "2507.03365v1": {
      "arxiv_id": "2507.03365v1",
      "title": "Label-Free Long-Horizon 3D UAV Trajectory Prediction via Motion-Aligned RGB and Event Cues",
      "authors": [
        "Hanfang Liang",
        "Shenghai Yuan",
        "Fen Liu",
        "Yizhuo Yang",
        "Bing Wang",
        "Zhuyu Huang",
        "Chenyang Shi",
        "Jing Jin"
      ],
      "summary": "The widespread use of consumer drones has introduced serious challenges for airspace security and public safety. Their high agility and unpredictable motion make drones difficult to track and intercept. While existing methods focus on detecting current positions, many counter-drone strategies rely on forecasting future trajectories and thus require more than reactive detection to be effective. To address this critical gap, we propose an unsupervised vision-based method for predicting the three-dimensional trajectories of drones. Our approach first uses an unsupervised technique to extract drone trajectories from raw LiDAR point clouds, then aligns these trajectories with camera images through motion consistency to generate reliable pseudo-labels. We then combine kinematic estimation with a visual Mamba neural network in a self-supervised manner to predict future drone trajectories. We evaluate our method on the challenging MMAUD dataset, including the V2 sequences that feature wide-field-of-view multimodal sensors and dynamic UAV motion in urban scenes. Extensive experiments show that our framework outperforms supervised image-only and audio-visual baselines in long-horizon trajectory prediction, reducing 5-second 3D error by around 40 percent without using any manual 3D labels. The proposed system offers a cost-effective, scalable alternative for real-time counter-drone deployment. All code will be released upon acceptance to support reproducible research in the robotics community.",
      "published_utc": "2025-07-04T07:58:04Z",
      "updated_utc": "2025-07-04T07:58:04Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2507.03365v1",
      "abs_url": "http://arxiv.org/abs/2507.03365v1"
    },
    "2506.23164v1": {
      "arxiv_id": "2506.23164v1",
      "title": "Mode Collapse Happens: Evaluating Critical Interactions in Joint Trajectory Prediction Models",
      "authors": [
        "Maarten Hugenholtz",
        "Anna Meszaros",
        "Jens Kober",
        "Zlatan Ajanovic"
      ],
      "summary": "Autonomous Vehicle decisions rely on multimodal prediction models that account for multiple route options and the inherent uncertainty in human behavior. However, models can suffer from mode collapse, where only the most likely mode is predicted, posing significant safety risks. While existing methods employ various strategies to generate diverse predictions, they often overlook the diversity in interaction modes among agents. Additionally, traditional metrics for evaluating prediction models are dataset-dependent and do not evaluate inter-agent interactions quantitatively. To our knowledge, none of the existing metrics explicitly evaluates mode collapse. In this paper, we propose a novel evaluation framework that assesses mode collapse in joint trajectory predictions, focusing on safety-critical interactions. We introduce metrics for mode collapse, mode correctness, and coverage, emphasizing the sequential dimension of predictions. By testing four multi-agent trajectory prediction models, we demonstrate that mode collapse indeed happens. When looking at the sequential dimension, although prediction accuracy improves closer to interaction events, there are still cases where the models are unable to predict the correct interaction mode, even just before the interaction mode becomes inevitable. We hope that our framework can help researchers gain new insights and advance the development of more consistent and accurate prediction models, thus enhancing the safety of autonomous driving systems.",
      "published_utc": "2025-06-29T09:53:12Z",
      "updated_utc": "2025-06-29T09:53:12Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.23164v1",
      "abs_url": "http://arxiv.org/abs/2506.23164v1"
    },
    "2506.19341v1": {
      "arxiv_id": "2506.19341v1",
      "title": "Trajectory Prediction in Dynamic Object Tracking: A Critical Study",
      "authors": [
        "Zhongping Dong",
        "Liming Chen",
        "Mohand Tahar Kechadi"
      ],
      "summary": "This study provides a detailed analysis of current advancements in dynamic object tracking (DOT) and trajectory prediction (TP) methodologies, including their applications and challenges. It covers various approaches, such as feature-based, segmentation-based, estimation-based, and learning-based methods, evaluating their effectiveness, deployment, and limitations in real-world scenarios. The study highlights the significant impact of these technologies in automotive and autonomous vehicles, surveillance and security, healthcare, and industrial automation, contributing to safety and efficiency. Despite the progress, challenges such as improved generalization, computational efficiency, reduced data dependency, and ethical considerations still exist. The study suggests future research directions to address these challenges, emphasizing the importance of multimodal data integration, semantic information fusion, and developing context-aware systems, along with ethical and privacy-preserving frameworks.",
      "published_utc": "2025-06-24T06:10:01Z",
      "updated_utc": "2025-06-24T06:10:01Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.19341v1",
      "abs_url": "http://arxiv.org/abs/2506.19341v1"
    },
    "2506.11419v1": {
      "arxiv_id": "2506.11419v1",
      "title": "FocalAD: Local Motion Planning for End-to-End Autonomous Driving",
      "authors": [
        "Bin Sun",
        "Boao Zhang",
        "Jiayi Lu",
        "Xinjie Feng",
        "Jiachen Shang",
        "Rui Cao",
        "Mengchao Zheng",
        "Chuanye Wang",
        "Shichun Yang",
        "Yaoguang Cao",
        "Ziying Song"
      ],
      "summary": "In end-to-end autonomous driving,the motion prediction plays a pivotal role in ego-vehicle planning. However, existing methods often rely on globally aggregated motion features, ignoring the fact that planning decisions are primarily influenced by a small number of locally interacting agents. Failing to attend to these critical local interactions can obscure potential risks and undermine planning reliability. In this work, we propose FocalAD, a novel end-to-end autonomous driving framework that focuses on critical local neighbors and refines planning by enhancing local motion representations. Specifically, FocalAD comprises two core modules: the Ego-Local-Agents Interactor (ELAI) and the Focal-Local-Agents Loss (FLA Loss). ELAI conducts a graph-based ego-centric interaction representation that captures motion dynamics with local neighbors to enhance both ego planning and agent motion queries. FLA Loss increases the weights of decision-critical neighboring agents, guiding the model to prioritize those more relevant to planning. Extensive experiments show that FocalAD outperforms existing state-of-the-art methods on the open-loop nuScenes datasets and closed-loop Bench2Drive benchmark. Notably, on the robustness-focused Adv-nuScenes dataset, FocalAD achieves even greater improvements, reducing the average colilision rate by 41.9% compared to DiffusionDrive and by 15.6% compared to SparseDrive.",
      "published_utc": "2025-06-13T02:39:01Z",
      "updated_utc": "2025-06-13T02:39:01Z",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.11419v1",
      "abs_url": "http://arxiv.org/abs/2506.11419v1"
    },
    "2601.12119v1": {
      "arxiv_id": "2601.12119v1",
      "title": "CARLA-Round: A Multi-Factor Simulation Dataset for Roundabout Trajectory Prediction",
      "authors": [
        "Xiaotong Zhou",
        "Zhenhui Yuan",
        "Yi Han",
        "Tianhua Xu",
        "Laurence T. Yang"
      ],
      "summary": "Accurate trajectory prediction of vehicles at roundabouts is critical for reducing traffic accidents, yet it remains highly challenging due to their circular road geometry, continuous merging and yielding interactions, and absence of traffic signals. Developing accurate prediction algorithms relies on reliable, multimodal, and realistic datasets; however, such datasets for roundabout scenarios are scarce, as real-world data collection is often limited by incomplete observations and entangled factors that are difficult to isolate. We present CARLA-Round, a systematically designed simulation dataset for roundabout trajectory prediction. The dataset varies weather conditions (five types) and traffic density levels (spanning Level-of-Service A-E) in a structured manner, resulting in 25 controlled scenarios. Each scenario incorporates realistic mixtures of driving behaviors and provides explicit annotations that are largely absent from existing datasets. Unlike randomly sampled simulation data, this structured design enables precise analysis of how different conditions influence trajectory prediction performance. Validation experiments using standard baselines (LSTM, GCN, GRU+GCN) reveal traffic density dominates prediction difficulty with strong monotonic effects, while weather shows non-linear impacts. The best model achieves 0.312m ADE on real-world rounD dataset, demonstrating effective sim-to-real transfer. This systematic approach quantifies factor impacts impossible to isolate in confounded real-world datasets. Our CARLA-Round dataset is available at https://github.com/Rebecca689/CARLA-Round.",
      "published_utc": "2026-01-17T17:43:34Z",
      "updated_utc": "2026-01-17T17:43:34Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2601.12119v1",
      "abs_url": "http://arxiv.org/abs/2601.12119v1"
    },
    "2601.10521v2": {
      "arxiv_id": "2601.10521v2",
      "title": "BikeActions: An Open Platform and Benchmark for Cyclist-Centric VRU Action Recognition",
      "authors": [
        "Max A. Buettner",
        "Kanak Mazumder",
        "Luca Koecher",
        "Mario Finkbeiner",
        "Sebastian Niebler",
        "Fabian B. Flohr"
      ],
      "summary": "Anticipating the intentions of Vulnerable Road Users (VRUs) is a critical challenge for safe autonomous driving (AD) and mobile robotics. While current research predominantly focuses on pedestrian crossing behaviors from a vehicle's perspective, interactions within dense shared spaces remain underexplored. To bridge this gap, we introduce FUSE-Bike, the first fully open perception platform of its kind. Equipped with two LiDARs, a camera, and GNSS, it facilitates high-fidelity, close-range data capture directly from a cyclist's viewpoint. Leveraging this platform, we present BikeActions, a novel multi-modal dataset comprising 852 annotated samples across 5 distinct action classes, specifically tailored to improve VRU behavior modeling. We establish a rigorous benchmark by evaluating state-of-the-art graph convolution and transformer-based models on our publicly released data splits, establishing the first performance baselines for this challenging task. We release the full dataset together with data curation tools, the open hardware design, and the benchmark code to foster future research in VRU action understanding under https://iv.ee.hm.edu/bikeactions/.",
      "published_utc": "2026-01-15T15:47:46Z",
      "updated_utc": "2026-01-20T12:44:36Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2601.10521v2",
      "abs_url": "http://arxiv.org/abs/2601.10521v2"
    },
    "2508.10427v3": {
      "arxiv_id": "2508.10427v3",
      "title": "STRIDE-QA: Visual Question Answering Dataset for Spatiotemporal Reasoning in Urban Driving Scenes",
      "authors": [
        "Keishi Ishihara",
        "Kento Sasaki",
        "Tsubasa Takahashi",
        "Daiki Shiono",
        "Yu Yamaguchi"
      ],
      "summary": "Vision-Language Models (VLMs) have been applied to autonomous driving to support decision-making in complex real-world scenarios. However, their training on static, web-sourced image-text pairs fundamentally limits the precise spatiotemporal reasoning required to understand and predict dynamic traffic scenes. We address this critical gap with STRIDE-QA, a large-scale visual question answering (VQA) dataset for physically grounded reasoning from an ego-centric perspective. Constructed from 100 hours of multi-sensor driving data in Tokyo, capturing diverse and challenging conditions, STRIDE-QA is the largest VQA dataset for spatiotemporal reasoning in urban driving, offering 16M QA pairs over 270K frames. Grounded by dense, automatically generated annotations including 3D bounding boxes, segmentation masks, and multi-object tracks, the dataset uniquely supports both object-centric and ego-centric reasoning through three novel QA tasks that require spatial localization and temporal prediction. Our benchmarks demonstrate that existing VLMs struggle significantly, with near-zero scores on prediction consistency. In contrast, VLMs fine-tuned on STRIDE-QA exhibit dramatic performance gains, achieving 55% success in spatial localization and 28% consistency in future motion prediction, compared to near-zero scores from general-purpose VLMs. Therefore, STRIDE-QA establishes a comprehensive foundation for developing more reliable VLMs for safety-critical autonomous systems.",
      "published_utc": "2025-08-14T07:57:06Z",
      "updated_utc": "2026-01-19T04:44:59Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2508.10427v3",
      "abs_url": "http://arxiv.org/abs/2508.10427v3"
    },
    "2512.02368v2": {
      "arxiv_id": "2512.02368v2",
      "title": "MoE-Enhanced Multi-Domain Feature Selection and Fusion for Fast Map-Free Trajectory Prediction",
      "authors": [
        "Wenyi Xiong",
        "Jian Chen",
        "Ziheng Qi",
        "Wenhua Chen"
      ],
      "summary": "Trajectory prediction is crucial for the reliability and safety of autonomous driving systems, yet it remains a challenging task in complex interactive scenarios due to noisy trajectory observations and intricate agent interactions. Existing methods often struggle to filter redundant scene data for discriminative information extraction, directly impairing trajectory prediction accuracy especially when handling outliers and dynamic multi-agent interactions. In response to these limitations, we present a novel map-free trajectory prediction method which adaptively eliminates redundant information and selects discriminative features across the temporal, spatial, and frequency domains, thereby enabling precise trajectory prediction in real-world driving environments. First, we design a MoE based frequency domain filter to adaptively weight distinct frequency components of observed trajectory data and suppress outlier related noise; then a selective spatiotemporal attention module that reallocates weights across temporal nodes (sequential dependencies), temporal trends (evolution patterns), and spatial nodes to extract salient information is proposed. Finally, our multimodal decoder-supervised by joint patch level and point-level losses generates reasonable and temporally consistent trajectories, and comprehensive experiments on the large-scale NuScenes and Argoverse dataset demonstrate that our method achieves competitive performance and low-latency inference performance compared with recently proposed methods.",
      "published_utc": "2025-12-02T03:20:07Z",
      "updated_utc": "2026-01-23T04:59:15Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2512.02368v2",
      "abs_url": "http://arxiv.org/abs/2512.02368v2"
    },
    "2512.10350v4": {
      "arxiv_id": "2512.10350v4",
      "title": "Geometric Dynamics of Agentic Loops in Large Language Models",
      "authors": [
        "Nicolas Tacheny"
      ],
      "summary": "Iterative LLM systems(self-refinement, chain-of-thought, autonomous agents) are increasingly deployed, yet their temporal dynamics remain uncharacterized. Prior work evaluates task performance at convergence but ignores the trajectory: how does semantic content evolve across iterations? Does it stabilize, drift, or oscillate? Without answering these questions, we cannot predict system behavior, guarantee stability, or systematically design iterative architectures. We formalize agentic loops as discrete dynamical systems in semantic space. Borrowing from dynamical systems theory, we define trajectories, attractors and dynamical regimes for recursive LLM transformations, providing rigorous geometric definitions adapted to this setting. Our framework reveals that agentic loops exhibit classifiable dynamics: contractive (convergence toward stable semantic attractors), oscillatory (cycling among attractors), or exploratory (unbounded divergence). Experiments on singular loops validate the framework. Iterative paraphrasing produces contractive dynamics with measurable attractor formation and decreasing dispersion. Iterative negation produces exploratory dynamics with no stable structure. Crucially, prompt design directly controls the dynamical regime - the same model exhibits fundamentally different geometric behaviors depending solely on the transformation applied. This work establishes that iterative LLM dynamics are predictable and controllable, opening new directions for stability analysis, trajectory forecasting, and principled design of composite loops that balance convergence and exploration.",
      "published_utc": "2025-12-11T07:06:14Z",
      "updated_utc": "2026-01-27T14:01:32Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2512.10350v4",
      "abs_url": "http://arxiv.org/abs/2512.10350v4"
    },
    "2601.20720v1": {
      "arxiv_id": "2601.20720v1",
      "title": "Li-ViP3D++: Query-Gated Deformable Camera-LiDAR Fusion for End-to-End Perception and Trajectory Prediction",
      "authors": [
        "Matej Halinkovic",
        "Nina Masarykova",
        "Alexey Vinel",
        "Marek Galinski"
      ],
      "summary": "End-to-end perception and trajectory prediction from raw sensor data is one of the key capabilities for autonomous driving. Modular pipelines restrict information flow and can amplify upstream errors. Recent query-based, fully differentiable perception-and-prediction (PnP) models mitigate these issues, yet the complementarity of cameras and LiDAR in the query-space has not been sufficiently explored. Models often rely on fusion schemes that introduce heuristic alignment and discrete selection steps which prevent full utilization of available information and can introduce unwanted bias. We propose Li-ViP3D++, a query-based multimodal PnP framework that introduces Query-Gated Deformable Fusion (QGDF) to integrate multi-view RGB and LiDAR in query space. QGDF (i) aggregates image evidence via masked attention across cameras and feature levels, (ii) extracts LiDAR context through fully differentiable BEV sampling with learned per-query offsets, and (iii) applies query-conditioned gating to adaptively weight visual and geometric cues per agent. The resulting architecture jointly optimizes detection, tracking, and multi-hypothesis trajectory forecasting in a single end-to-end model. On nuScenes, Li-ViP3D++ improves end-to-end behavior and detection quality, achieving higher EPA (0.335) and mAP (0.502) while substantially reducing false positives (FP ratio 0.147), and it is faster than the prior Li-ViP3D variant (139.82 ms vs. 145.91 ms). These results indicate that query-space, fully differentiable camera-LiDAR fusion can increase robustness of end-to-end PnP without sacrificing deployability.",
      "published_utc": "2026-01-28T15:53:32Z",
      "updated_utc": "2026-01-28T15:53:32Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2601.20720v1",
      "abs_url": "http://arxiv.org/abs/2601.20720v1"
    },
    "2601.20367v1": {
      "arxiv_id": "2601.20367v1",
      "title": "Unsupervised Anomaly Detection in Multi-Agent Trajectory Prediction via Transformer-Based Models",
      "authors": [
        "Qing Lyu",
        "Zhe Fu",
        "Alexandre Bayen"
      ],
      "summary": "Identifying safety-critical scenarios is essential for autonomous driving, but the rarity of such events makes supervised labeling impractical. Traditional rule-based metrics like Time-to-Collision are too simplistic to capture complex interaction risks, and existing methods lack a systematic way to verify whether statistical anomalies truly reflect physical danger. To address this gap, we propose an unsupervised anomaly detection framework based on a multi-agent Transformer that models normal driving and measures deviations through prediction residuals. A dual evaluation scheme has been proposed to assess both detection stability and physical alignment: Stability is measured using standard ranking metrics in which Kendall Rank Correlation Coefficient captures rank agreement and Jaccard index captures the consistency of the top-K selected items; Physical alignment is assessed through correlations with established Surrogate Safety Measures (SSM). Experiments on the NGSIM dataset demonstrate our framework's effectiveness: We show that the maximum residual aggregator achieves the highest physical alignment while maintaining stability. Furthermore, our framework identifies 388 unique anomalies missed by Time-to-Collision and statistical baselines, capturing subtle multi-agent risks like reactive braking under lateral drift. The detected anomalies are further clustered into four interpretable risk types, offering actionable insights for simulation and testing.",
      "published_utc": "2026-01-28T08:33:10Z",
      "updated_utc": "2026-01-28T08:33:10Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2601.20367v1",
      "abs_url": "http://arxiv.org/abs/2601.20367v1"
    },
    "2511.17045v3": {
      "arxiv_id": "2511.17045v3",
      "title": "RacketVision: A Multiple Racket Sports Benchmark for Unified Ball and Racket Analysis",
      "authors": [
        "Linfeng Dong",
        "Yuchen Yang",
        "Hao Wu",
        "Wei Wang",
        "Yuenan Hou",
        "Zhihang Zhong",
        "Xiao Sun"
      ],
      "summary": "We introduce RacketVision, a novel dataset and benchmark for advancing computer vision in sports analytics, covering table tennis, tennis, and badminton. The dataset is the first to provide large-scale, fine-grained annotations for racket pose alongside traditional ball positions, enabling research into complex human-object interactions. It is designed to tackle three interconnected tasks: fine-grained ball tracking, articulated racket pose estimation, and predictive ball trajectory forecasting. Our evaluation of established baselines reveals a critical insight for multi-modal fusion: while naively concatenating racket pose features degrades performance, a CrossAttention mechanism is essential to unlock their value, leading to trajectory prediction results that surpass strong unimodal baselines. RacketVision provides a versatile resource and a strong starting point for future research in dynamic object tracking, conditional motion forecasting, and multimodal analysis in sports. Project page at https://github.com/OrcustD/RacketVision",
      "published_utc": "2025-11-21T08:44:33Z",
      "updated_utc": "2026-01-28T16:59:28Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2511.17045v3",
      "abs_url": "http://arxiv.org/abs/2511.17045v3"
    },
    "2601.21504v1": {
      "arxiv_id": "2601.21504v1",
      "title": "Don't double it: Efficient Agent Prediction in Occlusions",
      "authors": [
        "Anna Rothenhäusler",
        "Markus Mazzola",
        "Andreas Look",
        "Raghu Rajan",
        "Joschka Bödecker"
      ],
      "summary": "Occluded traffic agents pose a significant challenge for autonomous vehicles, as hidden pedestrians or vehicles can appear unexpectedly, yet this problem remains understudied. Existing learning-based methods, while capable of inferring the presence of hidden agents, often produce redundant occupancy predictions where a single agent is identified multiple times. This issue complicates downstream planning and increases computational load. To address this, we introduce MatchInformer, a novel transformer-based approach that builds on the state-of-the-art SceneInformer architecture. Our method improves upon prior work by integrating Hungarian Matching, a state-of-the-art object matching algorithm from object detection, into the training process to enforce a one-to-one correspondence between predictions and ground truth, thereby reducing redundancy. We further refine trajectory forecasts by decoupling an agent's heading from its motion, a strategy that improves the accuracy and interpretability of predicted paths. To better handle class imbalances, we propose using the Matthews Correlation Coefficient (MCC) to evaluate occupancy predictions. By considering all entries in the confusion matrix, MCC provides a robust measure even in sparse or imbalanced scenarios. Experiments on the Waymo Open Motion Dataset demonstrate that our approach improves reasoning about occluded regions and produces more accurate trajectory forecasts than prior methods.",
      "published_utc": "2026-01-29T10:22:38Z",
      "updated_utc": "2026-01-29T10:22:38Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2601.21504v1",
      "abs_url": "http://arxiv.org/abs/2601.21504v1"
    },
    "2601.21346v1": {
      "arxiv_id": "2601.21346v1",
      "title": "HPTune: Hierarchical Proactive Tuning for Collision-Free Model Predictive Control",
      "authors": [
        "Wei Zuo",
        "Chengyang Li",
        "Yikun Wang",
        "Bingyang Cheng",
        "Zeyi Ren",
        "Shuai Wang",
        "Derrick Wing Kwan Ng",
        "Yik-Chung Wu"
      ],
      "summary": "Parameter tuning is a powerful approach to enhance adaptability in model predictive control (MPC) motion planners. However, existing methods typically operate in a myopic fashion that only evaluates executed actions, leading to inefficient parameter updates due to the sparsity of failure events (e.g., obstacle nearness or collision). To cope with this issue, we propose to extend evaluation from executed to non-executed actions, yielding a hierarchical proactive tuning (HPTune) framework that combines both a fast-level tuning and a slow-level tuning. The fast one adopts risk indicators of predictive closing speed and predictive proximity distance, and the slow one leverages an extended evaluation loss for closed-loop backpropagation. Additionally, we integrate HPTune with the Doppler LiDAR that provides obstacle velocities apart from position-only measurements for enhanced motion predictions, thus facilitating the implementation of HPTune. Extensive experiments on high-fidelity simulator demonstrate that HPTune achieves efficient MPC tuning and outperforms various baseline schemes in complex environments. It is found that HPTune enables situation-tailored motion planning by formulating a safe, agile collision avoidance strategy.",
      "published_utc": "2026-01-29T07:15:39Z",
      "updated_utc": "2026-01-29T07:15:39Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2601.21346v1",
      "abs_url": "http://arxiv.org/abs/2601.21346v1"
    },
    "2512.10350v5": {
      "arxiv_id": "2512.10350v5",
      "title": "Geometric Dynamics of Agentic Loops in Large Language Models",
      "authors": [
        "Nicolas Tacheny"
      ],
      "summary": "Iterative LLM systems(self-refinement, chain-of-thought, autonomous agents) are increasingly deployed, yet their temporal dynamics remain uncharacterized. Prior work evaluates task performance at convergence but ignores the trajectory: how does semantic content evolve across iterations? Does it stabilize, drift, or oscillate? Without answering these questions, we cannot predict system behavior, guarantee stability, or systematically design iterative architectures. We formalize agentic loops as discrete dynamical systems in semantic space. Borrowing from dynamical systems theory, we define trajectories, attractors and dynamical regimes for recursive LLM transformations, providing rigorous geometric definitions adapted to this setting. Our framework reveals that agentic loops exhibit classifiable dynamics: contractive (convergence toward stable semantic attractors), oscillatory (cycling among attractors), or exploratory (unbounded divergence). Experiments on singular loops validate the framework. Iterative paraphrasing produces contractive dynamics with measurable attractor formation and decreasing dispersion. Iterative negation produces exploratory dynamics with no stable structure. Crucially, prompt design directly controls the dynamical regime - the same model exhibits fundamentally different geometric behaviors depending solely on the transformation applied. This work establishes that iterative LLM dynamics are predictable and controllable, opening new directions for stability analysis, trajectory forecasting, and principled design of composite loops that balance convergence and exploration.",
      "published_utc": "2025-12-11T07:06:14Z",
      "updated_utc": "2026-01-30T07:54:43Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2512.10350v5",
      "abs_url": "http://arxiv.org/abs/2512.10350v5"
    },
    "2602.01329v1": {
      "arxiv_id": "2602.01329v1",
      "title": "FlowCast: Trajectory Forecasting for Scalable Zero-Cost Speculative Flow Matching",
      "authors": [
        "Divya Jyoti Bajpai",
        "Shubham Agarwal",
        "Apoorv Saxena",
        "Kuldeep Kulkarni",
        "Subrata Mitra",
        "Manjesh Kumar Hanawal"
      ],
      "summary": "Flow Matching (FM) has recently emerged as a powerful approach for high-quality visual generation. However, their prohibitively slow inference due to a large number of denoising steps limits their potential use in real-time or interactive applications. Existing acceleration methods, like distillation, truncation, or consistency training, either degrade quality, incur costly retraining, or lack generalization. We propose FlowCast, a training-free speculative generation framework that accelerates inference by exploiting the fact that FM models are trained to preserve constant velocity. FlowCast speculates future velocity by extrapolating current velocity without incurring additional time cost, and accepts it if it is within a mean-squared error threshold. This constant-velocity forecasting allows redundant steps in stable regions to be aggressively skipped while retaining precision in complex ones. FlowCast is a plug-and-play framework that integrates seamlessly with any FM model and requires no auxiliary networks. We also present a theoretical analysis and bound the worst-case deviation between speculative and full FM trajectories. Empirical evaluations demonstrate that FlowCast achieves $>2.5\\times$ speedup in image generation, video generation, and editing tasks, outperforming existing baselines with no quality loss as compared to standard full generation.",
      "published_utc": "2026-02-01T16:50:15Z",
      "updated_utc": "2026-02-01T16:50:15Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.01329v1",
      "abs_url": "http://arxiv.org/abs/2602.01329v1"
    },
    "2602.00993v1": {
      "arxiv_id": "2602.00993v1",
      "title": "HERMES: A Holistic End-to-End Risk-Aware Multimodal Embodied System with Vision-Language Models for Long-Tail Autonomous Driving",
      "authors": [
        "Weizhe Tang",
        "Junwei You",
        "Jiaxi Liu",
        "Zhaoyi Wang",
        "Rui Gan",
        "Zilin Huang",
        "Feng Wei",
        "Bin Ran"
      ],
      "summary": "End-to-end autonomous driving models increasingly benefit from large vision--language models for semantic understanding, yet ensuring safe and accurate operation under long-tail conditions remains challenging. These challenges are particularly prominent in long-tail mixed-traffic scenarios, where autonomous vehicles must interact with heterogeneous road users, including human-driven vehicles and vulnerable road users, under complex and uncertain conditions. This paper proposes HERMES, a holistic risk-aware end-to-end multimodal driving framework designed to inject explicit long-tail risk cues into trajectory planning. HERMES employs a foundation-model-assisted annotation pipeline to produce structured Long-Tail Scene Context and Long-Tail Planning Context, capturing hazard-centric cues together with maneuver intent and safety preference, and uses these signals to guide end-to-end planning. HERMES further introduces a Tri-Modal Driving Module that fuses multi-view perception, historical motion cues, and semantic guidance, ensuring risk-aware accurate trajectory planning under long-tail scenarios. Experiments on the real-world long-tail dataset demonstrate that HERMES consistently outperforms representative end-to-end and VLM-driven baselines under long-tail mixed-traffic scenarios. Ablation studies verify the complementary contributions of key components.",
      "published_utc": "2026-02-01T03:15:08Z",
      "updated_utc": "2026-02-01T03:15:08Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.00993v1",
      "abs_url": "http://arxiv.org/abs/2602.00993v1"
    },
    "2602.03376v1": {
      "arxiv_id": "2602.03376v1",
      "title": "PlanTRansformer: Unified Prediction and Planning with Goal-conditioned Transformer",
      "authors": [
        "Constantin Selzer",
        "Fabina B. Flohr"
      ],
      "summary": "Trajectory prediction and planning are fundamental yet disconnected components in autonomous driving. Prediction models forecast surrounding agent motion under unknown intentions, producing multimodal distributions, while planning assumes known ego objectives and generates deterministic trajectories. This mismatch creates a critical bottleneck: prediction lacks supervision for agent intentions, while planning requires this information. Existing prediction models, despite strong benchmarking performance, often remain disconnected from planning constraints such as collision avoidance and dynamic feasibility. We introduce Plan TRansformer (PTR), a unified Gaussian Mixture Transformer framework integrating goal-conditioned prediction, dynamic feasibility, interaction awareness, and lane-level topology reasoning. A teacher-student training strategy progressively masks surrounding agent commands during training to align with inference conditions where agent intentions are unavailable. PTR achieves 4.3%/3.5% improvement in marginal/joint mAP compared to the baseline Motion Transformer (MTR) and 15.5% planning error reduction at 5s horizon compared to GameFormer. The architecture-agnostic design enables application to diverse Transformer-based prediction models. Project Website: https://github.com/SelzerConst/PlanTRansformer",
      "published_utc": "2026-02-03T10:55:05Z",
      "updated_utc": "2026-02-03T10:55:05Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.03376v1",
      "abs_url": "http://arxiv.org/abs/2602.03376v1"
    },
    "2601.10554v2": {
      "arxiv_id": "2601.10554v2",
      "title": "DeepUrban: Interaction-Aware Trajectory Prediction and Planning for Automated Driving by Aerial Imagery",
      "authors": [
        "Constantin Selzer",
        "Fabian B. Flohr"
      ],
      "summary": "The efficacy of autonomous driving systems hinges critically on robust prediction and planning capabilities. However, current benchmarks are impeded by a notable scarcity of scenarios featuring dense traffic, which is essential for understanding and modeling complex interactions among road users. To address this gap, we collaborated with our industrial partner, DeepScenario, to develop DeepUrban-a new drone dataset designed to enhance trajectory prediction and planning benchmarks focusing on dense urban settings. DeepUrban provides a rich collection of 3D traffic objects, extracted from high-resolution images captured over urban intersections at approximately 100 meters altitude. The dataset is further enriched with comprehensive map and scene information to support advanced modeling and simulation tasks. We evaluate state-of-the-art (SOTA) prediction and planning methods, and conducted experiments on generalization capabilities. Our findings demonstrate that adding DeepUrban to nuScenes can boost the accuracy of vehicle predictions and planning, achieving improvements up to 44.1 % / 44.3% on the ADE / FDE metrics. Website: https://iv.ee.hm.edu/deepurban",
      "published_utc": "2026-01-15T16:18:42Z",
      "updated_utc": "2026-02-03T10:22:16Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2601.10554v2",
      "abs_url": "http://arxiv.org/abs/2601.10554v2"
    },
    "2602.04204v1": {
      "arxiv_id": "2602.04204v1",
      "title": "AGMA: Adaptive Gaussian Mixture Anchors for Prior-Guided Multimodal Human Trajectory Forecasting",
      "authors": [
        "Chao Li",
        "Rui Zhang",
        "Siyuan Huang",
        "Xian Zhong",
        "Hongbo Jiang"
      ],
      "summary": "Human trajectory forecasting requires capturing the multimodal nature of pedestrian behavior. However, existing approaches suffer from prior misalignment. Their learned or fixed priors often fail to capture the full distribution of plausible futures, limiting both prediction accuracy and diversity. We theoretically establish that prediction error is lower-bounded by prior quality, making prior modeling a key performance bottleneck. Guided by this insight, we propose AGMA (Adaptive Gaussian Mixture Anchors), which constructs expressive priors through two stages: extracting diverse behavioral patterns from training data and distilling them into a scene-adaptive global prior for inference. Extensive experiments on ETH-UCY, Stanford Drone, and JRDB datasets demonstrate that AGMA achieves state-of-the-art performance, confirming the critical role of high-quality priors in trajectory forecasting.",
      "published_utc": "2026-02-04T04:42:57Z",
      "updated_utc": "2026-02-04T04:42:57Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.04204v1",
      "abs_url": "http://arxiv.org/abs/2602.04204v1"
    },
    "2602.03986v1": {
      "arxiv_id": "2602.03986v1",
      "title": "eCP: Informative uncertainty quantification via Equivariantized Conformal Prediction with pre-trained models",
      "authors": [
        "Nikolaos Bousias",
        "Lars Lindemann",
        "George Pappas"
      ],
      "summary": "We study the effect of group symmetrization of pre-trained models on conformal prediction (CP), a post-hoc, distribution-free, finite-sample method of uncertainty quantification that offers formal coverage guarantees under the assumption of data exchangeability. Unfortunately, CP uncertainty regions can grow significantly in long horizon missions, rendering the statistical guarantees uninformative. To that end, we propose infusing CP with geometric information via group-averaging of the pretrained predictor to distribute the non-conformity mass across the orbits. Each sample now is treated as a representative of an orbit, thus uncertainty can be mitigated by other samples entangled to it via the orbit inducing elements of the symmetry group. Our approach provably yields contracted non-conformity scores in increasing convex order, implying improved exponential-tail bounds and sharper conformal prediction sets in expectation, especially at high confidence levels. We then propose an experimental design to test these theoretical claims in pedestrian trajectory prediction.",
      "published_utc": "2026-02-03T20:18:59Z",
      "updated_utc": "2026-02-03T20:18:59Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.RO",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.03986v1",
      "abs_url": "http://arxiv.org/abs/2602.03986v1"
    },
    "2602.04522v1": {
      "arxiv_id": "2602.04522v1",
      "title": "A Unified Complementarity-based Approach for Rigid-Body Manipulation and Motion Prediction",
      "authors": [
        "Bingkun Huang",
        "Xin Ma",
        "Nilanjan Chakraborty",
        "Riddhiman Laha"
      ],
      "summary": "Robotic manipulation in unstructured environments requires planners to reason jointly about free-space motion and sustained, frictional contact with the environment. Existing (local) planning and simulation frameworks typically separate these regimes or rely on simplified contact representations, particularly when modeling non-convex or distributed contact patches. Such approximations limit the fidelity of contact-mode transitions and hinder the robust execution of contact-rich behaviors in real time. This paper presents a unified discrete-time modeling framework for robotic manipulation that consistently captures both free motion and frictional contact within a single mathematical formalism (Unicomp). Building on complementarity-based rigid-body dynamics, we formulate free-space motion and contact interactions as coupled linear and nonlinear complementarity problems, enabling principled transitions between contact modes without enforcing fixed-contact assumptions. For planar patch contact, we derive a frictional contact model from the maximum power dissipation principle in which the set of admissible contact wrenches is represented by an ellipsoidal limit surface. This representation captures coupled force-moment effects, including torsional friction, while remaining agnostic to the underlying pressure distribution across the contact patch. The resulting formulation yields a discrete-time predictive model that relates generalized velocities and contact wrenches through quadratic constraints and is suitable for real-time optimization-based planning. Experimental results show that the proposed approach enables stable, physically consistent behavior at interactive speeds across tasks, from planar pushing to contact-rich whole-body maneuvers.",
      "published_utc": "2026-02-04T13:10:57Z",
      "updated_utc": "2026-02-04T13:10:57Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.04522v1",
      "abs_url": "http://arxiv.org/abs/2602.04522v1"
    },
    "2601.02082v2": {
      "arxiv_id": "2601.02082v2",
      "title": "Realistic adversarial scenario generation via human-like pedestrian model for autonomous vehicle control parameter optimisation",
      "authors": [
        "Yueyang Wang",
        "Mehmet Dogar",
        "Russell Darling",
        "Gustav Markkula"
      ],
      "summary": "Autonomous vehicles (AVs) are rapidly advancing and are expected to play a central role in future mobility. Ensuring their safe deployment requires reliable interaction with other road users, not least pedestrians. Direct testing on public roads is costly and unsafe for rare but critical interactions, making simulation a practical alternative. Within simulation-based testing, adversarial scenarios are widely used to probe safety limits, but many prioritise difficulty over realism, producing exaggerated behaviours which may result in AV controllers that are overly conservative. We propose an alternative method, instead using a cognitively inspired pedestrian model featuring both inter-individual and intra-individual variability to generate behaviourally plausible adversarial scenarios. We provide a proof of concept demonstration of this method's potential for AV control optimisation, in closed-loop testing and tuning of an AV controller. Our results show that replacing the rule-based CARLA pedestrian with the human-like model yields more realistic gap acceptance patterns and smoother vehicle decelerations. Unsafe interactions occur only for certain pedestrian individuals and conditions, underscoring the importance of human variability in AV testing. Adversarial scenarios generated by this model can be used to optimise AV control towards safer and more efficient behaviour. Overall, this work illustrates how incorporating human-like road user models into simulation-based adversarial testing can enhance the credibility of AV evaluation and provide a practical basis to behaviourally informed controller optimisation.",
      "published_utc": "2026-01-05T13:10:32Z",
      "updated_utc": "2026-02-04T14:57:40Z",
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2601.02082v2",
      "abs_url": "http://arxiv.org/abs/2601.02082v2"
    },
    "2505.18647v2": {
      "arxiv_id": "2505.18647v2",
      "title": "STFlow: Data-Coupled Flow Matching for Geometric Trajectory Simulation",
      "authors": [
        "Kiet Bennema ten Brinke",
        "Koen Minartz",
        "Vlado Menkovski"
      ],
      "summary": "Simulating trajectories of dynamical systems is a fundamental problem in a wide range of fields such as molecular dynamics, biochemistry, and pedestrian dynamics. Machine learning has become an invaluable tool for scaling physics-based simulators and developing models directly from experimental data. In particular, recent advances in deep generative modeling and geometric deep learning enable probabilistic simulation by learning complex trajectory distributions while respecting intrinsic permutation and time-shift symmetries. However, trajectories of N-body systems are commonly characterized by high sensitivity to perturbations leading to bifurcations, as well as multi-scale temporal and spatial correlations. To address these challenges, we introduce STFlow (Spatio-Temporal Flow), a generative model based on graph neural networks and hierarchical convolutions. By incorporating data-dependent couplings within the Flow Matching framework, STFlow denoises starting from conditioned random-walks instead of Gaussian noise. This novel informed prior simplifies the learning task by reducing transport cost, increasing training and inference efficiency. We validate our approach on N-body systems, molecular dynamics, and human trajectory forecasting. Across these benchmarks, STFlow achieves the lowest prediction errors with fewer simulation steps and improved scalability.",
      "published_utc": "2025-05-24T11:18:59Z",
      "updated_utc": "2026-02-05T20:54:19Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.18647v2",
      "abs_url": "http://arxiv.org/abs/2505.18647v2"
    },
    "2602.07938v1": {
      "arxiv_id": "2602.07938v1",
      "title": "Integrating Specialized and Generic Agent Motion Prediction with Dynamic Occupancy Grid Maps",
      "authors": [
        "Rabbia Asghar",
        "Lukas Rummelhard",
        "Wenqian Liu",
        "Anne Spalanzani",
        "Christian Laugier"
      ],
      "summary": "Accurate prediction of driving scene is a challenging task due to uncertainty in sensor data, the complex behaviors of agents, and the possibility of multiple feasible futures. Existing prediction methods using occupancy grid maps primarily focus on agent-agnostic scene predictions, while agent-specific predictions provide specialized behavior insights with the help of semantic information. However, both paradigms face distinct limitations: agent-agnostic models struggle to capture the behavioral complexities of dynamic actors, whereas agent-specific approaches fail to generalize to poorly perceived or unrecognized agents; combining both enables robust and safer motion forecasting. To address this, we propose a unified framework by leveraging Dynamic Occupancy Grid Maps within a streamlined temporal decoding pipeline to simultaneously predict future occupancy state grids, vehicle grids, and scene flow grids. Relying on a lightweight spatiotemporal backbone, our approach is centered on a tailored, interdependent loss function that captures inter-grid dependencies and enables diverse future predictions. By using occupancy state information to enforce flow-guided transitions, the loss function acts as a regularizer that directs occupancy evolution while accounting for obstacles and occlusions. Consequently, the model not only predicts the specific behaviors of vehicle agents, but also identifies other dynamic entities and anticipates their evolution within the complex scene. Evaluations on real-world nuScenes and Woven Planet datasets demonstrate superior prediction performances for dynamic vehicles and generic dynamic scene elements compared to baseline methods.",
      "published_utc": "2026-02-08T12:13:06Z",
      "updated_utc": "2026-02-08T12:13:06Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.07938v1",
      "abs_url": "http://arxiv.org/abs/2602.07938v1"
    },
    "2602.07633v1": {
      "arxiv_id": "2602.07633v1",
      "title": "Flow-Based Conformal Predictive Distributions",
      "authors": [
        "Trevor Harris"
      ],
      "summary": "Conformal prediction provides a distribution-free framework for uncertainty quantification via prediction sets with exact finite-sample coverage. In low dimensions these sets are easy to interpret, but in high-dimensional or structured output spaces they are difficult to represent and use, which can limit their ability to integrate with downstream tasks such as sampling and probabilistic forecasting. We show that any differentiable nonconformity score induces a deterministic flow on the output space whose trajectories converge to the boundary of the corresponding conformal prediction set. This leads to a computationally efficient, training-free method for sampling conformal boundaries in arbitrary dimensions. Boundary samples can be reconformalized to form pointwise prediction sets with controlled risk, and mixing across confidence levels yields conformal predictive distributions whose quantile regions coincide exactly with conformal prediction sets. We evaluate the approach on PDE inverse problems, precipitation downscaling, climate model debiasing, and hurricane trajectory forecasting.",
      "published_utc": "2026-02-07T17:26:50Z",
      "updated_utc": "2026-02-07T17:26:50Z",
      "primary_category": "stat.ML",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.07633v1",
      "abs_url": "http://arxiv.org/abs/2602.07633v1"
    },
    "2602.09076v1": {
      "arxiv_id": "2602.09076v1",
      "title": "Legs Over Arms: On the Predictive Value of Lower-Body Pose for Human Trajectory Prediction from Egocentric Robot Perception",
      "authors": [
        "Nhat Le",
        "Daeun Song",
        "Xuesu Xiao"
      ],
      "summary": "Predicting human trajectory is crucial for social robot navigation in crowded environments. While most existing approaches treat human as point mass, we present a study on multi-agent trajectory prediction that leverages different human skeletal features for improved forecast accuracy. In particular, we systematically evaluate the predictive utility of 2D and 3D skeletal keypoints and derived biomechanical cues as additional inputs. Through a comprehensive study on the JRDB dataset and another new dataset for social navigation with 360-degree panoramic videos, we find that focusing on lower-body 3D keypoints yields a 13% reduction in Average Displacement Error and augmenting 3D keypoint inputs with corresponding biomechanical cues provides a further 1-4% improvement. Notably, the performance gain persists when using 2D keypoint inputs extracted from equirectangular panoramic images, indicating that monocular surround vision can capture informative cues for motion forecasting. Our finding that robots can forecast human movement efficiently by watching their legs provides actionable insights for designing sensing capabilities for social robot navigation.",
      "published_utc": "2026-02-09T16:56:23Z",
      "updated_utc": "2026-02-09T16:56:23Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.09076v1",
      "abs_url": "http://arxiv.org/abs/2602.09076v1"
    },
    "2602.10884v1": {
      "arxiv_id": "2602.10884v1",
      "title": "ResWorld: Temporal Residual World Model for End-to-End Autonomous Driving",
      "authors": [
        "Jinqing Zhang",
        "Zehua Fu",
        "Zelin Xu",
        "Wenying Dai",
        "Qingjie Liu",
        "Yunhong Wang"
      ],
      "summary": "The comprehensive understanding capabilities of world models for driving scenarios have significantly improved the planning accuracy of end-to-end autonomous driving frameworks. However, the redundant modeling of static regions and the lack of deep interaction with trajectories hinder world models from exerting their full effectiveness. In this paper, we propose Temporal Residual World Model (TR-World), which focuses on dynamic object modeling. By calculating the temporal residuals of scene representations, the information of dynamic objects can be extracted without relying on detection and tracking. TR-World takes only temporal residuals as input, thus predicting the future spatial distribution of dynamic objects more precisely. By combining the prediction with the static object information contained in the current BEV features, accurate future BEV features can be obtained. Furthermore, we propose Future-Guided Trajectory Refinement (FGTR) module, which conducts interaction between prior trajectories (predicted from the current scene representation) and the future BEV features. This module can not only utilize future road conditions to refine trajectories, but also provides sparse spatial-temporal supervision on future BEV features to prevent world model collapse. Comprehensive experiments conducted on the nuScenes and NAVSIM datasets demonstrate that our method, namely ResWorld, achieves state-of-the-art planning performance. The code is available at https://github.com/mengtan00/ResWorld.git.",
      "published_utc": "2026-02-11T14:12:26Z",
      "updated_utc": "2026-02-11T14:12:26Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.10884v1",
      "abs_url": "http://arxiv.org/abs/2602.10884v1"
    },
    "2601.10233v2": {
      "arxiv_id": "2601.10233v2",
      "title": "Proactive Local-Minima-Free Robot Navigation: Blending Motion Prediction with Safe Control",
      "authors": [
        "Yifan Xue",
        "Ze Zhang",
        "Knut Åkesson",
        "Nadia Figueroa"
      ],
      "summary": "This work addresses the challenge of safe and efficient mobile robot navigation in complex dynamic environments with concave moving obstacles. Reactive safe controllers like Control Barrier Functions (CBFs) design obstacle avoidance strategies based only on the current states of the obstacles, risking future collisions. To alleviate this problem, we use Gaussian processes to learn barrier functions online from multimodal motion predictions of obstacles generated by neural networks trained with energy-based learning. The learned barrier functions are then fed into quadratic programs using modulated CBFs (MCBFs), a local-minimum-free version of CBFs, to achieve safe and efficient navigation. The proposed framework makes two key contributions. First, it develops a prediction-to-barrier function online learning pipeline. Second, it introduces an autonomous parameter tuning algorithm that adapts MCBFs to deforming, prediction-based barrier functions. The framework is evaluated in both simulations and real-world experiments, consistently outperforming baselines and demonstrating superior safety and efficiency in crowded dynamic environments.",
      "published_utc": "2026-01-15T09:46:03Z",
      "updated_utc": "2026-02-11T16:48:21Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2601.10233v2",
      "abs_url": "http://arxiv.org/abs/2601.10233v2"
    },
    "2602.11214v1": {
      "arxiv_id": "2602.11214v1",
      "title": "DD-MDN: Human Trajectory Forecasting with Diffusion-Based Dual Mixture Density Networks and Uncertainty Self-Calibration",
      "authors": [
        "Manuel Hetzel",
        "Kerim Turacan",
        "Hannes Reichert",
        "Konrad Doll",
        "Bernhard Sick"
      ],
      "summary": "Human Trajectory Forecasting (HTF) predicts future human movements from past trajectories and environmental context, with applications in Autonomous Driving, Smart Surveillance, and Human-Robot Interaction. While prior work has focused on accuracy, social interaction modeling, and diversity, little attention has been paid to uncertainty modeling, calibration, and forecasts from short observation periods, which are crucial for downstream tasks such as path planning and collision avoidance. We propose DD-MDN, an end-to-end probabilistic HTF model that combines high positional accuracy, calibrated uncertainty, and robustness to short observations. Using a few-shot denoising diffusion backbone and a dual mixture density network, our method learns self-calibrated residence areas and probability-ranked anchor paths, from which diverse trajectory hypotheses are derived, without predefined anchors or endpoints. Experiments on the ETH/UCY, SDD, inD, and IMPTC datasets demonstrate state-of-the-art accuracy, robustness at short observation intervals, and reliable uncertainty modeling. The code is available at: https://github.com/kav-institute/ddmdn.",
      "published_utc": "2026-02-11T08:59:33Z",
      "updated_utc": "2026-02-11T08:59:33Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2602.11214v1",
      "abs_url": "http://arxiv.org/abs/2602.11214v1"
    }
  },
  "topics": {
    "pedestrian_trajectory_prediction": {
      "name": "Pedestrian Trajectory Prediction",
      "papers": [
        "2505.00586v2",
        "2504.17371v3",
        "2405.16439v3",
        "2403.00353v1",
        "2306.01075v1",
        "2210.10254v2",
        "2209.02297v1",
        "2106.00559v2",
        "2008.08294v2",
        "2003.09996v1",
        "1911.09476v1",
        "1910.03088v1",
        "1906.00486v4",
        "1806.09453v1",
        "1806.09444v1",
        "1804.00495v2",
        "1803.03577v1",
        "2511.11164v2",
        "2511.09735v1",
        "2510.17731v1",
        "2510.04365v1",
        "2510.02469v1",
        "2510.03314v1",
        "2509.24020v1",
        "2509.15219v1",
        "2509.10570v1",
        "2509.00624v1",
        "2508.14523v1",
        "2508.07146v1",
        "2508.07079v1",
        "2508.04229v1",
        "2508.03541v1",
        "2507.22742v1",
        "2507.19119v3",
        "2507.13397v3",
        "2506.22111v1",
        "2506.14144v1",
        "2506.09626v1",
        "2506.00871v3",
        "2505.06743v3",
        "2504.18944v1",
        "2504.15616v1",
        "2504.13647v1",
        "2503.24272v1",
        "2503.22201v1",
        "2503.08016v1",
        "2503.06832v1",
        "2503.04952v1",
        "2502.16847v1",
        "2502.14676v2",
        "2502.10585v1",
        "2502.02504v1",
        "2501.13848v1",
        "2501.13973v1",
        "2501.09878v1",
        "2501.07711v1",
        "2501.06680v2",
        "2501.02530v1",
        "2501.01915v1",
        "2412.08096v1",
        "2412.04673v1",
        "2412.03689v1",
        "2412.02395v1",
        "2411.17376v3",
        "2411.00174v1",
        "2410.19544v1",
        "2410.16864v1",
        "2409.20324v1",
        "2409.15224v1",
        "2409.14984v1",
        "2409.04069v2",
        "2409.01971v2",
        "2408.15250v1",
        "2408.12609v1",
        "2407.17162v1",
        "2407.11588v1",
        "2407.05811v3",
        "2406.18050v1",
        "2406.14746v3",
        "2406.14422v1",
        "2406.02436v3",
        "2406.00749v1",
        "2405.18945v1",
        "2405.17680v2",
        "2405.07164v1",
        "2601.21504v1",
        "2602.04204v1",
        "2602.03986v1",
        "2505.18647v2"
      ],
      "latest_update": "2026-02-09"
    },
    "cyclist_micromobility_prediction": {
      "name": "Cyclist & Micromobility Prediction",
      "papers": [
        "1803.03577v1",
        "2510.03314v1",
        "2508.14523v1",
        "2507.22742v1",
        "2505.06743v3",
        "2502.14676v2",
        "2412.20368v1",
        "2406.01431v4",
        "2404.08363v3",
        "2403.16374v1",
        "2312.16168v3",
        "2311.04383v1",
        "2311.00815v1",
        "2211.01696v5",
        "2206.15275v1",
        "2106.15991v1",
        "2104.11212v1",
        "2104.09176v1",
        "2102.09117v1",
        "2006.14480v2",
        "2004.12255v2",
        "2002.06241v1",
        "2002.05966v5",
        "1910.06673v1",
        "1909.13486v2",
        "1907.08752v1",
        "1907.12887v5",
        "1907.03395v2",
        "1902.10928v4",
        "1812.04767v4",
        "1811.02146v5",
        "1601.00998v1"
      ],
      "latest_update": "2026-01-16"
    },
    "interaction_aware_models": {
      "name": "Interaction-aware & Social Models",
      "papers": [
        "2512.02777v1",
        "2506.12474v1",
        "2505.00586v2",
        "2504.17371v3",
        "2504.15541v1",
        "2504.13111v3",
        "2501.13461v2",
        "2405.16439v3",
        "2405.02145v1",
        "2404.11946v1",
        "2404.11181v2",
        "2403.00353v1",
        "2312.05144v1",
        "2309.13893v3",
        "2306.01075v1",
        "2211.10226v1",
        "2207.10398v1",
        "2202.05140v2",
        "2111.00788v3",
        "2106.00559v2",
        "2011.12406v2",
        "2010.16267v3",
        "2008.08294v2",
        "2005.08307v2",
        "2003.09996v1",
        "2002.01965v1",
        "1911.03801v1",
        "1910.03088v1",
        "1909.00792v1",
        "1906.00486v4",
        "1808.06887v5",
        "1807.09995v1",
        "1803.03577v1",
        "1705.02445v1",
        "2601.10554v1",
        "2601.09856v1",
        "2601.09377v1",
        "2601.01547v1",
        "2512.21133v1",
        "2512.16907v2",
        "2512.13903v1",
        "2512.12717v1",
        "2512.12211v1",
        "2512.05270v1",
        "2512.05008v1",
        "2512.03936v1",
        "2512.03795v1",
        "2512.03756v1",
        "2512.02368v1",
        "2512.01478v2",
        "2511.22181v1",
        "2511.18874v1",
        "2511.18248v2",
        "2511.17941v1",
        "2511.17798v1",
        "2511.17150v1",
        "2511.17045v2",
        "2511.16183v1",
        "2511.14237v1",
        "2511.12878v3",
        "2511.12232v2",
        "2511.11218v2",
        "2511.11164v2",
        "2511.13753v1",
        "2511.10411v1",
        "2511.10203v1",
        "2511.09735v1",
        "2510.22205v1",
        "2510.19789v1",
        "2510.14250v1",
        "2510.10731v1",
        "2510.10086v1",
        "2510.04365v1",
        "2510.04233v1",
        "2510.03776v1",
        "2510.03031v1",
        "2510.02627v1",
        "2510.02469v1",
        "2510.00405v1",
        "2510.00060v2",
        "2509.24209v1",
        "2509.24020v1",
        "2509.17850v1",
        "2509.17080v1",
        "2509.16095v1",
        "2509.15984v1",
        "2509.15513v1",
        "2509.15219v1",
        "2509.14801v1",
        "2509.12151v1",
        "2509.10426v2",
        "2509.10096v1",
        "2509.09210v1",
        "2509.01836v1",
        "2508.21043v2",
        "2508.20812v1",
        "2509.09696v2",
        "2601.12119v1",
        "2512.02368v2",
        "2512.10350v4",
        "2601.20720v1",
        "2601.20367v1",
        "2511.17045v3",
        "2601.21504v1",
        "2512.10350v5",
        "2602.01329v1",
        "2602.03376v1",
        "2601.10554v2",
        "2602.04522v1",
        "2602.07938v1",
        "2602.09076v1",
        "2602.10884v1",
        "2602.11214v1"
      ],
      "latest_update": "2026-02-13"
    },
    "intention_crossing_behavior": {
      "name": "Intention & Crossing Behavior",
      "papers": [
        "2505.09935v1",
        "2502.15824v1",
        "2405.16439v3",
        "2311.16091v1",
        "2201.04742v1",
        "2010.05115v1",
        "2008.08294v2",
        "2003.09998v1",
        "2003.09996v1",
        "1809.03705v3",
        "1806.09444v1",
        "1804.00495v2",
        "1803.03577v1",
        "1803.02242v1",
        "2601.10521v1",
        "2601.02082v1",
        "2601.01989v1",
        "2512.24129v1",
        "2511.20020v1",
        "2511.20011v1",
        "2511.20008v1",
        "2511.19109v1",
        "2511.00858v1",
        "2510.20158v1",
        "2510.15673v1",
        "2510.03314v1",
        "2509.14303v1",
        "2509.04117v1",
        "2508.21690v1",
        "2508.19866v1",
        "2508.15336v1",
        "2508.07146v1",
        "2508.04229v1",
        "2507.21161v1",
        "2507.12433v1",
        "2507.04141v1",
        "2506.22111v1",
        "2506.17590v2",
        "2506.09626v1",
        "2506.05883v1",
        "2504.15616v1",
        "2504.06292v1",
        "2503.08437v1",
        "2503.08016v1",
        "2503.04952v1",
        "2501.07711v1",
        "2412.03689v1",
        "2412.02447v2",
        "2411.13302v1",
        "2410.13039v2",
        "2410.06400v1",
        "2409.20223v2",
        "2409.07645v1",
        "2409.06707v1",
        "2408.15250v1",
        "2407.18451v1",
        "2407.17162v1",
        "2407.00446v1",
        "2406.16817v1",
        "2406.00473v1",
        "2405.19202v5",
        "2405.13969v1",
        "2404.17031v2",
        "2404.09574v1",
        "2403.16374v1",
        "2403.06774v1",
        "2402.19002v2",
        "2402.12810v3",
        "2601.10521v2",
        "2602.00993v1",
        "2601.02082v2"
      ],
      "latest_update": "2026-02-05"
    },
    "risk_safety_collision": {
      "name": "Risk-aware / Safety / Collision Prediction",
      "papers": [
        "2511.09735v1",
        "2510.04365v1",
        "2510.03314v1",
        "2509.15219v1",
        "2509.10570v1",
        "2509.00624v1",
        "2508.14523v1",
        "2508.07079v1",
        "2507.22742v1",
        "2506.22111v1",
        "2506.14144v1",
        "2504.17371v3",
        "2503.08016v1",
        "2501.02530v1",
        "2412.03689v1",
        "2409.20324v1",
        "2409.15224v1",
        "2406.18050v1",
        "2406.02436v3",
        "2404.15557v2",
        "2404.02227v1",
        "2403.16485v1",
        "2402.08698v2",
        "2402.03893v2",
        "2312.15881v2",
        "2312.03296v1",
        "2311.15193v2",
        "2311.14922v3",
        "2312.10041v1",
        "2311.04383v1",
        "2310.14570v1",
        "2308.06654v1",
        "2307.05288v2",
        "2305.15942v1",
        "2305.02859v2",
        "2303.04320v2",
        "2303.01424v1",
        "2302.07583v1",
        "2210.10254v2",
        "2209.02297v1",
        "2207.02281v1",
        "2207.02279v1",
        "2206.13387v1",
        "2203.09293v1",
        "2202.03954v1",
        "2112.06624v1",
        "2111.03822v1",
        "2108.10879v2",
        "2108.08236v3",
        "2107.11637v5",
        "2106.12442v1",
        "2011.10670v3",
        "2010.12007v2",
        "2009.10468v2",
        "2007.14558v2",
        "2003.06594v1",
        "2002.03038v1",
        "2001.11597v1",
        "1910.06673v1",
        "1910.03088v1",
        "1907.01577v2",
        "1906.08469v2",
        "1903.01860v1",
        "1902.05437v1",
        "1803.03577v1",
        "1706.05904v2",
        "2601.10233v1",
        "2601.09856v1",
        "2601.09377v1",
        "2512.18211v1",
        "2512.16784v1",
        "2512.12717v1",
        "2512.10056v1",
        "2512.06190v1",
        "2512.05682v1",
        "2512.05270v1",
        "2512.03795v1",
        "2512.02777v1",
        "2512.02368v1",
        "2511.18170v1",
        "2511.17941v1",
        "2511.12232v2",
        "2511.13753v1",
        "2511.10411v1",
        "2511.10203v1",
        "2511.00126v1",
        "2510.22789v1",
        "2510.22205v1",
        "2510.17191v2",
        "2510.14250v1",
        "2510.13461v1",
        "2510.10086v1",
        "2510.03776v1",
        "2510.03031v1",
        "2510.02627v1",
        "2509.17080v1",
        "2509.10426v2",
        "2509.07464v1",
        "2509.01836v1",
        "2509.01294v1",
        "2508.21559v1",
        "2508.20812v1",
        "2508.14198v1",
        "2508.12456v1",
        "2508.10567v1",
        "2508.10427v2",
        "2508.07668v1",
        "2508.05616v1",
        "2508.04564v1",
        "2508.06544v1",
        "2507.15266v1",
        "2507.13073v1",
        "2507.12463v1",
        "2507.12083v1",
        "2507.08563v2",
        "2507.03365v1",
        "2506.23164v1",
        "2506.19341v1",
        "2506.11419v1",
        "2508.10427v3",
        "2512.02368v2",
        "2601.20367v1",
        "2601.21346v1",
        "2602.03376v1",
        "2602.07633v1",
        "2601.10233v2",
        "2602.11214v1"
      ],
      "latest_update": "2026-02-13"
    }
  }
}