{
  "papers": {
    "2505.00586v2": {
      "arxiv_id": "2505.00586v2",
      "title": "ParkDiffusion: Heterogeneous Multi-Agent Multi-Modal Trajectory Prediction for Automated Parking using Diffusion Models",
      "authors": [
        "Jiarong Wei",
        "Niclas Vödisch",
        "Anna Rehr",
        "Christian Feist",
        "Abhinav Valada"
      ],
      "summary": "Automated parking is a critical feature of Advanced Driver Assistance Systems (ADAS), where accurate trajectory prediction is essential to bridge perception and planning modules. Despite its significance, research in this domain remains relatively limited, with most existing studies concentrating on single-modal trajectory prediction of vehicles. In this work, we propose ParkDiffusion, a novel approach that predicts the trajectories of both vehicles and pedestrians in automated parking scenarios. ParkDiffusion employs diffusion models to capture the inherent uncertainty and multi-modality of future trajectories, incorporating several key innovations. First, we propose a dual map encoder that processes soft semantic cues and hard geometric constraints using a two-step cross-attention mechanism. Second, we introduce an adaptive agent type embedding module, which dynamically conditions the prediction process on the distinct characteristics of vehicles and pedestrians. Third, to ensure kinematic feasibility, our model outputs control signals that are subsequently used within a kinematic framework to generate physically feasible trajectories. We evaluate ParkDiffusion on the Dragon Lake Parking (DLP) dataset and the Intersections Drone (inD) dataset. Our work establishes a new baseline for heterogeneous trajectory prediction in parking scenarios, outperforming existing methods by a considerable margin.",
      "published_utc": "2025-05-01T15:16:59Z",
      "updated_utc": "2025-08-13T15:18:31Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.00586v2",
      "abs_url": "http://arxiv.org/abs/2505.00586v2"
    },
    "2504.17371v3": {
      "arxiv_id": "2504.17371v3",
      "title": "Highly Accurate and Diverse Traffic Data: The DeepScenario Open 3D Dataset",
      "authors": [
        "Oussema Dhaouadi",
        "Johannes Meier",
        "Luca Wahl",
        "Jacques Kaiser",
        "Luca Scalerandi",
        "Nick Wandelburg",
        "Zhuolun Zhou",
        "Nijanthan Berinpanathan",
        "Holger Banzhaf",
        "Daniel Cremers"
      ],
      "summary": "Accurate 3D trajectory data is crucial for advancing autonomous driving. Yet, traditional datasets are usually captured by fixed sensors mounted on a car and are susceptible to occlusion. Additionally, such an approach can precisely reconstruct the dynamic environment in the close vicinity of the measurement vehicle only, while neglecting objects that are further away. In this paper, we introduce the DeepScenario Open 3D Dataset (DSC3D), a high-quality, occlusion-free dataset of 6 degrees of freedom bounding box trajectories acquired through a novel monocular camera drone tracking pipeline. Our dataset includes more than 175,000 trajectories of 14 types of traffic participants and significantly exceeds existing datasets in terms of diversity and scale, containing many unprecedented scenarios such as complex vehicle-pedestrian interaction on highly populated urban streets and comprehensive parking maneuvers from entry to exit. DSC3D dataset was captured in five various locations in Europe and the United States and include: a parking lot, a crowded inner-city, a steep urban intersection, a federal highway, and a suburban intersection. Our 3D trajectory dataset aims to enhance autonomous driving systems by providing detailed environmental 3D representations, which could lead to improved obstacle interactions and safety. We demonstrate its utility across multiple applications including motion prediction, motion planning, scenario mining, and generative reactive traffic agents. Our interactive online visualization platform and the complete dataset are publicly available at https://app.deepscenario.com, facilitating research in motion prediction, behavior modeling, and safety validation.",
      "published_utc": "2025-04-24T08:43:48Z",
      "updated_utc": "2025-08-22T13:21:59Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.17371v3",
      "abs_url": "http://arxiv.org/abs/2504.17371v3"
    },
    "2405.16439v3": {
      "arxiv_id": "2405.16439v3",
      "title": "Multi-Agent Inverse Reinforcement Learning in Real World Unstructured Pedestrian Crowds",
      "authors": [
        "Rohan Chandra",
        "Haresh Karnan",
        "Negar Mehr",
        "Peter Stone",
        "Joydeep Biswas"
      ],
      "summary": "Social robot navigation in crowded public spaces such as university campuses, restaurants, grocery stores, and hospitals, is an increasingly important area of research. One of the core strategies for achieving this goal is to understand humans' intent--underlying psychological factors that govern their motion--by learning their reward functions, typically via inverse reinforcement learning (IRL). Despite significant progress in IRL, learning reward functions of multiple agents simultaneously in dense unstructured pedestrian crowds has remained intractable due to the nature of the tightly coupled social interactions that occur in these scenarios \\textit{e.g.} passing, intersections, swerving, weaving, etc. In this paper, we present a new multi-agent maximum entropy inverse reinforcement learning algorithm for real world unstructured pedestrian crowds. Key to our approach is a simple, but effective, mathematical trick which we name the so-called tractability-rationality trade-off trick that achieves tractability at the cost of a slight reduction in accuracy. We compare our approach to the classical single-agent MaxEnt IRL as well as state-of-the-art trajectory prediction methods on several datasets including the ETH, UCY, SCAND, JRDB, and a new dataset, called Speedway, collected at a busy intersection on a University campus focusing on dense, complex agent interactions. Our key findings show that, on the dense Speedway dataset, our approach ranks 1st among top 7 baselines with >2X improvement over single-agent IRL, and is competitive with state-of-the-art large transformer-based encoder-decoder models on sparser datasets such as ETH/UCY (ranks 3rd among top 7 baselines).",
      "published_utc": "2024-05-26T05:48:21Z",
      "updated_utc": "2025-03-26T21:19:58Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.16439v3",
      "abs_url": "http://arxiv.org/abs/2405.16439v3"
    },
    "2403.00353v1": {
      "arxiv_id": "2403.00353v1",
      "title": "MS-Net: A Multi-Path Sparse Model for Motion Prediction in Multi-Scenes",
      "authors": [
        "Xiaqiang Tang",
        "Weigao Sun",
        "Siyuan Hu",
        "Yiyang Sun",
        "Yafeng Guo"
      ],
      "summary": "The multi-modality and stochastic characteristics of human behavior make motion prediction a highly challenging task, which is critical for autonomous driving. While deep learning approaches have demonstrated their great potential in this area, it still remains unsolved to establish a connection between multiple driving scenes (e.g., merging, roundabout, intersection) and the design of deep learning models. Current learning-based methods typically use one unified model to predict trajectories in different scenarios, which may result in sub-optimal results for one individual scene. To address this issue, we propose Multi-Scenes Network (aka. MS-Net), which is a multi-path sparse model trained by an evolutionary process. MS-Net selectively activates a subset of its parameters during the inference stage to produce prediction results for each scene. In the training stage, the motion prediction task under differentiated scenes is abstracted as a multi-task learning problem, an evolutionary algorithm is designed to encourage the network search of the optimal parameters for each scene while sharing common knowledge between different scenes. Our experiment results show that with substantially reduced parameters, MS-Net outperforms existing state-of-the-art methods on well-established pedestrian motion prediction datasets, e.g., ETH and UCY, and ranks the 2nd place on the INTERACTION challenge.",
      "published_utc": "2024-03-01T08:32:12Z",
      "updated_utc": "2024-03-01T08:32:12Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.00353v1",
      "abs_url": "http://arxiv.org/abs/2403.00353v1"
    },
    "2306.01075v1": {
      "arxiv_id": "2306.01075v1",
      "title": "Pedestrian Crossing Action Recognition and Trajectory Prediction with 3D Human Keypoints",
      "authors": [
        "Jiachen Li",
        "Xinwei Shi",
        "Feiyu Chen",
        "Jonathan Stroud",
        "Zhishuai Zhang",
        "Tian Lan",
        "Junhua Mao",
        "Jeonhyung Kang",
        "Khaled S. Refaat",
        "Weilong Yang",
        "Eugene Ie",
        "Congcong Li"
      ],
      "summary": "Accurate understanding and prediction of human behaviors are critical prerequisites for autonomous vehicles, especially in highly dynamic and interactive scenarios such as intersections in dense urban areas. In this work, we aim at identifying crossing pedestrians and predicting their future trajectories. To achieve these goals, we not only need the context information of road geometry and other traffic participants but also need fine-grained information of the human pose, motion and activity, which can be inferred from human keypoints. In this paper, we propose a novel multi-task learning framework for pedestrian crossing action recognition and trajectory prediction, which utilizes 3D human keypoints extracted from raw sensor data to capture rich information on human pose and activity. Moreover, we propose to apply two auxiliary tasks and contrastive learning to enable auxiliary supervisions to improve the learned keypoints representation, which further enhances the performance of major tasks. We validate our approach on a large-scale in-house dataset, as well as a public benchmark dataset, and show that our approach achieves state-of-the-art performance on a wide range of evaluation metrics. The effectiveness of each model component is validated in a detailed ablation study.",
      "published_utc": "2023-06-01T18:27:48Z",
      "updated_utc": "2023-06-01T18:27:48Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.01075v1",
      "abs_url": "http://arxiv.org/abs/2306.01075v1"
    },
    "2210.10254v2": {
      "arxiv_id": "2210.10254v2",
      "title": "Safe Planning in Dynamic Environments using Conformal Prediction",
      "authors": [
        "Lars Lindemann",
        "Matthew Cleaveland",
        "Gihyun Shim",
        "George J. Pappas"
      ],
      "summary": "We propose a framework for planning in unknown dynamic environments with probabilistic safety guarantees using conformal prediction. Particularly, we design a model predictive controller (MPC) that uses i) trajectory predictions of the dynamic environment, and ii) prediction regions quantifying the uncertainty of the predictions. To obtain prediction regions, we use conformal prediction, a statistical tool for uncertainty quantification, that requires availability of offline trajectory data - a reasonable assumption in many applications such as autonomous driving. The prediction regions are valid, i.e., they hold with a user-defined probability, so that the MPC is provably safe. We illustrate the results in the self-driving car simulator CARLA at a pedestrian-filled intersection. The strength of our approach is compatibility with state of the art trajectory predictors, e.g., RNNs and LSTMs, while making no assumptions on the underlying trajectory-generating distribution. To the best of our knowledge, these are the first results that provide valid safety guarantees in such a setting.",
      "published_utc": "2022-10-19T02:25:33Z",
      "updated_utc": "2023-06-08T04:46:57Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.10254v2",
      "abs_url": "http://arxiv.org/abs/2210.10254v2"
    },
    "2209.02297v1": {
      "arxiv_id": "2209.02297v1",
      "title": "SIND: A Drone Dataset at Signalized Intersection in China",
      "authors": [
        "Yanchao Xu",
        "Wenbo Shao",
        "Jun Li",
        "Kai Yang",
        "Weida Wang",
        "Hua Huang",
        "Chen Lv",
        "Hong Wang"
      ],
      "summary": "Intersection is one of the most challenging scenarios for autonomous driving tasks. Due to the complexity and stochasticity, essential applications (e.g., behavior modeling, motion prediction, safety validation, etc.) at intersections rely heavily on data-driven techniques. Thus, there is an intense demand for trajectory datasets of traffic participants (TPs) in intersections. Currently, most intersections in urban areas are equipped with traffic lights. However, there is not yet a large-scale, high-quality, publicly available trajectory dataset for signalized intersections. Therefore, in this paper, a typical two-phase signalized intersection is selected in Tianjin, China. Besides, a pipeline is designed to construct a Signalized INtersection Dataset (SIND), which contains 7 hours of recording including over 13,000 TPs with 7 types. Then, the behaviors of traffic light violations in SIND are recorded. Furthermore, the SIND is also compared with other similar works. The features of the SIND can be summarized as follows: 1) SIND provides more comprehensive information, including traffic light states, motion parameters, High Definition (HD) map, etc. 2) The category of TPs is diverse and characteristic, where the proportion of vulnerable road users (VRUs) is up to 62.6% 3) Multiple traffic light violations of non-motor vehicles are shown. We believe that SIND would be an effective supplement to existing datasets and can promote related research on autonomous driving.The dataset is available online via: https://github.com/SOTIF-AVLab/SinD",
      "published_utc": "2022-09-06T08:49:44Z",
      "updated_utc": "2022-09-06T08:49:44Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.GL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.02297v1",
      "abs_url": "http://arxiv.org/abs/2209.02297v1"
    },
    "2106.00559v2": {
      "arxiv_id": "2106.00559v2",
      "title": "Predicting Vehicles Trajectories in Urban Scenarios with Transformer Networks and Augmented Information",
      "authors": [
        "A. Quintanar",
        "D. Fernández-Llorca",
        "I. Parra",
        "R. Izquierdo",
        "M. A. Sotelo"
      ],
      "summary": "Understanding the behavior of road users is of vital importance for the development of trajectory prediction systems. In this context, the latest advances have focused on recurrent structures, establishing the social interaction between the agents involved in the scene. More recently, simpler structures have also been introduced for predicting pedestrian trajectories, based on Transformer Networks, and using positional information. They allow the individual modelling of each agent's trajectory separately without any complex interaction terms. Our model exploits these simple structures by adding augmented data (position and heading), and adapting their use to the problem of vehicle trajectory prediction in urban scenarios in prediction horizons up to 5 seconds. In addition, a cross-performance analysis is performed between different types of scenarios, including highways, intersections and roundabouts, using recent datasets (inD, rounD, highD and INTERACTION). Our model achieves state-of-the-art results and proves to be flexible and adaptable to different types of urban contexts.",
      "published_utc": "2021-06-01T15:18:55Z",
      "updated_utc": "2021-06-07T23:38:30Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2106.00559v2",
      "abs_url": "http://arxiv.org/abs/2106.00559v2"
    },
    "2008.08294v2": {
      "arxiv_id": "2008.08294v2",
      "title": "TNT: Target-driveN Trajectory Prediction",
      "authors": [
        "Hang Zhao",
        "Jiyang Gao",
        "Tian Lan",
        "Chen Sun",
        "Benjamin Sapp",
        "Balakrishnan Varadarajan",
        "Yue Shen",
        "Yi Shen",
        "Yuning Chai",
        "Cordelia Schmid",
        "Congcong Li",
        "Dragomir Anguelov"
      ],
      "summary": "Predicting the future behavior of moving agents is essential for real world applications. It is challenging as the intent of the agent and the corresponding behavior is unknown and intrinsically multimodal. Our key insight is that for prediction within a moderate time horizon, the future modes can be effectively captured by a set of target states. This leads to our target-driven trajectory prediction (TNT) framework. TNT has three stages which are trained end-to-end. It first predicts an agent's potential target states $T$ steps into the future, by encoding its interactions with the environment and the other agents. TNT then generates trajectory state sequences conditioned on targets. A final stage estimates trajectory likelihoods and a final compact set of trajectory predictions is selected. This is in contrast to previous work which models agent intents as latent variables, and relies on test-time sampling to generate diverse trajectories. We benchmark TNT on trajectory prediction of vehicles and pedestrians, where we outperform state-of-the-art on Argoverse Forecasting, INTERACTION, Stanford Drone and an in-house Pedestrian-at-Intersection dataset.",
      "published_utc": "2020-08-19T06:52:46Z",
      "updated_utc": "2020-08-21T07:33:10Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2008.08294v2",
      "abs_url": "http://arxiv.org/abs/2008.08294v2"
    },
    "2003.09996v1": {
      "arxiv_id": "2003.09996v1",
      "title": "Analysis and Prediction of Pedestrian Crosswalk Behavior during Automated Vehicle Interactions",
      "authors": [
        "Suresh Kumaar Jayaraman",
        "Dawn M. Tilbury",
        "X. Jessie Yang",
        "Anuj K. Pradhan",
        "Lionel P. Robert"
      ],
      "summary": "For safe navigation around pedestrians, automated vehicles (AVs) need to plan their motion by accurately predicting pedestrians trajectories over long time horizons. Current approaches to AV motion planning around crosswalks predict only for short time horizons (1-2 s) and are based on data from pedestrian interactions with human-driven vehicles (HDVs). In this paper, we develop a hybrid systems model that uses pedestrians gap acceptance behavior and constant velocity dynamics for long-term pedestrian trajectory prediction when interacting with AVs. Results demonstrate the applicability of the model for long-term (> 5 s) pedestrian trajectory prediction at crosswalks. Further we compared measures of pedestrian crossing behaviors in the immersive virtual environment (when interacting with AVs) to that in the real world (results of published studies of pedestrians interacting with HDVs), and found similarities between the two. These similarities demonstrate the applicability of the hybrid model of AV interactions developed from an immersive virtual environment (IVE) for real-world scenarios for both AVs and HDVs.",
      "published_utc": "2020-03-22T21:28:39Z",
      "updated_utc": "2020-03-22T21:28:39Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CY",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2003.09996v1",
      "abs_url": "http://arxiv.org/abs/2003.09996v1"
    },
    "1911.09476v1": {
      "arxiv_id": "1911.09476v1",
      "title": "Incremental Learning of Motion Primitives for Pedestrian Trajectory Prediction at Intersections",
      "authors": [
        "Golnaz Habibi",
        "Nikita Japuria",
        "Jonathan P. How"
      ],
      "summary": "This paper presents a novel incremental learning algorithm for pedestrian motion prediction, with the ability to improve the learned model over time when data is incrementally available. In this setup, trajectories are modeled as simple segments called motion primitives. Transitions between motion primitives are modeled as Gaussian Processes. When new data is available, the motion primitives learned from the new data are compared with the previous ones by measuring the inner product of the motion primitive vectors. Similar motion primitives and transitions are fused and novel motion primitives are added to capture newly observed behaviors. The proposed approach is tested and compared with other baselines in intersection scenarios where the data is incrementally available either from a single intersection or from multiple intersections with different geometries. In both cases, our method incrementally learns motion patterns and outperforms the offline learning approach in terms of prediction errors. The results also show that the model size in our algorithm grows at a much lower rate than standard incremental learning, where newly learned motion primitives and transitions are simply accumulated over time.",
      "published_utc": "2019-11-21T14:06:18Z",
      "updated_utc": "2019-11-21T14:06:18Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/1911.09476v1",
      "abs_url": "http://arxiv.org/abs/1911.09476v1"
    },
    "1910.03088v1": {
      "arxiv_id": "1910.03088v1",
      "title": "INTERACTION Dataset: An INTERnational, Adversarial and Cooperative moTION Dataset in Interactive Driving Scenarios with Semantic Maps",
      "authors": [
        "Wei Zhan",
        "Liting Sun",
        "Di Wang",
        "Haojie Shi",
        "Aubrey Clausse",
        "Maximilian Naumann",
        "Julius Kummerle",
        "Hendrik Konigshof",
        "Christoph Stiller",
        "Arnaud de La Fortelle",
        "Masayoshi Tomizuka"
      ],
      "summary": "Behavior-related research areas such as motion prediction/planning, representation/imitation learning, behavior modeling/generation, and algorithm testing, require support from high-quality motion datasets containing interactive driving scenarios with different driving cultures. In this paper, we present an INTERnational, Adversarial and Cooperative moTION dataset (INTERACTION dataset) in interactive driving scenarios with semantic maps. Five features of the dataset are highlighted. 1) The interactive driving scenarios are diverse, including urban/highway/ramp merging and lane changes, roundabouts with yield/stop signs, signalized intersections, intersections with one/two/all-way stops, etc. 2) Motion data from different countries and different continents are collected so that driving preferences and styles in different cultures are naturally included. 3) The driving behavior is highly interactive and complex with adversarial and cooperative motions of various traffic participants. Highly complex behavior such as negotiations, aggressive/irrational decisions and traffic rule violations are densely contained in the dataset, while regular behavior can also be found from cautious car-following, stop, left/right/U-turn to rational lane-change and cycling and pedestrian crossing, etc. 4) The levels of criticality span wide, from regular safe operations to dangerous, near-collision maneuvers. Real collision, although relatively slight, is also included. 5) Maps with complete semantic information are provided with physical layers, reference lines, lanelet connections and traffic rules. The data is recorded from drones and traffic cameras. Statistics of the dataset in terms of number of entities and interaction density are also provided, along with some utilization examples in a variety of behavior-related research areas. The dataset can be downloaded via https://interaction-dataset.com.",
      "published_utc": "2019-09-30T17:26:51Z",
      "updated_utc": "2019-09-30T17:26:51Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.LG",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/1910.03088v1",
      "abs_url": "http://arxiv.org/abs/1910.03088v1"
    },
    "1906.00486v4": {
      "arxiv_id": "1906.00486v4",
      "title": "Impact of Traffic Lights on Trajectory Forecasting of Human-driven Vehicles Near Signalized Intersections",
      "authors": [
        "Geunseob Oh",
        "Huei Peng"
      ],
      "summary": "Forecasting trajectories of human-driven vehicles is a crucial problem in autonomous driving. Trajectory forecasting in the urban area is particularly hard due to complex interactions with cars and pedestrians, and traffic lights (TLs). Unlike the former that has been widely studied, the impact of TLs on the trajectory prediction has been rarely discussed. In this work, we first identify the less studied, perhaps overlooked impact of TLs. Second, we present a novel resolution that is mindful of the impact, inspired by the fact that human drives differently depending on signal phase (green, yellow, red) and timing (elapsed time). Central to the proposed approach is Human Policy Models which model how drivers react to various states of TLs by mapping a sequence of states of vehicles and TLs to a subsequent action (acceleration) of the vehicle. We then combine the Human Policy Models with a known transition function (system dynamics) to conduct a sequential prediction; thus our approach is viewed as Behavior Cloning. One novelty of our approach is the use of vehicle-to-infrastructure communications to obtain the future states of TLs. We demonstrate the impact of TL and the proposed approach using an ablation study for longitudinal trajectory forecasting tasks on real-world driving data recorded near a signalized intersection. Finally, we propose probabilistic (generative) Human Policy Models which provide probabilistic contexts and capture competing policies, e.g., pass or stop in the yellow-light dilemma zone.",
      "published_utc": "2019-06-02T21:06:25Z",
      "updated_utc": "2020-04-26T23:39:54Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/1906.00486v4",
      "abs_url": "http://arxiv.org/abs/1906.00486v4"
    },
    "1806.09453v1": {
      "arxiv_id": "1806.09453v1",
      "title": "Context-Aware Pedestrian Motion Prediction In Urban Intersections",
      "authors": [
        "Golnaz Habibi",
        "Nikita Jaipuria",
        "Jonathan P. How"
      ],
      "summary": "This paper presents a novel context-based approach for pedestrian motion prediction in crowded, urban intersections, with the additional flexibility of prediction in similar, but new, environments. Previously, Chen et. al. combined Markovian-based and clustering-based approaches to learn motion primitives in a grid-based world and subsequently predict pedestrian trajectories by modeling the transition between learned primitives as a Gaussian Process (GP). This work extends that prior approach by incorporating semantic features from the environment (relative distance to curbside and status of pedestrian traffic lights) in the GP formulation for more accurate predictions of pedestrian trajectories over the same timescale. We evaluate the new approach on real-world data collected using one of the vehicles in the MIT Mobility On Demand fleet. The results show 12.5% improvement in prediction accuracy and a 2.65 times reduction in Area Under the Curve (AUC), which is used as a metric to quantify the span of predicted set of trajectories, such that a lower AUC corresponds to a higher level of confidence in the future direction of pedestrian motion.",
      "published_utc": "2018-06-25T13:45:57Z",
      "updated_utc": "2018-06-25T13:45:57Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.RO",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/1806.09453v1",
      "abs_url": "http://arxiv.org/abs/1806.09453v1"
    },
    "1806.09444v1": {
      "arxiv_id": "1806.09444v1",
      "title": "A Transferable Pedestrian Motion Prediction Model for Intersections with Different Geometries",
      "authors": [
        "Nikita Jaipuria",
        "Golnaz Habibi",
        "Jonathan P. How"
      ],
      "summary": "This paper presents a novel framework for accurate pedestrian intent prediction at intersections. Given some prior knowledge of the curbside geometry, the presented framework can accurately predict pedestrian trajectories, even in new intersections that it has not been trained on. This is achieved by making use of the contravariant components of trajectories in the curbside coordinate system, which ensures that the transformation of trajectories across intersections is affine, regardless of the curbside geometry. Our method is based on the Augmented Semi Nonnegative Sparse Coding (ASNSC) formulation and we use that as a baseline to show improvement in prediction performance on real pedestrian datasets collected at two intersections in Cambridge, with distinctly different curbside and crosswalk geometries. We demonstrate a 7.2% improvement in prediction accuracy in the case of same train and test intersections. Furthermore, we show a comparable prediction performance of TASNSC when trained and tested in different intersections with the baseline, trained and tested on the same intersection.",
      "published_utc": "2018-06-25T13:19:45Z",
      "updated_utc": "2018-06-25T13:19:45Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/1806.09444v1",
      "abs_url": "http://arxiv.org/abs/1806.09444v1"
    },
    "1804.00495v2": {
      "arxiv_id": "1804.00495v2",
      "title": "Transferable Pedestrian Motion Prediction Models at Intersections",
      "authors": [
        "Macheng Shen",
        "Golnaz Habibi",
        "Jonathan P. How"
      ],
      "summary": "One desirable capability of autonomous cars is to accurately predict the pedestrian motion near intersections for safe and efficient trajectory planning. We are interested in developing transfer learning algorithms that can be trained on the pedestrian trajectories collected at one intersection and yet still provide accurate predictions of the trajectories at another, previously unseen intersection. We first discussed the feature selection for transferable pedestrian motion models in general. Following this discussion, we developed one transferable pedestrian motion prediction algorithm based on Inverse Reinforcement Learning (IRL) that infers pedestrian intentions and predicts future trajectories based on observed trajectory. We evaluated our algorithm on a dataset collected at two intersections, trained at one intersection and tested at the other intersection. We used the accuracy of augmented semi-nonnegative sparse coding (ASNSC), trained and tested at the same intersection as a baseline. The result shows that the proposed algorithm improves the baseline accuracy by 40% in the non-transfer task, and 16% in the transfer task.",
      "published_utc": "2018-03-15T23:58:19Z",
      "updated_utc": "2019-09-18T23:51:54Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/1804.00495v2",
      "abs_url": "http://arxiv.org/abs/1804.00495v2"
    },
    "1803.03577v1": {
      "arxiv_id": "1803.03577v1",
      "title": "Intentions of Vulnerable Road Users - Detection and Forecasting by Means of Machine Learning",
      "authors": [
        "Michael Goldhammer",
        "Sebastian Köhler",
        "Stefan Zernetsch",
        "Konrad Doll",
        "Bernhard Sick",
        "Klaus Dietmayer"
      ],
      "summary": "Avoiding collisions with vulnerable road users (VRUs) using sensor-based early recognition of critical situations is one of the manifold opportunities provided by the current development in the field of intelligent vehicles. As especially pedestrians and cyclists are very agile and have a variety of movement options, modeling their behavior in traffic scenes is a challenging task. In this article we propose movement models based on machine learning methods, in particular artificial neural networks, in order to classify the current motion state and to predict the future trajectory of VRUs. Both model types are also combined to enable the application of specifically trained motion predictors based on a continuously updated pseudo probabilistic state classification. Furthermore, the architecture is used to evaluate motion-specific physical models for starting and stopping and video-based pedestrian motion classification. A comprehensive dataset consisting of 1068 pedestrian and 494 cyclist scenes acquired at an urban intersection is used for optimization, training, and evaluation of the different models. The results show substantial higher classification rates and the ability to earlier recognize motion state changes with the machine learning approaches compared to interacting multiple model (IMM) Kalman Filtering. The trajectory prediction quality is also improved for all kinds of test scenes, especially when starting and stopping motions are included. Here, 37\\% and 41\\% lower position errors were achieved on average, respectively.",
      "published_utc": "2018-03-09T15:49:07Z",
      "updated_utc": "2018-03-09T15:49:07Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/1803.03577v1",
      "abs_url": "http://arxiv.org/abs/1803.03577v1"
    },
    "2512.02777v1": {
      "arxiv_id": "2512.02777v1",
      "title": "CogDrive: Cognition-Driven Multimodal Prediction-Planning Fusion for Safe Autonomy",
      "authors": [
        "Heye Huang",
        "Yibin Yang",
        "Mingfeng Fan",
        "Haoran Wang",
        "Xiaocong Zhao",
        "Jianqiang Wang"
      ],
      "summary": "Safe autonomous driving in mixed traffic requires a unified understanding of multimodal interactions and dynamic planning under uncertainty. Existing learning based approaches struggle to capture rare but safety critical behaviors, while rule based systems often lack adaptability in complex interactions. To address these limitations, CogDrive introduces a cognition driven multimodal prediction and planning framework that integrates explicit modal reasoning with safety aware trajectory optimization. The prediction module adopts cognitive representations of interaction modes based on topological motion semantics and nearest neighbor relational encoding. With a differentiable modal loss and multimodal Gaussian decoding, CogDrive learns sparse and unbalanced interaction behaviors and improves long horizon trajectory prediction. The planning module incorporates an emergency response concept and optimizes safety stabilized trajectories, where short term consistent branches ensure safety during replanning cycles and long term branches support smooth and collision free motion under low probability switching modes. Experiments on Argoverse2 and INTERACTION datasets show that CogDrive achieves strong performance in trajectory accuracy and miss rate, while closed loop simulations confirm adaptive behavior in merge and intersection scenarios. By combining cognitive multimodal prediction with safety oriented planning, CogDrive offers an interpretable and reliable paradigm for safe autonomy in complex traffic.",
      "published_utc": "2025-12-02T13:53:18Z",
      "updated_utc": "2025-12-02T13:53:18Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.MA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2512.02777v1",
      "abs_url": "http://arxiv.org/abs/2512.02777v1"
    },
    "2506.12474v1": {
      "arxiv_id": "2506.12474v1",
      "title": "Generalizable Trajectory Prediction via Inverse Reinforcement Learning with Mamba-Graph Architecture",
      "authors": [
        "Wenyun Li",
        "Wenjie Huang",
        "Zejian Deng",
        "Chen Sun"
      ],
      "summary": "Accurate driving behavior modeling is fundamental to safe and efficient trajectory prediction, yet remains challenging in complex traffic scenarios. This paper presents a novel Inverse Reinforcement Learning (IRL) framework that captures human-like decision-making by inferring diverse reward functions, enabling robust cross-scenario adaptability. The learned reward function is utilized to maximize the likelihood of output by the encoder-decoder architecture that combines Mamba blocks for efficient long-sequence dependency modeling with graph attention networks to encode spatial interactions among traffic agents. Comprehensive evaluations on urban intersections and roundabouts demonstrate that the proposed method not only outperforms various popular approaches in prediction accuracy but also achieves 2 times higher generalization performance to unseen scenarios compared to other IRL-based method.",
      "published_utc": "2025-06-14T12:18:19Z",
      "updated_utc": "2025-06-14T12:18:19Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.12474v1",
      "abs_url": "http://arxiv.org/abs/2506.12474v1"
    },
    "2504.15541v1": {
      "arxiv_id": "2504.15541v1",
      "title": "RiskNet: Interaction-Aware Risk Forecasting for Autonomous Driving in Long-Tail Scenarios",
      "authors": [
        "Qichao Liu",
        "Heye Huang",
        "Shiyue Zhao",
        "Lei Shi",
        "Soyoung Ahn",
        "Xiaopeng Li"
      ],
      "summary": "Ensuring the safety of autonomous vehicles (AVs) in long-tail scenarios remains a critical challenge, particularly under high uncertainty and complex multi-agent interactions. To address this, we propose RiskNet, an interaction-aware risk forecasting framework, which integrates deterministic risk modeling with probabilistic behavior prediction for comprehensive risk assessment. At its core, RiskNet employs a field-theoretic model that captures interactions among ego vehicle, surrounding agents, and infrastructure via interaction fields and force. This model supports multidimensional risk evaluation across diverse scenarios (highways, intersections, and roundabouts), and shows robustness under high-risk and long-tail settings. To capture the behavioral uncertainty, we incorporate a graph neural network (GNN)-based trajectory prediction module, which learns multi-modal future motion distributions. Coupled with the deterministic risk field, it enables dynamic, probabilistic risk inference across time, enabling proactive safety assessment under uncertainty. Evaluations on the highD, inD, and rounD datasets, spanning lane changes, turns, and complex merges, demonstrate that our method significantly outperforms traditional approaches (e.g., TTC, THW, RSS, NC Field) in terms of accuracy, responsiveness, and directional sensitivity, while maintaining strong generalization across scenarios. This framework supports real-time, scenario-adaptive risk forecasting and demonstrates strong generalization across uncertain driving environments. It offers a unified foundation for safety-critical decision-making in long-tail scenarios.",
      "published_utc": "2025-04-22T02:36:54Z",
      "updated_utc": "2025-04-22T02:36:54Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.15541v1",
      "abs_url": "http://arxiv.org/abs/2504.15541v1"
    },
    "2504.13111v3": {
      "arxiv_id": "2504.13111v3",
      "title": "Uncertainty-Aware Trajectory Prediction via Rule-Regularized Heteroscedastic Deep Classification",
      "authors": [
        "Kumar Manas",
        "Christian Schlauch",
        "Adrian Paschke",
        "Christian Wirth",
        "Nadja Klein"
      ],
      "summary": "Deep learning-based trajectory prediction models have demonstrated promising capabilities in capturing complex interactions. However, their out-of-distribution generalization remains a significant challenge, particularly due to unbalanced data and a lack of enough data and diversity to ensure robustness and calibration. To address this, we propose SHIFT (Spectral Heteroscedastic Informed Forecasting for Trajectories), a novel framework that uniquely combines well-calibrated uncertainty modeling with informative priors derived through automated rule extraction. SHIFT reformulates trajectory prediction as a classification task and employs heteroscedastic spectral-normalized Gaussian processes to effectively disentangle epistemic and aleatoric uncertainties. We learn informative priors from training labels, which are automatically generated from natural language driving rules, such as stop rules and drivability constraints, using a retrieval-augmented generation framework powered by a large language model. Extensive evaluations over the nuScenes dataset, including challenging low-data and cross-location scenarios, demonstrate that SHIFT outperforms state-of-the-art methods, achieving substantial gains in uncertainty calibration and displacement metrics. In particular, our model excels in complex scenarios, such as intersections, where uncertainty is inherently higher. Project page: https://kumarmanas.github.io/SHIFT/.",
      "published_utc": "2025-04-17T17:24:50Z",
      "updated_utc": "2025-08-28T12:33:23Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.13111v3",
      "abs_url": "http://arxiv.org/abs/2504.13111v3"
    },
    "2501.13461v2": {
      "arxiv_id": "2501.13461v2",
      "title": "Knowledge-Informed Multi-Agent Trajectory Prediction at Signalized Intersections for Infrastructure-to-Everything",
      "authors": [
        "Huilin Yin",
        "Yangwenhui Xu",
        "Jiaxiang Li",
        "Hao Zhang",
        "Gerhard Rigoll"
      ],
      "summary": "Multi-agent trajectory prediction at signalized intersections is crucial for developing efficient intelligent transportation systems and safe autonomous driving systems. Due to the complexity of intersection scenarios and the limitations of single-vehicle perception, the performance of vehicle-centric prediction methods has reached a plateau. In this paper, we introduce an Infrastructure-to-Everything (I2X) collaborative prediction scheme. In this scheme, roadside units (RSUs) independently forecast the future trajectories of all vehicles and transmit these predictions unidirectionally to subscribing vehicles. Building on this scheme, we propose I2XTraj, a dedicated infrastructure-based trajectory prediction model. I2XTraj leverages real-time traffic signal states, prior maneuver strategy knowledge, and multi-agent interactions to generate accurate, joint multi-modal trajectory prediction. First, a continuous signal-informed mechanism is proposed to adaptively process real-time traffic signals to guide trajectory proposal generation under varied intersection configurations. Second, a driving strategy awareness mechanism estimates the joint distribution of maneuver strategies by integrating spatial priors of intersection areas with dynamic vehicle states, enabling coverage of the full set of feasible maneuvers. Third, a spatial-temporal-mode attention network models multi-agent interactions to refine and adjust joint trajectory outputs.Finally, I2XTraj is evaluated on two real-world datasets of signalized intersections, the V2X-Seq and the SinD drone dataset. In both single-infrastructure and online collaborative scenarios, our model outperforms state-of-the-art methods by over 30\\% on V2X-Seq and 15\\% on SinD, demonstrating strong generalizability and robustness.",
      "published_utc": "2025-01-23T08:23:45Z",
      "updated_utc": "2025-05-17T09:18:02Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.MA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.13461v2",
      "abs_url": "http://arxiv.org/abs/2501.13461v2"
    },
    "2405.02145v1": {
      "arxiv_id": "2405.02145v1",
      "title": "Characterized Diffusion and Spatial-Temporal Interaction Network for Trajectory Prediction in Autonomous Driving",
      "authors": [
        "Haicheng Liao",
        "Xuelin Li",
        "Yongkang Li",
        "Hanlin Kong",
        "Chengyue Wang",
        "Bonan Wang",
        "Yanchen Guan",
        "KaHou Tam",
        "Zhenning Li",
        "Chengzhong Xu"
      ],
      "summary": "Trajectory prediction is a cornerstone in autonomous driving (AD), playing a critical role in enabling vehicles to navigate safely and efficiently in dynamic environments. To address this task, this paper presents a novel trajectory prediction model tailored for accuracy in the face of heterogeneous and uncertain traffic scenarios. At the heart of this model lies the Characterized Diffusion Module, an innovative module designed to simulate traffic scenarios with inherent uncertainty. This module enriches the predictive process by infusing it with detailed semantic information, thereby enhancing trajectory prediction accuracy. Complementing this, our Spatio-Temporal (ST) Interaction Module captures the nuanced effects of traffic scenarios on vehicle dynamics across both spatial and temporal dimensions with remarkable effectiveness. Demonstrated through exhaustive evaluations, our model sets a new standard in trajectory prediction, achieving state-of-the-art (SOTA) results on the Next Generation Simulation (NGSIM), Highway Drone (HighD), and Macao Connected Autonomous Driving (MoCAD) datasets across both short and extended temporal spans. This performance underscores the model's unparalleled adaptability and efficacy in navigating complex traffic scenarios, including highways, urban streets, and intersections.",
      "published_utc": "2024-05-03T14:51:50Z",
      "updated_utc": "2024-05-03T14:51:50Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.02145v1",
      "abs_url": "http://arxiv.org/abs/2405.02145v1"
    },
    "2404.11946v1": {
      "arxiv_id": "2404.11946v1",
      "title": "S4TP: Social-Suitable and Safety-Sensitive Trajectory Planning for Autonomous Vehicles",
      "authors": [
        "Xiao Wang",
        "Ke Tang",
        "Xingyuan Dai",
        "Jintao Xu",
        "Quancheng Du",
        "Rui Ai",
        "Yuxiao Wang",
        "Weihao Gu"
      ],
      "summary": "In public roads, autonomous vehicles (AVs) face the challenge of frequent interactions with human-driven vehicles (HDVs), which render uncertain driving behavior due to varying social characteristics among humans. To effectively assess the risks prevailing in the vicinity of AVs in social interactive traffic scenarios and achieve safe autonomous driving, this article proposes a social-suitable and safety-sensitive trajectory planning (S4TP) framework. Specifically, S4TP integrates the Social-Aware Trajectory Prediction (SATP) and Social-Aware Driving Risk Field (SADRF) modules. SATP utilizes Transformers to effectively encode the driving scene and incorporates an AV's planned trajectory during the prediction decoding process. SADRF assesses the expected surrounding risk degrees during AVs-HDVs interactions, each with different social characteristics, visualized as two-dimensional heat maps centered on the AV. SADRF models the driving intentions of the surrounding HDVs and predicts trajectories based on the representation of vehicular interactions. S4TP employs an optimization-based approach for motion planning, utilizing the predicted HDVs'trajectories as input. With the integration of SADRF, S4TP executes real-time online optimization of the planned trajectory of AV within lowrisk regions, thus improving the safety and the interpretability of the planned trajectory. We have conducted comprehensive tests of the proposed method using the SMARTS simulator. Experimental results in complex social scenarios, such as unprotected left turn intersections, merging, cruising, and overtaking, validate the superiority of our proposed S4TP in terms of safety and rationality. S4TP achieves a pass rate of 100% across all scenarios, surpassing the current state-of-the-art methods Fanta of 98.25% and Predictive-Decision of 94.75%.",
      "published_utc": "2024-04-18T06:58:02Z",
      "updated_utc": "2024-04-18T06:58:02Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.11946v1",
      "abs_url": "http://arxiv.org/abs/2404.11946v1"
    },
    "2404.11181v2": {
      "arxiv_id": "2404.11181v2",
      "title": "KI-GAN: Knowledge-Informed Generative Adversarial Networks for Enhanced Multi-Vehicle Trajectory Forecasting at Signalized Intersections",
      "authors": [
        "Chuheng Wei",
        "Guoyuan Wu",
        "Matthew J. Barth",
        "Amr Abdelraouf",
        "Rohit Gupta",
        "Kyungtae Han"
      ],
      "summary": "Reliable prediction of vehicle trajectories at signalized intersections is crucial to urban traffic management and autonomous driving systems. However, it presents unique challenges, due to the complex roadway layout at intersections, involvement of traffic signal controls, and interactions among different types of road users. To address these issues, we present in this paper a novel model called Knowledge-Informed Generative Adversarial Network (KI-GAN), which integrates both traffic signal information and multi-vehicle interactions to predict vehicle trajectories accurately. Additionally, we propose a specialized attention pooling method that accounts for vehicle orientation and proximity at intersections. Based on the SinD dataset, our KI-GAN model is able to achieve an Average Displacement Error (ADE) of 0.05 and a Final Displacement Error (FDE) of 0.12 for a 6-second observation and 6-second prediction cycle. When the prediction window is extended to 9 seconds, the ADE and FDE values are further reduced to 0.11 and 0.26, respectively. These results demonstrate the effectiveness of the proposed KI-GAN model in vehicle trajectory prediction under complex scenarios at signalized intersections, which represents a significant advancement in the target field.",
      "published_utc": "2024-04-17T08:53:59Z",
      "updated_utc": "2024-04-19T14:28:00Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.11181v2",
      "abs_url": "http://arxiv.org/abs/2404.11181v2"
    },
    "2312.05144v1": {
      "arxiv_id": "2312.05144v1",
      "title": "Kraken: enabling joint trajectory prediction by utilizing Mode Transformer and Greedy Mode Processing",
      "authors": [
        "Daniil S. Antonenko",
        "Stepan Konev",
        "Yuriy Biktairov",
        "Boris Yangel"
      ],
      "summary": "Accurate and reliable motion prediction is essential for safe urban autonomy. The most prominent motion prediction approaches are based on modeling the distribution of possible future trajectories of each actor in autonomous system's vicinity. These \"independent\" marginal predictions might be accurate enough to properly describe casual driving situations where the prediction target is not likely to interact with other actors. They are, however, inadequate for modeling interactive situations where the actors' future trajectories are likely to intersect. To mitigate this issue we propose Kraken -- a real-time trajectory prediction model capable of approximating pairwise interactions between the actors as well as producing accurate marginal predictions. Kraken relies on a simple Greedy Mode Processing technique allowing it to convert a factorized prediction for a pair of agents into a physically-plausible joint prediction. It also utilizes the Mode Transformer module to increase the diversity of predicted trajectories and make the joint prediction more informative. We evaluate Kraken on Waymo Motion Prediction challenge where it held the first place in the Interaction leaderboard and the second place in the Motion leaderboard in October 2021.",
      "published_utc": "2023-12-08T16:24:05Z",
      "updated_utc": "2023-12-08T16:24:05Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.05144v1",
      "abs_url": "http://arxiv.org/abs/2312.05144v1"
    },
    "2309.13893v3": {
      "arxiv_id": "2309.13893v3",
      "title": "Scene Informer: Anchor-based Occlusion Inference and Trajectory Prediction in Partially Observable Environments",
      "authors": [
        "Bernard Lange",
        "Jiachen Li",
        "Mykel J. Kochenderfer"
      ],
      "summary": "Navigating complex and dynamic environments requires autonomous vehicles (AVs) to reason about both visible and occluded regions. This involves predicting the future motion of observed agents, inferring occluded ones, and modeling their interactions based on vectorized scene representations of the partially observable environment. However, prior work on occlusion inference and trajectory prediction have developed in isolation, with the former based on simplified rasterized methods and the latter assuming full environment observability. We introduce the Scene Informer, a unified approach for predicting both observed agent trajectories and inferring occlusions in a partially observable setting. It uses a transformer to aggregate various input modalities and facilitate selective queries on occlusions that might intersect with the AV's planned path. The framework estimates occupancy probabilities and likely trajectories for occlusions, as well as forecast motion for observed agents. We explore common observability assumptions in both domains and their performance impact. Our approach outperforms existing methods in both occupancy prediction and trajectory prediction in partially observable setting on the Waymo Open Motion Dataset.",
      "published_utc": "2023-09-25T06:16:09Z",
      "updated_utc": "2024-03-09T00:40:08Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.13893v3",
      "abs_url": "http://arxiv.org/abs/2309.13893v3"
    },
    "2211.10226v1": {
      "arxiv_id": "2211.10226v1",
      "title": "Leveraging Multi-stream Information Fusion for Trajectory Prediction in Low-illumination Scenarios: A Multi-channel Graph Convolutional Approach",
      "authors": [
        "Hailong Gong",
        "Zirui Li",
        "Chao Lu",
        "Guodong Du",
        "Jianwei Gong"
      ],
      "summary": "Trajectory prediction is a fundamental problem and challenge for autonomous vehicles. Early works mainly focused on designing complicated architectures for deep-learning-based prediction models in normal-illumination environments, which fail in dealing with low-light conditions. This paper proposes a novel approach for trajectory prediction in low-illumination scenarios by leveraging multi-stream information fusion, which flexibly integrates image, optical flow, and object trajectory information. The image channel employs Convolutional Neural Network (CNN) and Long Short-term Memory (LSTM) networks to extract temporal information from the camera. The optical flow channel is applied to capture the pattern of relative motion between adjacent camera frames and modelled by Spatial-Temporal Graph Convolutional Network (ST-GCN). The trajectory channel is used to recognize high-level interactions between vehicles. Finally, information from all the three channels is effectively fused in the prediction module to generate future trajectories of surrounding vehicles in low-illumination conditions. The proposed multi-channel graph convolutional approach is validated on HEV-I and newly generated Dark-HEV-I, egocentric vision datasets that primarily focus on urban intersection scenarios. The results demonstrate that our method outperforms the baselines, in standard and low-illumination scenarios. Additionally, our approach is generic and applicable to scenarios with different types of perception data. The source code of the proposed approach is available at https://github.com/TommyGong08/MSIF}{https://github.com/TommyGong08/MSIF.",
      "published_utc": "2022-11-18T13:25:15Z",
      "updated_utc": "2022-11-18T13:25:15Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.10226v1",
      "abs_url": "http://arxiv.org/abs/2211.10226v1"
    },
    "2207.10398v1": {
      "arxiv_id": "2207.10398v1",
      "title": "D2-TPred: Discontinuous Dependency for Trajectory Prediction under Traffic Lights",
      "authors": [
        "Yuzhen Zhang",
        "Wentong Wang",
        "Weizhi Guo",
        "Pei Lv",
        "Mingliang Xu",
        "Wei Chen",
        "Dinesh Manocha"
      ],
      "summary": "A profound understanding of inter-agent relationships and motion behaviors is important to achieve high-quality planning when navigating in complex scenarios, especially at urban traffic intersections. We present a trajectory prediction approach with respect to traffic lights, D2-TPred, which uses a spatial dynamic interaction graph (SDG) and a behavior dependency graph (BDG) to handle the problem of discontinuous dependency in the spatial-temporal space. Specifically, the SDG is used to capture spatial interactions by reconstructing sub-graphs for different agents with dynamic and changeable characteristics during each frame. The BDG is used to infer motion tendency by modeling the implicit dependency of the current state on priors behaviors, especially the discontinuous motions corresponding to acceleration, deceleration, or turning direction. Moreover, we present a new dataset for vehicle trajectory prediction under traffic lights called VTP-TL. Our experimental results show that our model achieves more than {20.45% and 20.78% }improvement in terms of ADE and FDE, respectively, on VTP-TL as compared to other trajectory prediction algorithms. The dataset and code are available at: https://github.com/VTP-TL/D2-TPred.",
      "published_utc": "2022-07-21T10:19:07Z",
      "updated_utc": "2022-07-21T10:19:07Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.10398v1",
      "abs_url": "http://arxiv.org/abs/2207.10398v1"
    },
    "2202.05140v2": {
      "arxiv_id": "2202.05140v2",
      "title": "Transferable and Adaptable Driving Behavior Prediction",
      "authors": [
        "Letian Wang",
        "Yeping Hu",
        "Liting Sun",
        "Wei Zhan",
        "Masayoshi Tomizuka",
        "Changliu Liu"
      ],
      "summary": "While autonomous vehicles still struggle to solve challenging situations during on-road driving, humans have long mastered the essence of driving with efficient, transferable, and adaptable driving capability. By mimicking humans' cognition model and semantic understanding during driving, we propose HATN, a hierarchical framework to generate high-quality, transferable, and adaptable predictions for driving behaviors in multi-agent dense-traffic environments. Our hierarchical method consists of a high-level intention identification policy and a low-level trajectory generation policy. We introduce a novel semantic sub-task definition and generic state representation for each sub-task. With these techniques, the hierarchical framework is transferable across different driving scenarios. Besides, our model is able to capture variations of driving behaviors among individuals and scenarios by an online adaptation module. We demonstrate our algorithms in the task of trajectory prediction for real traffic data at intersections and roundabouts from the INTERACTION dataset. Through extensive numerical studies, it is evident that our method significantly outperformed other methods in terms of prediction accuracy, transferability, and adaptability. Pushing the state-of-the-art performance by a considerable margin, we also provide a cognitive view of understanding the driving behavior behind such improvement. We highlight that in the future, more research attention and effort are deserved for transferability and adaptability. It is not only due to the promising performance elevation of prediction and planning algorithms, but more fundamentally, they are crucial for the scalable and general deployment of autonomous vehicles.",
      "published_utc": "2022-02-10T16:46:24Z",
      "updated_utc": "2022-02-13T12:45:09Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2202.05140v2",
      "abs_url": "http://arxiv.org/abs/2202.05140v2"
    },
    "2111.00788v3": {
      "arxiv_id": "2111.00788v3",
      "title": "Hierarchical Adaptable and Transferable Networks (HATN) for Driving Behavior Prediction",
      "authors": [
        "Letian Wang",
        "Yeping Hu",
        "Liting Sun",
        "Wei Zhan",
        "Masayoshi Tomizuka",
        "Changliu Liu"
      ],
      "summary": "When autonomous vehicles still struggle to solve challenging situations during on-road driving, humans have long mastered the essence of driving with efficient transferable and adaptable driving capability. By mimicking humans' cognition model and semantic understanding during driving, we present HATN, a hierarchical framework to generate high-quality driving behaviors in multi-agent dense-traffic environments. Our method hierarchically consists of a high-level intention identification and low-level action generation policy. With the semantic sub-task definition and generic state representation, the hierarchical framework is transferable across different driving scenarios. Besides, our model is also able to capture variations of driving behaviors among individuals and scenarios by an online adaptation module. We demonstrate our algorithms in the task of trajectory prediction for real traffic data at intersections and roundabouts, where we conducted extensive studies of the proposed method and demonstrated how our method outperformed other methods in terms of prediction accuracy and transferability.",
      "published_utc": "2021-11-01T09:37:32Z",
      "updated_utc": "2021-12-11T17:05:17Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2111.00788v3",
      "abs_url": "http://arxiv.org/abs/2111.00788v3"
    },
    "2011.12406v2": {
      "arxiv_id": "2011.12406v2",
      "title": "Prediction-Based Reachability for Collision Avoidance in Autonomous Driving",
      "authors": [
        "Anjian Li",
        "Liting Sun",
        "Wei Zhan",
        "Masayoshi Tomizuka",
        "Mo Chen"
      ],
      "summary": "Safety is an important topic in autonomous driving since any collision may cause serious injury to people and damage to property. Hamilton-Jacobi (HJ) Reachability is a formal method that verifies safety in multi-agent interaction and provides a safety controller for collision avoidance. However, due to the worst-case assumption on the cars future behaviours, reachability might result in too much conservatism such that the normal operation of the vehicle is badly hindered. In this paper, we leverage the power of trajectory prediction and propose a prediction-based reachability framework to compute safety controllers. Instead of always assuming the worst case, we cluster the car's behaviors into multiple driving modes, e.g. left turn or right turn. Under each mode, a reachability-based safety controller is designed based on a less conservative action set. For online implementation, we first utilize the trajectory prediction and our proposed mode classifier to predict the possible modes, and then deploy the corresponding safety controller. Through simulations in a T-intersection and an 8-way roundabout, we demonstrate that our prediction-based reachability method largely avoids collision between two interacting cars and reduces the conservatism that the safety controller brings to the car's original operation.",
      "published_utc": "2020-11-24T21:33:01Z",
      "updated_utc": "2021-05-21T07:53:51Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2011.12406v2",
      "abs_url": "http://arxiv.org/abs/2011.12406v2"
    },
    "2010.16267v3": {
      "arxiv_id": "2010.16267v3",
      "title": "Exploring Dynamic Context for Multi-path Trajectory Prediction",
      "authors": [
        "Hao Cheng",
        "Wentong Liao",
        "Xuejiao Tang",
        "Michael Ying Yang",
        "Monika Sester",
        "Bodo Rosenhahn"
      ],
      "summary": "To accurately predict future positions of different agents in traffic scenarios is crucial for safely deploying intelligent autonomous systems in the real-world environment. However, it remains a challenge due to the behavior of a target agent being affected by other agents dynamically and there being more than one socially possible paths the agent could take. In this paper, we propose a novel framework, named Dynamic Context Encoder Network (DCENet). In our framework, first, the spatial context between agents is explored by using self-attention architectures. Then, the two-stream encoders are trained to learn temporal context between steps by taking the respective observed trajectories and the extracted dynamic spatial context as input. The spatial-temporal context is encoded into a latent space using a Conditional Variational Auto-Encoder (CVAE) module. Finally, a set of future trajectories for each agent is predicted conditioned on the learned spatial-temporal context by sampling from the latent space, repeatedly. DCENet is evaluated on one of the most popular challenging benchmarks for trajectory forecasting Trajnet and reports a new state-of-the-art performance. It also demonstrates superior performance evaluated on the benchmark inD for mixed traffic at intersections. A series of ablation studies is conducted to validate the effectiveness of each proposed module. Our code is available at https://github.com/wtliao/DCENet.",
      "published_utc": "2020-10-30T13:39:20Z",
      "updated_utc": "2021-03-24T10:28:47Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.MA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2010.16267v3",
      "abs_url": "http://arxiv.org/abs/2010.16267v3"
    },
    "2005.08307v2": {
      "arxiv_id": "2005.08307v2",
      "title": "AC-VRNN: Attentive Conditional-VRNN for Multi-Future Trajectory Prediction",
      "authors": [
        "Alessia Bertugli",
        "Simone Calderara",
        "Pasquale Coscia",
        "Lamberto Ballan",
        "Rita Cucchiara"
      ],
      "summary": "Anticipating human motion in crowded scenarios is essential for developing intelligent transportation systems, social-aware robots and advanced video surveillance applications. A key component of this task is represented by the inherently multi-modal nature of human paths which makes socially acceptable multiple futures when human interactions are involved. To this end, we propose a generative architecture for multi-future trajectory predictions based on Conditional Variational Recurrent Neural Networks (C-VRNNs). Conditioning mainly relies on prior belief maps, representing most likely moving directions and forcing the model to consider past observed dynamics in generating future positions. Human interactions are modeled with a graph-based attention mechanism enabling an online attentive hidden state refinement of the recurrent estimation. To corroborate our model, we perform extensive experiments on publicly-available datasets (e.g., ETH/UCY, Stanford Drone Dataset, STATS SportVU NBA, Intersection Drone Dataset and TrajNet++) and demonstrate its effectiveness in crowded scenes compared to several state-of-the-art methods.",
      "published_utc": "2020-05-17T17:21:23Z",
      "updated_utc": "2021-07-08T08:23:15Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2005.08307v2",
      "abs_url": "http://arxiv.org/abs/2005.08307v2"
    },
    "2002.01965v1": {
      "arxiv_id": "2002.01965v1",
      "title": "Learning Probabilistic Intersection Traffic Models for Trajectory Prediction",
      "authors": [
        "Andrew Patterson",
        "Aditya Gahlawat",
        "Naira Hovakimyan"
      ],
      "summary": "Autonomous agents must be able to safely interact with other vehicles to integrate into urban environments. The safety of these agents is dependent on their ability to predict collisions with other vehicles' future trajectories for replanning and collision avoidance. The information needed to predict collisions can be learned from previously observed vehicle trajectories in a specific environment, generating a traffic model. The learned traffic model can then be incorporated as prior knowledge into any trajectory estimation method being used in this environment. This work presents a Gaussian process based probabilistic traffic model that is used to quantify vehicle behaviors in an intersection. The Gaussian process model provides estimates for the average vehicle trajectory, while also capturing the variance between the different paths a vehicle may take in the intersection. The method is demonstrated on a set of time-series position trajectories. These trajectories are reconstructed by removing object recognition errors and missed frames that may occur due to data source processing. To create the intersection traffic model, the reconstructed trajectories are clustered based on their source and destination lanes. For each cluster, a Gaussian process model is created to capture the average behavior and the variance of the cluster. To show the applicability of the Gaussian model, the test trajectories are classified with only partial observations. Performance is quantified by the number of observations required to correctly classify the vehicle trajectory. Both the intersection traffic modeling computations and the classification procedure are timed. These times are presented as results and demonstrate that the model can be constructed in a reasonable amount of time and the classification procedure can be used for online applications.",
      "published_utc": "2020-02-05T19:22:26Z",
      "updated_utc": "2020-02-05T19:22:26Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2002.01965v1",
      "abs_url": "http://arxiv.org/abs/2002.01965v1"
    },
    "1911.03801v1": {
      "arxiv_id": "1911.03801v1",
      "title": "Human Driver Behavior Prediction based on UrbanFlow",
      "authors": [
        "Zhiqian Qiao",
        "Jing Zhao",
        "Zachariah Tyree",
        "Priyantha Mudalige",
        "Jeff Schneider",
        "John M. Dolan"
      ],
      "summary": "How autonomous vehicles and human drivers share public transportation systems is an important problem, as fully automatic transportation environments are still a long way off. Understanding human drivers' behavior can be beneficial for autonomous vehicle decision making and planning, especially when the autonomous vehicle is surrounded by human drivers who have various driving behaviors and patterns of interaction with other vehicles. In this paper, we propose an LSTM-based trajectory prediction method for human drivers which can help the autonomous vehicle make better decisions, especially in urban intersection scenarios. Meanwhile, in order to collect human drivers' driving behavior data in the urban scenario, we describe a system called UrbanFlow which includes the whole procedure from raw bird's-eye view data collection via drone to the final processed trajectories. The system is mainly intended for urban scenarios but can be extended to be used for any traffic scenarios.",
      "published_utc": "2019-11-09T23:25:41Z",
      "updated_utc": "2019-11-09T23:25:41Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/1911.03801v1",
      "abs_url": "http://arxiv.org/abs/1911.03801v1"
    },
    "1909.00792v1": {
      "arxiv_id": "1909.00792v1",
      "title": "Conditional Vehicle Trajectories Prediction in CARLA Urban Environment",
      "authors": [
        "Thibault Buhet",
        "Emilie Wirbel",
        "Xavier Perrotton"
      ],
      "summary": "Imitation learning is becoming more and more successful for autonomous driving. End-to-end (raw signal to command) performs well on relatively simple tasks (lane keeping and navigation). Mid-to-mid (environment abstraction to mid-level trajectory representation) or direct perception (raw signal to performance) approaches strive to handle more complex, real life environment and tasks (e.g. complex intersection). In this work, we show that complex urban situations can be handled with raw signal input and mid-level representation. We build a hybrid end-to-mid approach predicting trajectories for neighbor vehicles and for the ego vehicle with a conditional navigation goal. We propose an original architecture inspired from social pooling LSTM taking low and mid level data as input and producing trajectories as polynomials of time. We introduce a label augmentation mechanism to get the level of generalization that is required to control a vehicle. The performance is evaluated on CARLA 0.8 benchmark, showing significant improvements over previously published state of the art.",
      "published_utc": "2019-09-02T16:41:24Z",
      "updated_utc": "2019-09-02T16:41:24Z",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/1909.00792v1",
      "abs_url": "http://arxiv.org/abs/1909.00792v1"
    },
    "1808.06887v5": {
      "arxiv_id": "1808.06887v5",
      "title": "Multimodal Interaction-aware Motion Prediction for Autonomous Street Crossing",
      "authors": [
        "Noha Radwan",
        "Wolfram Burgard",
        "Abhinav Valada"
      ],
      "summary": "For mobile robots navigating on sidewalks, it is essential to be able to safely cross street intersections. Most existing approaches rely on the recognition of the traffic light signal to make an informed crossing decision. Although these approaches have been crucial enablers for urban navigation, the capabilities of robots employing such approaches are still limited to navigating only on streets containing signalized intersections. In this paper, we address this challenge and propose a multimodal convolutional neural network framework to predict the safety of a street intersection for crossing. Our architecture consists of two subnetworks; an interaction-aware trajectory estimation stream IA-TCNN, that predicts the future states of all observed traffic participants in the scene, and a traffic light recognition stream AtteNet. Our IA-TCNN utilizes dilated causal convolutions to model the behavior of the observable dynamic agents in the scene without explicitly assigning priorities to the interactions among them. While AtteNet utilizes Squeeze-Excitation blocks to learn a content-aware mechanism for selecting the relevant features from the data, thereby improving the noise robustness. Learned representations from the traffic light recognition stream are fused with the estimated trajectories from the motion prediction stream to learn the crossing decision. Furthermore, we extend our previously introduced Freiburg Street Crossing dataset with sequences captured at different types of intersections, demonstrating complex interactions among the traffic participants. Extensive experimental evaluations on public benchmark datasets and our proposed dataset demonstrate that our network achieves state-of-the-art performance for each of the subtasks, as well as for the crossing safety prediction.",
      "published_utc": "2018-08-21T13:18:27Z",
      "updated_utc": "2020-08-03T16:41:58Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/1808.06887v5",
      "abs_url": "http://arxiv.org/abs/1808.06887v5"
    },
    "1807.09995v1": {
      "arxiv_id": "1807.09995v1",
      "title": "Naturalistic Driver Intention and Path Prediction using Recurrent Neural Networks",
      "authors": [
        "Alex Zyner",
        "Stewart Worrall",
        "Eduardo Nebot"
      ],
      "summary": "Understanding the intentions of drivers at intersections is a critical component for autonomous vehicles. Urban intersections that do not have traffic signals are a common epicentre of highly variable vehicle movement and interactions. We present a method for predicting driver intent at urban intersections through multi-modal trajectory prediction with uncertainty. Our method is based on recurrent neural networks combined with a mixture density network output layer. To consolidate the multi-modal nature of the output probability distribution, we introduce a clustering algorithm that extracts the set of possible paths that exist in the prediction output, and ranks them according to likelihood. To verify the method's performance and generalizability, we present a real-world dataset that consists of over 23,000 vehicles traversing five different intersections, collected using a vehicle mounted Lidar based tracking system. An array of metrics is used to demonstrate the performance of the model against several baselines.",
      "published_utc": "2018-07-26T07:57:13Z",
      "updated_utc": "2018-07-26T07:57:13Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/1807.09995v1",
      "abs_url": "http://arxiv.org/abs/1807.09995v1"
    },
    "1705.02445v1": {
      "arxiv_id": "1705.02445v1",
      "title": "On human motion prediction using recurrent neural networks",
      "authors": [
        "Julieta Martinez",
        "Michael J. Black",
        "Javier Romero"
      ],
      "summary": "Human motion modelling is a classical problem at the intersection of graphics and computer vision, with applications spanning human-computer interaction, motion synthesis, and motion prediction for virtual and augmented reality. Following the success of deep learning methods in several computer vision tasks, recent work has focused on using deep recurrent neural networks (RNNs) to model human motion, with the goal of learning time-dependent representations that perform tasks such as short-term motion prediction and long-term human motion synthesis. We examine recent work, with a focus on the evaluation methodologies commonly used in the literature, and show that, surprisingly, state-of-the-art performance can be achieved by a simple baseline that does not attempt to model motion at all. We investigate this result, and analyze recent RNN methods by looking at the architectures, loss functions, and training procedures used in state-of-the-art approaches. We propose three changes to the standard RNN models typically used for human motion, which result in a simple and scalable RNN architecture that obtains state-of-the-art performance on human motion prediction.",
      "published_utc": "2017-05-06T05:08:05Z",
      "updated_utc": "2017-05-06T05:08:05Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/1705.02445v1",
      "abs_url": "http://arxiv.org/abs/1705.02445v1"
    },
    "2505.09935v1": {
      "arxiv_id": "2505.09935v1",
      "title": "VRU-CIPI: Crossing Intention Prediction at Intersections for Improving Vulnerable Road Users Safety",
      "authors": [
        "Ahmed S. Abdelrahman",
        "Mohamed Abdel-Aty",
        "Quoc Dai Tran"
      ],
      "summary": "Understanding and predicting human behavior in-thewild, particularly at urban intersections, remains crucial for enhancing interaction safety between road users. Among the most critical behaviors are crossing intentions of Vulnerable Road Users (VRUs), where misinterpretation may result in dangerous conflicts with oncoming vehicles. In this work, we propose the VRU-CIPI framework with a sequential attention-based model designed to predict VRU crossing intentions at intersections. VRU-CIPI employs Gated Recurrent Unit (GRU) to capture temporal dynamics in VRU movements, combined with a multi-head Transformer self-attention mechanism to encode contextual and spatial dependencies critical for predicting crossing direction. Evaluated on UCF-VRU dataset, our proposed achieves state-of-the-art performance with an accuracy of 96.45% and achieving real-time inference speed reaching 33 frames per second. Furthermore, by integrating with Infrastructure-to-Vehicles (I2V) communication, our approach can proactively enhance intersection safety through timely activation of crossing signals and providing early warnings to connected vehicles, ensuring smoother and safer interactions for all road users.",
      "published_utc": "2025-05-15T03:40:29Z",
      "updated_utc": "2025-05-15T03:40:29Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.09935v1",
      "abs_url": "http://arxiv.org/abs/2505.09935v1"
    },
    "2502.15824v1": {
      "arxiv_id": "2502.15824v1",
      "title": "Getting SMARTER for Motion Planning in Autonomous Driving Systems",
      "authors": [
        "Montgomery Alban",
        "Ehsan Ahmadi",
        "Randy Goebel",
        "Amir Rasouli"
      ],
      "summary": "Motion planning is a fundamental problem in autonomous driving and perhaps the most challenging to comprehensively evaluate because of the associated risks and expenses of real-world deployment. Therefore, simulations play an important role in efficient development of planning algorithms. To be effective, simulations must be accurate and realistic, both in terms of dynamics and behavior modeling, and also highly customizable in order to accommodate a broad spectrum of research frameworks. In this paper, we introduce SMARTS 2.0, the second generation of our motion planning simulator which, in addition to being highly optimized for large-scale simulation, provides many new features, such as realistic map integration, vehicle-to-vehicle (V2V) communication, traffic and pedestrian simulation, and a broad variety of sensor models. Moreover, we present a novel benchmark suite for evaluating planning algorithms in various highly challenging scenarios, including interactive driving, such as turning at intersections, and adaptive driving, in which the task is to closely follow a lead vehicle without any explicit knowledge of its intention. Each scenario is characterized by a variety of traffic patterns and road structures. We further propose a series of common and task-specific metrics to effectively evaluate the performance of the planning algorithms. At the end, we evaluate common motion planning algorithms using the proposed benchmark and highlight the challenges the proposed scenarios impose. The new SMARTS 2.0 features and the benchmark are publicly available at github.com/huawei-noah/SMARTS.",
      "published_utc": "2025-02-20T03:51:49Z",
      "updated_utc": "2025-02-20T03:51:49Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.15824v1",
      "abs_url": "http://arxiv.org/abs/2502.15824v1"
    },
    "2311.16091v1": {
      "arxiv_id": "2311.16091v1",
      "title": "Interactive Autonomous Navigation with Internal State Inference and Interactivity Estimation",
      "authors": [
        "Jiachen Li",
        "David Isele",
        "Kanghoon Lee",
        "Jinkyoo Park",
        "Kikuo Fujimura",
        "Mykel J. Kochenderfer"
      ],
      "summary": "Deep reinforcement learning (DRL) provides a promising way for intelligent agents (e.g., autonomous vehicles) to learn to navigate complex scenarios. However, DRL with neural networks as function approximators is typically considered a black box with little explainability and often suffers from suboptimal performance, especially for autonomous navigation in highly interactive multi-agent environments. To address these issues, we propose three auxiliary tasks with spatio-temporal relational reasoning and integrate them into the standard DRL framework, which improves the decision making performance and provides explainable intermediate indicators. We propose to explicitly infer the internal states (i.e., traits and intentions) of surrounding agents (e.g., human drivers) as well as to predict their future trajectories in the situations with and without the ego agent through counterfactual reasoning. These auxiliary tasks provide additional supervision signals to infer the behavior patterns of other interactive agents. Multiple variants of framework integration strategies are compared. We also employ a spatio-temporal graph neural network to encode relations between dynamic entities, which enhances both internal state inference and decision making of the ego agent. Moreover, we propose an interactivity estimation mechanism based on the difference between predicted trajectories in these two situations, which indicates the degree of influence of the ego agent on other agents. To validate the proposed method, we design an intersection driving simulator based on the Intelligent Intersection Driver Model (IIDM) that simulates vehicles and pedestrians. Our approach achieves robust and state-of-the-art performance in terms of standard evaluation metrics and provides explainable intermediate indicators (i.e., internal states, and interactivity scores) for decision making.",
      "published_utc": "2023-11-27T18:57:42Z",
      "updated_utc": "2023-11-27T18:57:42Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.MA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.16091v1",
      "abs_url": "http://arxiv.org/abs/2311.16091v1"
    },
    "2201.04742v1": {
      "arxiv_id": "2201.04742v1",
      "title": "nuReality: A VR environment for research of pedestrian and autonomous vehicle interactions",
      "authors": [
        "Paul Schmitt",
        "Nicholas Britten",
        "JiHyun Jeong",
        "Amelia Coffey",
        "Kevin Clark",
        "Shweta Sunil Kothawade",
        "Elena Corina Grigore",
        "Adam Khaw",
        "Christopher Konopka",
        "Linh Pham",
        "Kim Ryan",
        "Christopher Schmitt",
        "Aryaman Pandya",
        "Emilio Frazzoli"
      ],
      "summary": "We present nuReality, a virtual reality 'VR' environment designed to test the efficacy of vehicular behaviors to communicate intent during interactions between autonomous vehicles 'AVs' and pedestrians at urban intersections. In this project we focus on expressive behaviors as a means for pedestrians to readily recognize the underlying intent of the AV's movements. VR is an ideal tool to use to test these situations as it can be immersive and place subjects into these potentially dangerous scenarios without risk. nuReality provides a novel and immersive virtual reality environment that includes numerous visual details (road and building texturing, parked cars, swaying tree limbs) as well as auditory details (birds chirping, cars honking in the distance, people talking). In these files we present the nuReality environment, its 10 unique vehicle behavior scenarios, and the Unreal Engine and Autodesk Maya source files for each scenario. The files are publicly released as open source at www.nuReality.org, to support the academic community studying the critical AV-pedestrian interaction.",
      "published_utc": "2022-01-12T23:54:09Z",
      "updated_utc": "2022-01-12T23:54:09Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2201.04742v1",
      "abs_url": "http://arxiv.org/abs/2201.04742v1"
    },
    "2010.05115v1": {
      "arxiv_id": "2010.05115v1",
      "title": "Autonomous Vehicle Visual Signals for Pedestrians: Experiments and Design Recommendations",
      "authors": [
        "Henry Chen",
        "Robin Cohen",
        "Kerstin Dautenhahn",
        "Edith Law",
        "Krzysztof Czarnecki"
      ],
      "summary": "Autonomous Vehicles (AV) will transform transportation, but also the interaction between vehicles and pedestrians. In the absence of a driver, it is not clear how an AV can communicate its intention to pedestrians. One option is to use visual signals. To advance their design, we conduct four human-participant experiments and evaluate six representative AV visual signals for visibility, intuitiveness, persuasiveness, and usability at pedestrian crossings. Based on the results, we distill twelve practical design recommendations for AV visual signals, with focus on signal pattern design and placement. Moreover, the paper advances the methodology for experimental evaluation of visual signals, including lab, closed-course, and public road tests using an autonomous vehicle. In addition, the paper also reports insights on pedestrian crosswalk behaviours and the impacts of pedestrian trust towards AVs on the behaviors. We hope that this work will constitute valuable input to the ongoing development of international standards for AV lamps, and thus help mature automated driving in general.",
      "published_utc": "2020-10-10T22:56:46Z",
      "updated_utc": "2020-10-10T22:56:46Z",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.MA",
        "cs.RO",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2010.05115v1",
      "abs_url": "http://arxiv.org/abs/2010.05115v1"
    },
    "2003.09998v1": {
      "arxiv_id": "2003.09998v1",
      "title": "Efficient Behavior-aware Control of Automated Vehicles at Crosswalks using Minimal Information Pedestrian Prediction Model",
      "authors": [
        "Suresh Kumaar Jayaraman",
        "Lionel P. Robert",
        "Xi Jessie Yang",
        "Anuj K. Pradhan",
        "Dawn M. Tilbury"
      ],
      "summary": "For automated vehicles (AVs) to reliably navigate through crosswalks, they need to understand pedestrians crossing behaviors. Simple and reliable pedestrian behavior models aid in real-time AV control by allowing the AVs to predict future pedestrian behaviors. In this paper, we present a Behavior aware Model Predictive Controller (B-MPC) for AVs that incorporates long-term predictions of pedestrian crossing behavior using a previously developed pedestrian crossing model. The model incorporates pedestrians gap acceptance behavior and utilizes minimal pedestrian information, namely their position and speed, to predict pedestrians crossing behaviors. The BMPC controller is validated through simulations and compared to a rule-based controller. By incorporating predictions of pedestrian behavior, the B-MPC controller is able to efficiently plan for longer horizons and handle a wider range of pedestrian interaction scenarios than the rule-based controller. Results demonstrate the applicability of the controller for safe and efficient navigation at crossing scenarios.",
      "published_utc": "2020-03-22T21:34:38Z",
      "updated_utc": "2020-03-22T21:34:38Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CY",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2003.09998v1",
      "abs_url": "http://arxiv.org/abs/2003.09998v1"
    },
    "1809.03705v3": {
      "arxiv_id": "1809.03705v3",
      "title": "Bio-LSTM: A Biomechanically Inspired Recurrent Neural Network for 3D Pedestrian Pose and Gait Prediction",
      "authors": [
        "Xiaoxiao Du",
        "Ram Vasudevan",
        "Matthew Johnson-Roberson"
      ],
      "summary": "In applications such as autonomous driving, it is important to understand, infer, and anticipate the intention and future behavior of pedestrians. This ability allows vehicles to avoid collisions and improve ride safety and quality. This paper proposes a biomechanically inspired recurrent neural network (Bio-LSTM) that can predict the location and 3D articulated body pose of pedestrians in a global coordinate frame, given 3D poses and locations estimated in prior frames with inaccuracy. The proposed network is able to predict poses and global locations for multiple pedestrians simultaneously, for pedestrians up to 45 meters from the cameras (urban intersection scale). The outputs of the proposed network are full-body 3D meshes represented in Skinned Multi-Person Linear (SMPL) model parameters. The proposed approach relies on a novel objective function that incorporates the periodicity of human walking (gait), the mirror symmetry of the human body, and the change of ground reaction forces in a human gait cycle. This paper presents prediction results on the PedX dataset, a large-scale, in-the-wild data set collected at real urban intersections with heavy pedestrian traffic. Results show that the proposed network can successfully learn the characteristics of pedestrian gait and produce accurate and consistent 3D pose predictions.",
      "published_utc": "2018-09-11T07:11:32Z",
      "updated_utc": "2019-09-13T15:28:14Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/1809.03705v3",
      "abs_url": "http://arxiv.org/abs/1809.03705v3"
    },
    "1803.02242v1": {
      "arxiv_id": "1803.02242v1",
      "title": "Early Start Intention Detection of Cyclists Using Motion History Images and a Deep Residual Network",
      "authors": [
        "Stefan Zernetsch",
        "Viktor Kress",
        "Bernhard Sick",
        "Konrad Doll"
      ],
      "summary": "In this article, we present a novel approach to detect starting motions of cyclists in real world traffic scenarios based on Motion History Images (MHIs). The method uses a deep Convolutional Neural Network (CNN) with a residual network architecture (ResNet), which is commonly used in image classification and detection tasks. By combining MHIs with a ResNet classifier and performing a frame by frame classification of the MHIs, we are able to detect starting motions in image sequences. The detection is performed using a wide angle stereo camera system at an urban intersection. We compare our algorithm to an existing method to detect movement transitions of pedestrians that uses MHIs in combination with a Histograms of Oriented Gradients (HOG) like descriptor and a Support Vector Machine (SVM), which we adapted to cyclists. To train and evaluate the methods a dataset containing MHIs of 394 cyclist starting motions was created. The results show that both methods can be used to detect starting motions of cyclists. Using the SVM approach, we were able to safely detect starting motions 0.506 s on average after the bicycle starts moving with an F1-score of 97.7%. The ResNet approach achieved an F1-score of 100% at an average detection time of 0.144 s. The ResNet approach outperformed the SVM approach in both robustness against false positive detections and detection time.",
      "published_utc": "2018-03-06T15:12:27Z",
      "updated_utc": "2018-03-06T15:12:27Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/1803.02242v1",
      "abs_url": "http://arxiv.org/abs/1803.02242v1"
    },
    "2511.09735v1": {
      "arxiv_id": "2511.09735v1",
      "title": "Social LSTM with Dynamic Occupancy Modeling for Realistic Pedestrian Trajectory Prediction",
      "authors": [
        "Ahmed Alia",
        "Mohcine Chraibi",
        "Armin Seyfried"
      ],
      "summary": "In dynamic and crowded environments, realistic pedestrian trajectory prediction remains a challenging task due to the complex nature of human motion and the mutual influences among individuals. Deep learning models have recently achieved promising results by implicitly learning such patterns from 2D trajectory data. However, most approaches treat pedestrians as point entities, ignoring the physical space that each person occupies. To address these limitations, this paper proposes a novel deep learning model that enhances the Social LSTM with a new Dynamic Occupied Space loss function. This loss function guides Social LSTM in learning to avoid realistic collisions without increasing displacement error across different crowd densities, ranging from low to high, in both homogeneous and heterogeneous density settings. Such a function achieves this by combining the average displacement error with a new collision penalty that is sensitive to scene density and individual spatial occupancy. For efficient training and evaluation, five datasets were generated from real pedestrian trajectories recorded during the Festival of Lights in Lyon 2022. Four datasets represent homogeneous crowd conditions -- low, medium, high, and very high density -- while the fifth corresponds to a heterogeneous density distribution. The experimental findings indicate that the proposed model not only lowers collision rates but also enhances displacement prediction accuracy in each dataset. Specifically, the model achieves up to a 31% reduction in the collision rate and reduces the average displacement error and the final displacement error by 5% and 6%, respectively, on average across all datasets compared to the baseline. Moreover, the proposed model consistently outperforms several state-of-the-art deep learning models across most test sets.",
      "published_utc": "2025-11-12T20:49:58Z",
      "updated_utc": "2025-11-12T20:49:58Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2511.09735v1",
      "abs_url": "http://arxiv.org/abs/2511.09735v1"
    },
    "2510.04365v1": {
      "arxiv_id": "2510.04365v1",
      "title": "Diffusion^2: Dual Diffusion Model with Uncertainty-Aware Adaptive Noise for Momentary Trajectory Prediction",
      "authors": [
        "Yuhao Luo",
        "Yuang Zhang",
        "Kehua Chen",
        "Xinyu Zheng",
        "Shucheng Zhang",
        "Sikai Chen",
        "Yinhai Wang"
      ],
      "summary": "Accurate pedestrian trajectory prediction is crucial for ensuring safety and efficiency in autonomous driving and human-robot interaction scenarios. Earlier studies primarily utilized sufficient observational data to predict future trajectories. However, in real-world scenarios, such as pedestrians suddenly emerging from blind spots, sufficient observational data is often unavailable (i.e. momentary trajectory), making accurate prediction challenging and increasing the risk of traffic accidents. Therefore, advancing research on pedestrian trajectory prediction under extreme scenarios is critical for enhancing traffic safety. In this work, we propose a novel framework termed Diffusion^2, tailored for momentary trajectory prediction. Diffusion^2 consists of two sequentially connected diffusion models: one for backward prediction, which generates unobserved historical trajectories, and the other for forward prediction, which forecasts future trajectories. Given that the generated unobserved historical trajectories may introduce additional noise, we propose a dual-head parameterization mechanism to estimate their aleatoric uncertainty and design a temporally adaptive noise module that dynamically modulates the noise scale in the forward diffusion process. Empirically, Diffusion^2 sets a new state-of-the-art in momentary trajectory prediction on ETH/UCY and Stanford Drone datasets.",
      "published_utc": "2025-10-05T21:19:33Z",
      "updated_utc": "2025-10-05T21:19:33Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2510.04365v1",
      "abs_url": "http://arxiv.org/abs/2510.04365v1"
    },
    "2510.03314v1": {
      "arxiv_id": "2510.03314v1",
      "title": "A Comprehensive Review on Artificial Intelligence Empowered Solutions for Enhancing Pedestrian and Cyclist Safety",
      "authors": [
        "Shucheng Zhang",
        "Yan Shi",
        "Bingzhang Wang",
        "Yuang Zhang",
        "Muhammad Monjurul Karim",
        "Kehua Chen",
        "Chenxi Liu",
        "Mehrdad Nasri",
        "Yinhai Wang"
      ],
      "summary": "Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and cyclists, remains a critical global challenge, as conventional infrastructure-based measures often prove inadequate in dynamic urban environments. Recent advances in artificial intelligence (AI), particularly in visual perception and reasoning, open new opportunities for proactive and context-aware VRU protection. However, existing surveys on AI applications for VRUs predominantly focus on detection, offering limited coverage of other vision-based tasks that are essential for comprehensive VRU understanding and protection. This paper presents a state-of-the-art review of recent progress in camera-based AI sensing systems for VRU safety, with an emphasis on developments from the past five years and emerging research trends. We systematically examine four core tasks, namely detection and classification, tracking and reidentification, trajectory prediction, and intent recognition and prediction, which together form the backbone of AI-empowered proactive solutions for VRU protection in intelligent transportation systems. To guide future research, we highlight four major open challenges from the perspectives of data, model, and deployment. By linking advances in visual AI with practical considerations for real-world implementation, this survey aims to provide a foundational reference for the development of next-generation sensing systems to enhance VRU safety.",
      "published_utc": "2025-09-30T23:50:55Z",
      "updated_utc": "2025-09-30T23:50:55Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2510.03314v1",
      "abs_url": "http://arxiv.org/abs/2510.03314v1"
    },
    "2509.15219v1": {
      "arxiv_id": "2509.15219v1",
      "title": "Out-of-Sight Trajectories: Tracking, Fusion, and Prediction",
      "authors": [
        "Haichao Zhang",
        "Yi Xu",
        "Yun Fu"
      ],
      "summary": "Trajectory prediction is a critical task in computer vision and autonomous systems, playing a key role in autonomous driving, robotics, surveillance, and virtual reality. Existing methods often rely on complete and noise-free observational data, overlooking the challenges associated with out-of-sight objects and the inherent noise in sensor data caused by limited camera coverage, obstructions, and the absence of ground truth for denoised trajectories. These limitations pose safety risks and hinder reliable prediction in real-world scenarios. In this extended work, we present advancements in Out-of-Sight Trajectory (OST), a novel task that predicts the noise-free visual trajectories of out-of-sight objects using noisy sensor data. Building on our previous research, we broaden the scope of Out-of-Sight Trajectory Prediction (OOSTraj) to include pedestrians and vehicles, extending its applicability to autonomous driving, robotics, surveillance, and virtual reality. Our enhanced Vision-Positioning Denoising Module leverages camera calibration to establish a vision-positioning mapping, addressing the lack of visual references, while effectively denoising noisy sensor data in an unsupervised manner. Through extensive evaluations on the Vi-Fi and JRDB datasets, our approach achieves state-of-the-art performance in both trajectory denoising and prediction, significantly surpassing previous baselines. Additionally, we introduce comparisons with traditional denoising methods, such as Kalman filtering, and adapt recent trajectory prediction models to our task, providing a comprehensive benchmark. This work represents the first initiative to integrate vision-positioning projection for denoising noisy sensor trajectories of out-of-sight agents, paving the way for future advances. The code and preprocessed datasets are available at github.com/Hai-chao-Zhang/OST",
      "published_utc": "2025-09-18T17:59:16Z",
      "updated_utc": "2025-09-18T17:59:16Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG",
        "cs.MA",
        "cs.MM",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2509.15219v1",
      "abs_url": "http://arxiv.org/abs/2509.15219v1"
    },
    "2509.10570v1": {
      "arxiv_id": "2509.10570v1",
      "title": "Large Foundation Models for Trajectory Prediction in Autonomous Driving: A Comprehensive Survey",
      "authors": [
        "Wei Dai",
        "Shengen Wu",
        "Wei Wu",
        "Zhenhao Wang",
        "Sisuo Lyu",
        "Haicheng Liao",
        "Limin Yu",
        "Weiping Ding",
        "Runwei Guan",
        "Yutao Yue"
      ],
      "summary": "Trajectory prediction serves as a critical functionality in autonomous driving, enabling the anticipation of future motion paths for traffic participants such as vehicles and pedestrians, which is essential for driving safety. Although conventional deep learning methods have improved accuracy, they remain hindered by inherent limitations, including lack of interpretability, heavy reliance on large-scale annotated data, and weak generalization in long-tail scenarios. The rise of Large Foundation Models (LFMs) is transforming the research paradigm of trajectory prediction. This survey offers a systematic review of recent advances in LFMs, particularly Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) for trajectory prediction. By integrating linguistic and scene semantics, LFMs facilitate interpretable contextual reasoning, significantly enhancing prediction safety and generalization in complex environments. The article highlights three core methodologies: trajectory-language mapping, multimodal fusion, and constraint-based reasoning. It covers prediction tasks for both vehicles and pedestrians, evaluation metrics, and dataset analyses. Key challenges such as computational latency, data scarcity, and real-world robustness are discussed, along with future research directions including low-latency inference, causality-aware modeling, and motion foundation models.",
      "published_utc": "2025-09-11T10:30:06Z",
      "updated_utc": "2025-09-11T10:30:06Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2509.10570v1",
      "abs_url": "http://arxiv.org/abs/2509.10570v1"
    },
    "2509.00624v1": {
      "arxiv_id": "2509.00624v1",
      "title": "Vehicle-in-Virtual-Environment (VVE) Method for Developing and Evaluating VRU Safety of Connected and Autonomous Driving with Focus on Bicyclist Safety",
      "authors": [
        "Haochong Chen",
        "Xincheng Cao",
        "Bilin Aksun-Guvenc",
        "Levent Guvenc"
      ],
      "summary": "Extensive research has already been conducted in the autonomous driving field to help vehicles navigate safely and efficiently. At the same time, plenty of current research on vulnerable road user (VRU) safety is performed which largely concentrates on perception, localization, or trajectory prediction of VRUs. However, existing research still exhibits several gaps, including the lack of a unified planning and collision avoidance system for autonomous vehicles, limited investigation into delay tolerant control strategies, and the absence of an efficient and standardized testing methodology. Ensuring VRU safety remains one of the most pressing challenges in autonomous driving, particularly in dynamic and unpredictable environments. In this two year project, we focused on applying the Vehicle in Virtual Environment (VVE) method to develop, evaluate, and demonstrate safety functions for Vulnerable Road Users (VRUs) using automated steering and braking of ADS. In this current second year project report, our primary focus was on enhancing the previous year results while also considering bicyclist safety.",
      "published_utc": "2025-08-30T22:43:14Z",
      "updated_utc": "2025-08-30T22:43:14Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2509.00624v1",
      "abs_url": "http://arxiv.org/abs/2509.00624v1"
    },
    "2508.14523v1": {
      "arxiv_id": "2508.14523v1",
      "title": "Great GATsBi: Hybrid, Multimodal, Trajectory Forecasting for Bicycles using Anticipation Mechanism",
      "authors": [
        "Kevin Riehl",
        "Shaimaa K. El-Baklish",
        "Anastasios Kouvelas",
        "Michail A. Makridis"
      ],
      "summary": "Accurate prediction of road user movement is increasingly required by many applications ranging from advanced driver assistance systems to autonomous driving, and especially crucial for road safety. Even though most traffic accident fatalities account to bicycles, they have received little attention, as previous work focused mainly on pedestrians and motorized vehicles. In this work, we present the Great GATsBi, a domain-knowledge-based, hybrid, multimodal trajectory prediction framework for bicycles. The model incorporates both physics-based modeling (inspired by motorized vehicles) and social-based modeling (inspired by pedestrian movements) to explicitly account for the dual nature of bicycle movement. The social interactions are modeled with a graph attention network, and include decayed historical, but also anticipated, future trajectory data of a bicycles neighborhood, following recent insights from psychological and social studies. The results indicate that the proposed ensemble of physics models -- performing well in the short-term predictions -- and social models -- performing well in the long-term predictions -- exceeds state-of-the-art performance. We also conducted a controlled mass-cycling experiment to demonstrate the framework's performance when forecasting bicycle trajectories and modeling social interactions with road users.",
      "published_utc": "2025-08-20T08:31:35Z",
      "updated_utc": "2025-08-20T08:31:35Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2508.14523v1",
      "abs_url": "http://arxiv.org/abs/2508.14523v1"
    },
    "2508.07079v1": {
      "arxiv_id": "2508.07079v1",
      "title": "Model Predictive Control for Crowd Navigation via Learning-Based Trajectory Prediction",
      "authors": [
        "Mohamed Parvez Aslam",
        "Bojan Derajic",
        "Mohamed-Khalil Bouzidi",
        "Sebastian Bernhard",
        "Jan Oliver Ringert"
      ],
      "summary": "Safe navigation in pedestrian-rich environments remains a key challenge for autonomous robots. This work evaluates the integration of a deep learning-based Social-Implicit (SI) pedestrian trajectory predictor within a Model Predictive Control (MPC) framework on the physical Continental Corriere robot. Tested across varied pedestrian densities, the SI-MPC system is compared to a traditional Constant Velocity (CV) model in both open-loop prediction and closed-loop navigation. Results show that SI improves trajectory prediction - reducing errors by up to 76% in low-density settings - and enhances safety and motion smoothness in crowded scenes. Moreover, real-world deployment reveals discrepancies between open-loop metrics and closed-loop performance, as the SI model yields broader, more cautious predictions. These findings emphasize the importance of system-level evaluation and highlight the SI-MPC framework's promise for safer, more adaptive navigation in dynamic, human-populated environments.",
      "published_utc": "2025-08-09T19:11:28Z",
      "updated_utc": "2025-08-09T19:11:28Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2508.07079v1",
      "abs_url": "http://arxiv.org/abs/2508.07079v1"
    },
    "2507.22742v1": {
      "arxiv_id": "2507.22742v1",
      "title": "Social-Pose: Enhancing Trajectory Prediction with Human Body Pose",
      "authors": [
        "Yang Gao",
        "Saeed Saadatnejad",
        "Alexandre Alahi"
      ],
      "summary": "Accurate human trajectory prediction is one of the most crucial tasks for autonomous driving, ensuring its safety. Yet, existing models often fail to fully leverage the visual cues that humans subconsciously communicate when navigating the space. In this work, we study the benefits of predicting human trajectories using human body poses instead of solely their Cartesian space locations in time. We propose `Social-pose', an attention-based pose encoder that effectively captures the poses of all humans in a scene and their social relations. Our method can be integrated into various trajectory prediction architectures. We have conducted extensive experiments on state-of-the-art models (based on LSTM, GAN, MLP, and Transformer), and showed improvements over all of them on synthetic (Joint Track Auto) and real (Human3.6M, Pedestrians and Cyclists in Road Traffic, and JRDB) datasets. We also explored the advantages of using 2D versus 3D poses, as well as the effect of noisy poses and the application of our pose-based predictor in robot navigation scenarios.",
      "published_utc": "2025-07-30T14:58:48Z",
      "updated_utc": "2025-07-30T14:58:48Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2507.22742v1",
      "abs_url": "http://arxiv.org/abs/2507.22742v1"
    },
    "2506.22111v1": {
      "arxiv_id": "2506.22111v1",
      "title": "Pedestrian Intention and Trajectory Prediction in Unstructured Traffic Using IDD-PeD",
      "authors": [
        "Ruthvik Bokkasam",
        "Shankar Gangisetty",
        "A. H. Abdul Hafez",
        "C. V. Jawahar"
      ],
      "summary": "With the rapid advancements in autonomous driving, accurately predicting pedestrian behavior has become essential for ensuring safety in complex and unpredictable traffic conditions. The growing interest in this challenge highlights the need for comprehensive datasets that capture unstructured environments, enabling the development of more robust prediction models to enhance pedestrian safety and vehicle navigation. In this paper, we introduce an Indian driving pedestrian dataset designed to address the complexities of modeling pedestrian behavior in unstructured environments, such as illumination changes, occlusion of pedestrians, unsignalized scene types and vehicle-pedestrian interactions. The dataset provides high-level and detailed low-level comprehensive annotations focused on pedestrians requiring the ego-vehicle's attention. Evaluation of the state-of-the-art intention prediction methods on our dataset shows a significant performance drop of up to $\\mathbf{15\\%}$, while trajectory prediction methods underperform with an increase of up to $\\mathbf{1208}$ MSE, defeating standard pedestrian datasets. Additionally, we present exhaustive quantitative and qualitative analysis of intention and trajectory baselines. We believe that our dataset will open new challenges for the pedestrian behavior research community to build robust models. Project Page: https://cvit.iiit.ac.in/research/projects/cvit-projects/iddped",
      "published_utc": "2025-06-27T10:41:18Z",
      "updated_utc": "2025-06-27T10:41:18Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.22111v1",
      "abs_url": "http://arxiv.org/abs/2506.22111v1"
    },
    "2506.14144v1": {
      "arxiv_id": "2506.14144v1",
      "title": "SceneAware: Scene-Constrained Pedestrian Trajectory Prediction with LLM-Guided Walkability",
      "authors": [
        "Juho Bai",
        "Inwook Shim"
      ],
      "summary": "Accurate prediction of pedestrian trajectories is essential for applications in robotics and surveillance systems. While existing approaches primarily focus on social interactions between pedestrians, they often overlook the rich environmental context that significantly shapes human movement patterns. In this paper, we propose SceneAware, a novel framework that explicitly incorporates scene understanding to enhance trajectory prediction accuracy. Our method leverages a Vision Transformer~(ViT) scene encoder to process environmental context from static scene images, while Multi-modal Large Language Models~(MLLMs) generate binary walkability masks that distinguish between accessible and restricted areas during training. We combine a Transformer-based trajectory encoder with the ViT-based scene encoder, capturing both temporal dynamics and spatial constraints. The framework integrates collision penalty mechanisms that discourage predicted trajectories from violating physical boundaries, ensuring physically plausible predictions. SceneAware is implemented in both deterministic and stochastic variants. Comprehensive experiments on the ETH/UCY benchmark datasets show that our approach outperforms state-of-the-art methods, with more than 50\\% improvement over previous models. Our analysis based on different trajectory categories shows that the model performs consistently well across various types of pedestrian movement. This highlights the importance of using explicit scene information and shows that our scene-aware approach is both effective and reliable in generating accurate and physically plausible predictions. Code is available at: https://github.com/juho127/SceneAware.",
      "published_utc": "2025-06-17T03:11:31Z",
      "updated_utc": "2025-06-17T03:11:31Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.14144v1",
      "abs_url": "http://arxiv.org/abs/2506.14144v1"
    },
    "2503.08016v1": {
      "arxiv_id": "2503.08016v1",
      "title": "SGNetPose+: Stepwise Goal-Driven Networks with Pose Information for Trajectory Prediction in Autonomous Driving",
      "authors": [
        "Akshat Ghiya",
        "Ali K. AlShami",
        "Jugal Kalita"
      ],
      "summary": "Predicting pedestrian trajectories is essential for autonomous driving systems, as it significantly enhances safety and supports informed decision-making. Accurate predictions enable the prevention of collisions, anticipation of crossing intent, and improved overall system efficiency. In this study, we present SGNetPose+, an enhancement of the SGNet architecture designed to integrate skeleton information or body segment angles with bounding boxes to predict pedestrian trajectories from video data to avoid hazards in autonomous driving. Skeleton information was extracted using a pose estimation model, and joint angles were computed based on the extracted joint data. We also apply temporal data augmentation by horizontally flipping video frames to increase the dataset size and improve performance. Our approach achieves state-of-the-art results on the JAAD and PIE datasets using pose data with the bounding boxes, outperforming the SGNet model. Code is available on Github: SGNetPose+.",
      "published_utc": "2025-03-11T03:45:51Z",
      "updated_utc": "2025-03-11T03:45:51Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.08016v1",
      "abs_url": "http://arxiv.org/abs/2503.08016v1"
    },
    "2501.02530v1": {
      "arxiv_id": "2501.02530v1",
      "title": "UDMC: Unified Decision-Making and Control Framework for Urban Autonomous Driving with Motion Prediction of Traffic Participants",
      "authors": [
        "Haichao Liu",
        "Kai Chen",
        "Yulin Li",
        "Zhenmin Huang",
        "Ming Liu",
        "Jun Ma"
      ],
      "summary": "Current autonomous driving systems often struggle to balance decision-making and motion control while ensuring safety and traffic rule compliance, especially in complex urban environments. Existing methods may fall short due to separate handling of these functionalities, leading to inefficiencies and safety compromises. To address these challenges, we introduce UDMC, an interpretable and unified Level 4 autonomous driving framework. UDMC integrates decision-making and motion control into a single optimal control problem (OCP), considering the dynamic interactions with surrounding vehicles, pedestrians, road lanes, and traffic signals. By employing innovative potential functions to model traffic participants and regulations, and incorporating a specialized motion prediction module, our framework enhances on-road safety and rule adherence. The integrated design allows for real-time execution of flexible maneuvers suited to diverse driving scenarios. High-fidelity simulations conducted in CARLA exemplify the framework's computational efficiency, robustness, and safety, resulting in superior driving performance when compared against various baseline models. Our open-source project is available at https://github.com/henryhcliu/udmc_carla.git.",
      "published_utc": "2025-01-05T13:16:05Z",
      "updated_utc": "2025-01-05T13:16:05Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.DC",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.02530v1",
      "abs_url": "http://arxiv.org/abs/2501.02530v1"
    },
    "2412.03689v1": {
      "arxiv_id": "2412.03689v1",
      "title": "Predicting Pedestrian Crossing Behavior in Germany and Japan: Insights into Model Transferability",
      "authors": [
        "Chi Zhang",
        "Janis Sprenger",
        "Zhongjun Ni",
        "Christian Berger"
      ],
      "summary": "Predicting pedestrian crossing behavior is important for intelligent traffic systems to avoid pedestrian-vehicle collisions. Most existing pedestrian crossing behavior models are trained and evaluated on datasets collected from a single country, overlooking differences between countries. To address this gap, we compared pedestrian road-crossing behavior at unsignalized crossings in Germany and Japan. We presented four types of machine learning models to predict gap selection behavior, zebra crossing usage, and their trajectories using simulator data collected from both countries. When comparing the differences between countries, pedestrians from the study conducted in Japan are more cautious, selecting larger gaps compared to those in Germany. We evaluate and analyze model transferability. Our results show that neural networks outperform other machine learning models in predicting gap selection and zebra crossing usage, while random forest models perform best on trajectory prediction tasks, demonstrating strong performance and transferability. We develop a transferable model using an unsupervised clustering method, which improves prediction accuracy for gap selection and trajectory prediction. These findings provide a deeper understanding of pedestrian crossing behaviors in different countries and offer valuable insights into model transferability.",
      "published_utc": "2024-12-04T19:55:40Z",
      "updated_utc": "2024-12-04T19:55:40Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.03689v1",
      "abs_url": "http://arxiv.org/abs/2412.03689v1"
    },
    "2409.20324v1": {
      "arxiv_id": "2409.20324v1",
      "title": "HEADS-UP: Head-Mounted Egocentric Dataset for Trajectory Prediction in Blind Assistance Systems",
      "authors": [
        "Yasaman Haghighi",
        "Celine Demonsant",
        "Panagiotis Chalimourdas",
        "Maryam Tavasoli Naeini",
        "Jhon Kevin Munoz",
        "Bladimir Bacca",
        "Silvan Suter",
        "Matthieu Gani",
        "Alexandre Alahi"
      ],
      "summary": "In this paper, we introduce HEADS-UP, the first egocentric dataset collected from head-mounted cameras, designed specifically for trajectory prediction in blind assistance systems. With the growing population of blind and visually impaired individuals, the need for intelligent assistive tools that provide real-time warnings about potential collisions with dynamic obstacles is becoming critical. These systems rely on algorithms capable of predicting the trajectories of moving objects, such as pedestrians, to issue timely hazard alerts. However, existing datasets fail to capture the necessary information from the perspective of a blind individual. To address this gap, HEADS-UP offers a novel dataset focused on trajectory prediction in this context. Leveraging this dataset, we propose a semi-local trajectory prediction approach to assess collision risks between blind individuals and pedestrians in dynamic environments. Unlike conventional methods that separately predict the trajectories of both the blind individual (ego agent) and pedestrians, our approach operates within a semi-local coordinate system, a rotated version of the camera's coordinate system, facilitating the prediction process. We validate our method on the HEADS-UP dataset and implement the proposed solution in ROS, performing real-time tests on an NVIDIA Jetson GPU through a user study. Results from both dataset evaluations and live tests demonstrate the robustness and efficiency of our approach.",
      "published_utc": "2024-09-30T14:26:09Z",
      "updated_utc": "2024-09-30T14:26:09Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.20324v1",
      "abs_url": "http://arxiv.org/abs/2409.20324v1"
    },
    "2409.15224v1": {
      "arxiv_id": "2409.15224v1",
      "title": "Enhancing Pedestrian Trajectory Prediction with Crowd Trip Information",
      "authors": [
        "Rei Tamaru",
        "Pei Li",
        "Bin Ran"
      ],
      "summary": "Pedestrian trajectory prediction is essential for various applications in active traffic management, urban planning, traffic control, crowd management, and autonomous driving, aiming to enhance traffic safety and efficiency. Accurately predicting pedestrian trajectories requires a deep understanding of individual behaviors, social interactions, and road environments. Existing studies have developed various models to capture the influence of social interactions and road conditions on pedestrian trajectories. However, these approaches are limited by the lack of a comprehensive view of social interactions and road environments. To address these limitations and enhance the accuracy of pedestrian trajectory prediction, we propose a novel approach incorporating trip information as a new modality into pedestrian trajectory models. We propose RNTransformer, a generic model that utilizes crowd trip information to capture global information on social interactions. We incorporated RNTransformer with various socially aware local pedestrian trajectory prediction models to demonstrate its performance. Specifically, by leveraging a pre-trained RNTransformer when training different pedestrian trajectory prediction models, we observed improvements in performance metrics: a 1.3/2.2% enhancement in ADE/FDE on Social-LSTM, a 6.5/28.4% improvement on Social-STGCNN, and an 8.6/4.3% improvement on S-Implicit. Evaluation results demonstrate that RNTransformer significantly enhances the accuracy of various pedestrian trajectory prediction models across multiple datasets. Further investigation reveals that the RNTransformer effectively guides local models to more accurate directions due to the consideration of global information. By exploring crowd behavior within the road network, our approach shows great promise in improving pedestrian safety through accurate trajectory predictions.",
      "published_utc": "2024-09-23T17:11:31Z",
      "updated_utc": "2024-09-23T17:11:31Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.15224v1",
      "abs_url": "http://arxiv.org/abs/2409.15224v1"
    },
    "2406.18050v1": {
      "arxiv_id": "2406.18050v1",
      "title": "A Multi-Stage Goal-Driven Network for Pedestrian Trajectory Prediction",
      "authors": [
        "Xiuen Wu",
        "Tao Wang",
        "Yuanzheng Cai",
        "Lingyu Liang",
        "George Papageorgiou"
      ],
      "summary": "Pedestrian trajectory prediction plays a pivotal role in ensuring the safety and efficiency of various applications, including autonomous vehicles and traffic management systems. This paper proposes a novel method for pedestrian trajectory prediction, called multi-stage goal-driven network (MGNet). Diverging from prior approaches relying on stepwise recursive prediction and the singular forecasting of a long-term goal, MGNet directs trajectory generation by forecasting intermediate stage goals, thereby reducing prediction errors. The network comprises three main components: a conditional variational autoencoder (CVAE), an attention module, and a multi-stage goal evaluator. Trajectories are encoded using conditional variational autoencoders to acquire knowledge about the approximate distribution of pedestrians' future trajectories, and combined with an attention mechanism to capture the temporal dependency between trajectory sequences. The pivotal module is the multi-stage goal evaluator, which utilizes the encoded feature vectors to predict intermediate goals, effectively minimizing cumulative errors in the recursive inference process. The effectiveness of MGNet is demonstrated through comprehensive experiments on the JAAD and PIE datasets. Comparative evaluations against state-of-the-art algorithms reveal significant performance improvements achieved by our proposed method.",
      "published_utc": "2024-06-26T03:59:21Z",
      "updated_utc": "2024-06-26T03:59:21Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.18050v1",
      "abs_url": "http://arxiv.org/abs/2406.18050v1"
    },
    "2406.02436v3": {
      "arxiv_id": "2406.02436v3",
      "title": "Safe, Out-of-Distribution-Adaptive MPC with Conformalized Neural Network Ensembles",
      "authors": [
        "Jose Leopoldo Contreras",
        "Ola Shorinwa",
        "Mac Schwager"
      ],
      "summary": "We present SODA-MPC, a Safe, Out-of-Distribution-Adaptive Model Predictive Control algorithm, which uses an ensemble of learned models for prediction, with a runtime monitor to flag unreliable out-of-distribution (OOD) predictions. When an OOD situation is detected, SODA-MPC triggers a safe fallback control strategy based on reachability, yielding a control framework that achieves the high performance of learning-based models while preserving the safety of reachability-based control. We demonstrate the method in the context of an autonomous vehicle, driving among dynamic pedestrians, where SODA-MPC uses a neural network ensemble for pedestrian prediction. We calibrate the OOD signal using conformal prediction to derive an OOD detector with probabilistic guarantees on the false-positive rate, given a user-specified confidence level. During in-distribution operation, the MPC controller avoids collisions with a pedestrian based on the trajectory predicted by the mean of the ensemble. When OOD conditions are detected, the MPC switches to a reachability-based controller to avoid collisions with the reachable set of the pedestrian assuming a maximum pedestrian speed, to guarantee safety under the worst-case actions of the pedestrian. We verify SODA-MPC in extensive autonomous driving simulations in a pedestrian-crossing scenario. Our model ensemble is trained and calibrated with real pedestrian data, showing that our OOD detector obtains the desired accuracy rate within a theoretically-predicted range. We empirically show improved safety and improved task completion compared with two state-of-the-art MPC methods that also use conformal prediction, but without OOD adaptation. Further, we demonstrate the effectiveness of our method with the large-scale multi-agent predictor Trajectron++, using large-scale traffic data from the nuScenes dataset for training and calibration.",
      "published_utc": "2024-06-04T15:58:14Z",
      "updated_utc": "2025-06-04T14:49:13Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.02436v3",
      "abs_url": "http://arxiv.org/abs/2406.02436v3"
    },
    "2404.15557v2": {
      "arxiv_id": "2404.15557v2",
      "title": "Safe POMDP Online Planning among Dynamic Agents via Adaptive Conformal Prediction",
      "authors": [
        "Shili Sheng",
        "Pian Yu",
        "David Parker",
        "Marta Kwiatkowska",
        "Lu Feng"
      ],
      "summary": "Online planning for partially observable Markov decision processes (POMDPs) provides efficient techniques for robot decision-making under uncertainty. However, existing methods fall short of preventing safety violations in dynamic environments. This work presents a novel safe POMDP online planning approach that maximizes expected returns while providing probabilistic safety guarantees amidst environments populated by multiple dynamic agents. Our approach utilizes data-driven trajectory prediction models of dynamic agents and applies Adaptive Conformal Prediction (ACP) to quantify the uncertainties in these predictions. Leveraging the obtained ACP-based trajectory predictions, our approach constructs safety shields on-the-fly to prevent unsafe actions within POMDP online planning. Through experimental evaluation in various dynamic environments using real-world pedestrian trajectory data, the proposed approach has been shown to effectively maintain probabilistic safety guarantees while accommodating up to hundreds of dynamic agents.",
      "published_utc": "2024-04-23T23:11:42Z",
      "updated_utc": "2024-09-06T20:21:15Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.15557v2",
      "abs_url": "http://arxiv.org/abs/2404.15557v2"
    },
    "2404.02227v1": {
      "arxiv_id": "2404.02227v1",
      "title": "OOSTraj: Out-of-Sight Trajectory Prediction With Vision-Positioning Denoising",
      "authors": [
        "Haichao Zhang",
        "Yi Xu",
        "Hongsheng Lu",
        "Takayuki Shimizu",
        "Yun Fu"
      ],
      "summary": "Trajectory prediction is fundamental in computer vision and autonomous driving, particularly for understanding pedestrian behavior and enabling proactive decision-making. Existing approaches in this field often assume precise and complete observational data, neglecting the challenges associated with out-of-view objects and the noise inherent in sensor data due to limited camera range, physical obstructions, and the absence of ground truth for denoised sensor data. Such oversights are critical safety concerns, as they can result in missing essential, non-visible objects. To bridge this gap, we present a novel method for out-of-sight trajectory prediction that leverages a vision-positioning technique. Our approach denoises noisy sensor observations in an unsupervised manner and precisely maps sensor-based trajectories of out-of-sight objects into visual trajectories. This method has demonstrated state-of-the-art performance in out-of-sight noisy sensor trajectory denoising and prediction on the Vi-Fi and JRDB datasets. By enhancing trajectory prediction accuracy and addressing the challenges of out-of-sight objects, our work significantly contributes to improving the safety and reliability of autonomous driving in complex environments. Our work represents the first initiative towards Out-Of-Sight Trajectory prediction (OOSTraj), setting a new benchmark for future research. The code is available at \\url{https://github.com/Hai-chao-Zhang/OOSTraj}.",
      "published_utc": "2024-04-02T18:30:29Z",
      "updated_utc": "2024-04-02T18:30:29Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.02227v1",
      "abs_url": "http://arxiv.org/abs/2404.02227v1"
    },
    "2403.16485v1": {
      "arxiv_id": "2403.16485v1",
      "title": "Real-time Model Predictive Control with Zonotope-Based Neural Networks for Bipedal Social Navigation",
      "authors": [
        "Abdulaziz Shamsah",
        "Krishanu Agarwal",
        "Shreyas Kousik",
        "Ye Zhao"
      ],
      "summary": "This study addresses the challenge of bipedal navigation in a dynamic human-crowded environment, a research area that remains largely underexplored in the field of legged navigation. We propose two cascaded zonotope-based neural networks: a Pedestrian Prediction Network (PPN) for pedestrians' future trajectory prediction and an Ego-agent Social Network (ESN) for ego-agent social path planning. Representing future paths as zonotopes allows for efficient reachability-based planning and collision checking. The ESN is then integrated with a Model Predictive Controller (ESN-MPC) for footstep planning for our bipedal robot Digit designed by Agility Robotics. ESN-MPC solves for a collision-free optimal trajectory by optimizing through the gradients of ESN. ESN-MPC optimal trajectory is sent to the low-level controller for full-order simulation of Digit. The overall proposed framework is validated with extensive simulations on randomly generated initial settings with varying human crowd densities.",
      "published_utc": "2024-03-25T07:12:51Z",
      "updated_utc": "2024-03-25T07:12:51Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.16485v1",
      "abs_url": "http://arxiv.org/abs/2403.16485v1"
    },
    "2402.08698v2": {
      "arxiv_id": "2402.08698v2",
      "title": "AMEND: A Mixture of Experts Framework for Long-tailed Trajectory Prediction",
      "authors": [
        "Ray Coden Mercurius",
        "Ehsan Ahmadi",
        "Soheil Mohamad Alizadeh Shabestary",
        "Amir Rasouli"
      ],
      "summary": "Accurate prediction of pedestrians' future motions is critical for intelligent driving systems. Developing models for this task requires rich datasets containing diverse sets of samples. However, the existing naturalistic trajectory prediction datasets are generally imbalanced in favor of simpler samples and lack challenging scenarios. Such a long-tail effect causes prediction models to underperform on the tail portion of the data distribution containing safety-critical scenarios. Previous methods tackle the long-tail problem using methods such as contrastive learning and class-conditioned hypernetworks. These approaches, however, are not modular and cannot be applied to many machine learning architectures. In this work, we propose a modular model-agnostic framework for trajectory prediction that leverages a specialized mixture of experts. In our approach, each expert is trained with a specialized skill with respect to a particular part of the data. To produce predictions, we utilise a router network that selects the best expert by generating relative confidence scores. We conduct experimentation on common pedestrian trajectory prediction datasets and show that our method improves performance on long-tail scenarios. We further conduct ablation studies to highlight the contribution of different proposed components.",
      "published_utc": "2024-02-13T02:43:41Z",
      "updated_utc": "2024-04-26T18:02:38Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.08698v2",
      "abs_url": "http://arxiv.org/abs/2402.08698v2"
    },
    "2402.03893v2": {
      "arxiv_id": "2402.03893v2",
      "title": "Prediction Horizon Requirements for Automated Driving: Optimizing Safety, Comfort, and Efficiency",
      "authors": [
        "Manuel Muñoz Sánchez",
        "Chris van der Ploeg",
        "Robin Smit",
        "Jos Elfring",
        "Emilia Silvas",
        "René van de Molengraft"
      ],
      "summary": "Predicting the movement of other road users is beneficial for improving automated vehicle (AV) performance. However, the relationship between the time horizon associated with these predictions and AV performance remains unclear. Despite the existence of numerous trajectory prediction algorithms, no studies have been conducted on how varying prediction lengths affect AV safety and other vehicle performance metrics, resulting in undefined horizon requirements for prediction methods. Our study addresses this gap by examining the effects of different prediction horizons on AV performance, focusing on safety, comfort, and efficiency. Through multiple experiments using a state-of-the-art, risk-based predictive trajectory planner, we simulated predictions with horizons up to 20 seconds. Based on our simulations, we propose a framework for specifying the minimum required and optimal prediction horizons based on specific AV performance criteria and application needs. Our results indicate that a horizon of 1.6 seconds is required to prevent collisions with crossing pedestrians, horizons of 7-8 seconds yield the best efficiency, and horizons up to 15 seconds improve passenger comfort. We conclude that prediction horizon requirements are application-dependent, and recommend aiming for a prediction horizon of 11.8 seconds as a general guideline for applications involving crossing pedestrians.",
      "published_utc": "2024-02-06T10:58:13Z",
      "updated_utc": "2024-04-10T13:34:24Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.03893v2",
      "abs_url": "http://arxiv.org/abs/2402.03893v2"
    },
    "2312.15881v2": {
      "arxiv_id": "2312.15881v2",
      "title": "Attention-aware Social Graph Transformer Networks for Stochastic Trajectory Prediction",
      "authors": [
        "Yao Liu",
        "Binghao Li",
        "Xianzhi Wang",
        "Claude Sammut",
        "Lina Yao"
      ],
      "summary": "Trajectory prediction is fundamental to various intelligent technologies, such as autonomous driving and robotics. The motion prediction of pedestrians and vehicles helps emergency braking, reduces collisions, and improves traffic safety. Current trajectory prediction research faces problems of complex social interactions, high dynamics and multi-modality. Especially, it still has limitations in long-time prediction. We propose Attention-aware Social Graph Transformer Networks for multi-modal trajectory prediction. We combine Graph Convolutional Networks and Transformer Networks by generating stable resolution pseudo-images from Spatio-temporal graphs through a designed stacking and interception method. Furthermore, we design the attention-aware module to handle social interaction information in scenarios involving mixed pedestrian-vehicle traffic. Thus, we maintain the advantages of the Graph and Transformer, i.e., the ability to aggregate information over an arbitrary number of neighbors and the ability to perform complex time-dependent data processing. We conduct experiments on datasets involving pedestrian, vehicle, and mixed trajectories, respectively. Our results demonstrate that our model minimizes displacement errors across various metrics and significantly reduces the likelihood of collisions. It is worth noting that our model effectively reduces the final displacement error, illustrating the ability of our model to predict for a long time.",
      "published_utc": "2023-12-26T04:24:01Z",
      "updated_utc": "2024-05-11T14:38:52Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.15881v2",
      "abs_url": "http://arxiv.org/abs/2312.15881v2"
    },
    "2312.03296v1": {
      "arxiv_id": "2312.03296v1",
      "title": "Cooperative Probabilistic Trajectory Forecasting under Occlusion",
      "authors": [
        "Anshul Nayak",
        "Azim Eskandarian"
      ],
      "summary": "Perception and planning under occlusion is essential for safety-critical tasks. Occlusion-aware planning often requires communicating the information of the occluded object to the ego agent for safe navigation. However, communicating rich sensor information under adverse conditions during communication loss and limited bandwidth may not be always feasible. Further, in GPS denied environments and indoor navigation, localizing and sharing of occluded objects can be challenging. To overcome this, relative pose estimation between connected agents sharing a common field of view can be a computationally effective way of communicating information about surrounding objects. In this paper, we design an end-to-end network that cooperatively estimates the current states of occluded pedestrian in the reference frame of ego agent and then predicts the trajectory with safety guarantees. Experimentally, we show that the uncertainty-aware trajectory prediction of occluded pedestrian by the ego agent is almost similar to the ground truth trajectory assuming no occlusion. The current research holds promise for uncertainty-aware navigation among multiple connected agents under occlusion.",
      "published_utc": "2023-12-06T05:36:52Z",
      "updated_utc": "2023-12-06T05:36:52Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.03296v1",
      "abs_url": "http://arxiv.org/abs/2312.03296v1"
    },
    "2311.15193v2": {
      "arxiv_id": "2311.15193v2",
      "title": "IA-LSTM: Interaction-Aware LSTM for Pedestrian Trajectory Prediction",
      "authors": [
        "Yuehai Chen"
      ],
      "summary": "Predicting the trajectory of pedestrians in crowd scenarios is indispensable in self-driving or autonomous mobile robot field because estimating the future locations of pedestrians around is beneficial for policy decision to avoid collision. It is a challenging issue because humans have different walking motions, and the interactions between humans and objects in the current environment, especially between humans themselves, are complex. Previous researchers focused on how to model human-human interactions but neglected the relative importance of interactions. To address this issue, a novel mechanism based on correntropy is introduced. The proposed mechanism not only can measure the relative importance of human-human interactions but also can build personal space for each pedestrian. An interaction module including this data-driven mechanism is further proposed. In the proposed module, the data-driven mechanism can effectively extract the feature representations of dynamic human-human interactions in the scene and calculate the corresponding weights to represent the importance of different interactions. To share such social messages among pedestrians, an interaction-aware architecture based on long short-term memory network for trajectory prediction is designed. Experiments are conducted on two public datasets. Experimental results demonstrate that our model can achieve better performance than several latest methods with good performance.",
      "published_utc": "2023-11-26T05:17:11Z",
      "updated_utc": "2024-01-25T06:32:22Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.15193v2",
      "abs_url": "http://arxiv.org/abs/2311.15193v2"
    },
    "2311.14922v3": {
      "arxiv_id": "2311.14922v3",
      "title": "GDTS: Goal-Guided Diffusion Model with Tree Sampling for Multi-Modal Pedestrian Trajectory Prediction",
      "authors": [
        "Ge Sun",
        "Sheng Wang",
        "Lei Zhu",
        "Ming Liu",
        "Jun Ma"
      ],
      "summary": "Accurate prediction of pedestrian trajectories is crucial for improving the safety of autonomous driving. However, this task is generally nontrivial due to the inherent stochasticity of human motion, which naturally requires the predictor to generate multi-modal prediction. Previous works leverage various generative methods, such as GAN and VAE, for pedestrian trajectory prediction. Nevertheless, these methods may suffer from mode collapse and relatively low-quality results. The denoising diffusion probabilistic model (DDPM) has recently been applied to trajectory prediction due to its simple training process and powerful reconstruction ability. However, current diffusion-based methods do not fully utilize input information and usually require many denoising iterations that lead to a long inference time or an additional network for initialization. To address these challenges and facilitate the use of diffusion models in multi-modal trajectory prediction, we propose GDTS, a novel Goal-Guided Diffusion Model with Tree Sampling for multi-modal trajectory prediction. Considering the \"goal-driven\" characteristics of human motion, GDTS leverages goal estimation to guide the generation of the diffusion network. A two-stage tree sampling algorithm is presented, which leverages common features to reduce the inference time and improve accuracy for multi-modal prediction. Experimental results demonstrate that our proposed framework achieves comparable state-of-the-art performance with real-time inference speed in public datasets.",
      "published_utc": "2023-11-25T03:55:06Z",
      "updated_utc": "2025-03-03T07:41:00Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.14922v3",
      "abs_url": "http://arxiv.org/abs/2311.14922v3"
    },
    "2312.10041v1": {
      "arxiv_id": "2312.10041v1",
      "title": "Digital Twin Technology Enabled Proactive Safety Application for Vulnerable Road Users: A Real-World Case Study",
      "authors": [
        "Erik Rua",
        "Kazi Hasan Shakib",
        "Sagar Dasgupta",
        "Mizanur Rahman",
        "Steven Jones"
      ],
      "summary": "While measures, such as traffic calming and advance driver assistance systems, can improve safety for Vulnerable Road Users (VRUs), their effectiveness ultimately relies on the responsible behavior of drivers and pedestrians who must adhere to traffic rules or take appropriate actions. However, these measures offer no solution in scenarios where a collision becomes imminent, leaving no time for warning or corrective actions. Recently, connected vehicle technology has introduced warning services that can alert drivers and VRUs about potential collisions. Nevertheless, there is still a significant gap in the system's ability to predict collisions in advance. The objective of this study is to utilize Digital Twin (DT) technology to enable a proactive safety alert system for VRUs. A pedestrian-vehicle trajectory prediction model has been developed using the Encoder-Decoder Long Short-Term Memory (LSTM) architecture to predict future trajectories of pedestrians and vehicles. Subsequently, parallel evaluation of all potential future safety-critical scenarios is carried out. Three Encoder-Decoder LSTM models, namely pedestrian-LSTM, vehicle-through-LSTM, and vehicle-left-turn-LSTM, are trained and validated using field-collected data, achieving corresponding root mean square errors (RMSE) of 0.049, 1.175, and 0.355 meters, respectively. A real-world case study has been conducted where a pedestrian crosses a road, and vehicles have the option to proceed through or left-turn, to evaluate the efficacy of DT-enabled proactive safety alert systems. Experimental results confirm that DT-enabled safety alert systems were succesfully able to detect potential crashes and proactively generate safety alerts to reduce potential crash risk.",
      "published_utc": "2023-11-24T15:27:57Z",
      "updated_utc": "2023-11-24T15:27:57Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.10041v1",
      "abs_url": "http://arxiv.org/abs/2312.10041v1"
    },
    "2311.04383v1": {
      "arxiv_id": "2311.04383v1",
      "title": "Active Collision Avoidance System for E-Scooters in Pedestrian Environment",
      "authors": [
        "Xuke Yan",
        "Dan Shen"
      ],
      "summary": "In the dense fabric of urban areas, electric scooters have rapidly become a preferred mode of transportation. As they cater to modern mobility demands, they present significant safety challenges, especially when interacting with pedestrians. In general, e-scooters are suggested to be ridden in bike lanes/sidewalks or share the road with cars at the maximum speed of about 15-20 mph, which is more flexible and much faster than pedestrians and bicyclists. Accurate prediction of pedestrian movement, coupled with assistant motion control of scooters, is essential in minimizing collision risks and seamlessly integrating scooters in areas dense with pedestrians. Addressing these safety concerns, our research introduces a novel e-Scooter collision avoidance system (eCAS) with a method for predicting pedestrian trajectories, employing an advanced LSTM network integrated with a state refinement module. This proactive model is designed to ensure unobstructed movement in areas with substantial pedestrian traffic without collisions. Results are validated on two public datasets, ETH and UCY, providing encouraging outcomes. Our model demonstrated proficiency in anticipating pedestrian paths and augmented scooter path planning, allowing for heightened adaptability in densely populated locales. This study shows the potential of melding pedestrian trajectory prediction with scooter motion planning. With the ubiquity of electric scooters in urban environments, such advancements have become crucial to safeguard all participants in urban transit.",
      "published_utc": "2023-11-07T23:07:22Z",
      "updated_utc": "2023-11-07T23:07:22Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.04383v1",
      "abs_url": "http://arxiv.org/abs/2311.04383v1"
    },
    "2310.14570v1": {
      "arxiv_id": "2310.14570v1",
      "title": "DICE: Diverse Diffusion Model with Scoring for Trajectory Prediction",
      "authors": [
        "Younwoo Choi",
        "Ray Coden Mercurius",
        "Soheil Mohamad Alizadeh Shabestary",
        "Amir Rasouli"
      ],
      "summary": "Road user trajectory prediction in dynamic environments is a challenging but crucial task for various applications, such as autonomous driving. One of the main challenges in this domain is the multimodal nature of future trajectories stemming from the unknown yet diverse intentions of the agents. Diffusion models have shown to be very effective in capturing such stochasticity in prediction tasks. However, these models involve many computationally expensive denoising steps and sampling operations that make them a less desirable option for real-time safety-critical applications. To this end, we present a novel framework that leverages diffusion models for predicting future trajectories in a computationally efficient manner. To minimize the computational bottlenecks in iterative sampling, we employ an efficient sampling mechanism that allows us to maximize the number of sampled trajectories for improved accuracy while maintaining inference time in real time. Moreover, we propose a scoring mechanism to select the most plausible trajectories by assigning relative ranks. We show the effectiveness of our approach by conducting empirical evaluations on common pedestrian (UCY/ETH) and autonomous driving (nuScenes) benchmark datasets on which our model achieves state-of-the-art performance on several subsets and metrics.",
      "published_utc": "2023-10-23T05:04:23Z",
      "updated_utc": "2023-10-23T05:04:23Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.14570v1",
      "abs_url": "http://arxiv.org/abs/2310.14570v1"
    },
    "2308.06654v1": {
      "arxiv_id": "2308.06654v1",
      "title": "Polar Collision Grids: Effective Interaction Modelling for Pedestrian Trajectory Prediction in Shared Space Using Collision Checks",
      "authors": [
        "Mahsa Golchoubian",
        "Moojan Ghafurian",
        "Kerstin Dautenhahn",
        "Nasser Lashgarian Azad"
      ],
      "summary": "Predicting pedestrians' trajectories is a crucial capability for autonomous vehicles' safe navigation, especially in spaces shared with pedestrians. Pedestrian motion in shared spaces is influenced by both the presence of vehicles and other pedestrians. Therefore, effectively modelling both pedestrian-pedestrian and pedestrian-vehicle interactions can increase the accuracy of the pedestrian trajectory prediction models. Despite the huge literature on ways to encode the effect of interacting agents on a pedestrian's predicted trajectory using deep-learning models, limited effort has been put into the effective selection of interacting agents. In the majority of cases, the interaction features used are mainly based on relative distances while paying less attention to the effect of the velocity and approaching direction in the interaction formulation. In this paper, we propose a heuristic-based process of selecting the interacting agents based on collision risk calculation. Focusing on interactions of potentially colliding agents with a target pedestrian, we propose the use of time-to-collision and the approach direction angle of two agents for encoding the interaction effect. This is done by introducing a novel polar collision grid map. Our results have shown predicted trajectories closer to the ground truth compared to existing methods (used as a baseline) on the HBS dataset.",
      "published_utc": "2023-08-13T00:20:22Z",
      "updated_utc": "2023-08-13T00:20:22Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.HC",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.06654v1",
      "abs_url": "http://arxiv.org/abs/2308.06654v1"
    },
    "2307.05288v2": {
      "arxiv_id": "2307.05288v2",
      "title": "Navigating Uncertainty: The Role of Short-Term Trajectory Prediction in Autonomous Vehicle Safety",
      "authors": [
        "Sushil Sharma",
        "Ganesh Sistu",
        "Lucie Yahiaoui",
        "Arindam Das",
        "Mark Halton",
        "Ciarán Eising"
      ],
      "summary": "Autonomous vehicles require accurate and reliable short-term trajectory predictions for safe and efficient driving. While most commercial automated vehicles currently use state machine-based algorithms for trajectory forecasting, recent efforts have focused on end-to-end data-driven systems. Often, the design of these models is limited by the availability of datasets, which are typically restricted to generic scenarios. To address this limitation, we have developed a synthetic dataset for short-term trajectory prediction tasks using the CARLA simulator. This dataset is extensive and incorporates what is considered complex scenarios - pedestrians crossing the road, vehicles overtaking - and comprises 6000 perspective view images with corresponding IMU and odometry information for each frame. Furthermore, an end-to-end short-term trajectory prediction model using convolutional neural networks (CNN) and long short-term memory (LSTM) networks has also been developed. This model can handle corner cases, such as slowing down near zebra crossings and stopping when pedestrians cross the road, without the need for explicit encoding of the surrounding environment. In an effort to accelerate this research and assist others, we are releasing our dataset and model to the research community. Our datasets are publicly available on https://github.com/sharmasushil/Navigating-Uncertainty-Trajectory-Prediction .",
      "published_utc": "2023-07-11T14:28:33Z",
      "updated_utc": "2023-07-12T09:25:03Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.05288v2",
      "abs_url": "http://arxiv.org/abs/2307.05288v2"
    },
    "2305.15942v1": {
      "arxiv_id": "2305.15942v1",
      "title": "Comparison of Pedestrian Prediction Models from Trajectory and Appearance Data for Autonomous Driving",
      "authors": [
        "Anthony Knittel",
        "Morris Antonello",
        "John Redford",
        "Subramanian Ramamoorthy"
      ],
      "summary": "The ability to anticipate pedestrian motion changes is a critical capability for autonomous vehicles. In urban environments, pedestrians may enter the road area and create a high risk for driving, and it is important to identify these cases. Typical predictors use the trajectory history to predict future motion, however in cases of motion initiation, motion in the trajectory may only be clearly visible after a delay, which can result in the pedestrian has entered the road area before an accurate prediction can be made. Appearance data includes useful information such as changes of gait, which are early indicators of motion changes, and can inform trajectory prediction. This work presents a comparative evaluation of trajectory-only and appearance-based methods for pedestrian prediction, and introduces a new dataset experiment for prediction using appearance. We create two trajectory and image datasets based on the combination of image and trajectory sequences from the popular NuScenes dataset, and examine prediction of trajectories using observed appearance to influence futures. This shows some advantages over trajectory prediction alone, although problems with the dataset prevent advantages of appearance-based models from being shown. We describe methods for improving the dataset and experiment to allow benefits of appearance-based models to be captured.",
      "published_utc": "2023-05-25T11:24:38Z",
      "updated_utc": "2023-05-25T11:24:38Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.15942v1",
      "abs_url": "http://arxiv.org/abs/2305.15942v1"
    },
    "2305.02859v2": {
      "arxiv_id": "2305.02859v2",
      "title": "Social Robot Navigation through Constrained Optimization: a Comparative Study of Uncertainty-based Objectives and Constraints",
      "authors": [
        "Timur Akhtyamov",
        "Aleksandr Kashirin",
        "Aleksey Postnikov",
        "Gonzalo Ferrer"
      ],
      "summary": "This work is dedicated to the study of how uncertainty estimation of the human motion prediction can be embedded into constrained optimization techniques, such as Model Predictive Control (MPC) for the social robot navigation. We propose several cost objectives and constraint functions obtained from the uncertainty of predicting pedestrian positions and related to the probability of the collision that can be applied to the MPC, and all the different variants are compared in challenging scenes with multiple agents. The main question this paper tries to answer is: what are the most important uncertainty-based criteria for social MPC? For that, we evaluate the proposed approaches with several social navigation metrics in an extensive set of scenarios of different complexity in reproducible synthetic environments. The main outcome of our study is a foundation for a practical guide on when and how to use uncertainty-aware approaches for social robot navigation in practice and what are the most effective criteria.",
      "published_utc": "2023-05-04T14:19:05Z",
      "updated_utc": "2023-07-17T18:35:46Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.02859v2",
      "abs_url": "http://arxiv.org/abs/2305.02859v2"
    },
    "2303.04320v2": {
      "arxiv_id": "2303.04320v2",
      "title": "SG-LSTM: Social Group LSTM for Robot Navigation Through Dense Crowds",
      "authors": [
        "Rashmi Bhaskara",
        "Maurice Chiu",
        "Aniket Bera"
      ],
      "summary": "With the increasing availability and affordability of personal robots, they will no longer be confined to large corporate warehouses or factories but will instead be expected to operate in less controlled environments alongside larger groups of people. In addition to ensuring safety and efficiency, it is crucial to minimize any negative psychological impact robots may have on humans and follow unwritten social norms in these situations. Our research aims to develop a model that can predict the movements of pedestrians and perceptually-social groups in crowded environments. We introduce a new Social Group Long Short-term Memory (SG-LSTM) model that models human groups and interactions in dense environments using a socially-aware LSTM to produce more accurate trajectory predictions. Our approach enables navigation algorithms to calculate collision-free paths faster and more accurately in crowded environments. Additionally, we also release a large video dataset with labeled pedestrian groups for the broader social navigation community. We show comparisons with different metrics on different datasets (ETH, Hotel, MOT15) and different prediction approaches (LIN, LSTM, O-LSTM, S-LSTM) as well as runtime performance.",
      "published_utc": "2023-03-08T01:38:20Z",
      "updated_utc": "2023-08-06T17:17:05Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.MA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.04320v2",
      "abs_url": "http://arxiv.org/abs/2303.04320v2"
    },
    "2303.01424v1": {
      "arxiv_id": "2303.01424v1",
      "title": "From Crowd Motion Prediction to Robot Navigation in Crowds",
      "authors": [
        "Sriyash Poddar",
        "Christoforos Mavrogiannis",
        "Siddhartha S. Srinivasa"
      ],
      "summary": "We focus on robot navigation in crowded environments. To navigate safely and efficiently within crowds, robots need models for crowd motion prediction. Building such models is hard due to the high dimensionality of multiagent domains and the challenge of collecting or simulating interaction-rich crowd-robot demonstrations. While there has been important progress on models for offline pedestrian motion forecasting, transferring their performance on real robots is nontrivial due to close interaction settings and novelty effects on users. In this paper, we investigate the utility of a recent state-of-the-art motion prediction model (S-GAN) for crowd navigation tasks. We incorporate this model into a model predictive controller (MPC) and deploy it on a self-balancing robot which we subject to a diverse range of crowd behaviors in the lab. We demonstrate that while S-GAN motion prediction accuracy transfers to the real world, its value is not reflected on navigation performance, measured with respect to safety and efficiency; in fact, the MPC performs indistinguishably even when using a simple constant-velocity prediction model, suggesting that substantial model improvements might be needed to yield significant gains for crowd navigation tasks. Footage from our experiments can be found at https://youtu.be/mzFiXg8KsZ0.",
      "published_utc": "2023-03-02T17:20:17Z",
      "updated_utc": "2023-03-02T17:20:17Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.01424v1",
      "abs_url": "http://arxiv.org/abs/2303.01424v1"
    },
    "2302.07583v1": {
      "arxiv_id": "2302.07583v1",
      "title": "ForceFormer: Exploring Social Force and Transformer for Pedestrian Trajectory Prediction",
      "authors": [
        "Weicheng Zhang",
        "Hao Cheng",
        "Fatema T. Johora",
        "Monika Sester"
      ],
      "summary": "Predicting trajectories of pedestrians based on goal information in highly interactive scenes is a crucial step toward Intelligent Transportation Systems and Autonomous Driving. The challenges of this task come from two key sources: (1) complex social interactions in high pedestrian density scenarios and (2) limited utilization of goal information to effectively associate with past motion information. To address these difficulties, we integrate social forces into a Transformer-based stochastic generative model backbone and propose a new goal-based trajectory predictor called ForceFormer. Differentiating from most prior works that simply use the destination position as an input feature, we leverage the driving force from the destination to efficiently simulate the guidance of a target on a pedestrian. Additionally, repulsive forces are used as another input feature to describe the avoidance action among neighboring pedestrians. Extensive experiments show that our proposed method achieves on-par performance measured by distance errors with the state-of-the-art models but evidently decreases collisions, especially in dense pedestrian scenarios on widely used pedestrian datasets.",
      "published_utc": "2023-02-15T10:54:14Z",
      "updated_utc": "2023-02-15T10:54:14Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.MA",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.07583v1",
      "abs_url": "http://arxiv.org/abs/2302.07583v1"
    },
    "2207.02281v1": {
      "arxiv_id": "2207.02281v1",
      "title": "BiPOCO: Bi-Directional Trajectory Prediction with Pose Constraints for Pedestrian Anomaly Detection",
      "authors": [
        "Asiegbu Miracle Kanu-Asiegbu",
        "Ram Vasudevan",
        "Xiaoxiao Du"
      ],
      "summary": "We present BiPOCO, a Bi-directional trajectory predictor with POse COnstraints, for detecting anomalous activities of pedestrians in videos. In contrast to prior work based on feature reconstruction, our work identifies pedestrian anomalous events by forecasting their future trajectories and comparing the predictions with their expectations. We introduce a set of novel compositional pose-based losses with our predictor and leverage prediction errors of each body joint for pedestrian anomaly detection. Experimental results show that our BiPOCO approach can detect pedestrian anomalous activities with a high detection rate (up to 87.0%) and incorporating pose constraints helps distinguish normal and anomalous poses in prediction. This work extends current literature of using prediction-based methods for anomaly detection and can benefit safety-critical applications such as autonomous driving and surveillance. Code is available at https://github.com/akanuasiegbu/BiPOCO.",
      "published_utc": "2022-07-05T19:45:49Z",
      "updated_utc": "2022-07-05T19:45:49Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.02281v1",
      "abs_url": "http://arxiv.org/abs/2207.02281v1"
    },
    "2207.02279v1": {
      "arxiv_id": "2207.02279v1",
      "title": "Leveraging Trajectory Prediction for Pedestrian Video Anomaly Detection",
      "authors": [
        "Asiegbu Miracle Kanu-Asiegbu",
        "Ram Vasudevan",
        "Xiaoxiao Du"
      ],
      "summary": "Video anomaly detection is a core problem in vision. Correctly detecting and identifying anomalous behaviors in pedestrians from video data will enable safety-critical applications such as surveillance, activity monitoring, and human-robot interaction. In this paper, we propose to leverage trajectory localization and prediction for unsupervised pedestrian anomaly event detection. Different than previous reconstruction-based approaches, our proposed framework rely on the prediction errors of normal and abnormal pedestrian trajectories to detect anomalies spatially and temporally. We present experimental results on real-world benchmark datasets on varying timescales and show that our proposed trajectory-predictor-based anomaly detection pipeline is effective and efficient at identifying anomalous activities of pedestrians in videos. Code will be made available at https://github.com/akanuasiegbu/Leveraging-Trajectory-Prediction-for-Pedestrian-Video-Anomaly-Detection.",
      "published_utc": "2022-07-05T19:44:34Z",
      "updated_utc": "2022-07-05T19:44:34Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.02279v1",
      "abs_url": "http://arxiv.org/abs/2207.02279v1"
    },
    "2206.13387v1": {
      "arxiv_id": "2206.13387v1",
      "title": "ScePT: Scene-consistent, Policy-based Trajectory Predictions for Planning",
      "authors": [
        "Yuxiao Chen",
        "Boris Ivanovic",
        "Marco Pavone"
      ],
      "summary": "Trajectory prediction is a critical functionality of autonomous systems that share environments with uncontrolled agents, one prominent example being self-driving vehicles. Currently, most prediction methods do not enforce scene consistency, i.e., there are a substantial amount of self-collisions between predicted trajectories of different agents in the scene. Moreover, many approaches generate individual trajectory predictions per agent instead of joint trajectory predictions of the whole scene, which makes downstream planning difficult. In this work, we present ScePT, a policy planning-based trajectory prediction model that generates accurate, scene-consistent trajectory predictions suitable for autonomous system motion planning. It explicitly enforces scene consistency and learns an agent interaction policy that can be used for conditional prediction. Experiments on multiple real-world pedestrians and autonomous vehicle datasets show that ScePT} matches current state-of-the-art prediction accuracy with significantly improved scene consistency. We also demonstrate ScePT's ability to work with a downstream contingency planner.",
      "published_utc": "2022-06-18T00:00:02Z",
      "updated_utc": "2022-06-18T00:00:02Z",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.13387v1",
      "abs_url": "http://arxiv.org/abs/2206.13387v1"
    },
    "2203.09293v1": {
      "arxiv_id": "2203.09293v1",
      "title": "PreTR: Spatio-Temporal Non-Autoregressive Trajectory Prediction Transformer",
      "authors": [
        "Lina Achaji",
        "Thierno Barry",
        "Thibault Fouqueray",
        "Julien Moreau",
        "Francois Aioun",
        "Francois Charpillet"
      ],
      "summary": "Nowadays, our mobility systems are evolving into the era of intelligent vehicles that aim to improve road safety. Due to their vulnerability, pedestrians are the users who will benefit the most from these developments. However, predicting their trajectory is one of the most challenging concerns. Indeed, accurate prediction requires a good understanding of multi-agent interactions that can be complex. Learning the underlying spatial and temporal patterns caused by these interactions is even more of a competitive and open problem that many researchers are tackling. In this paper, we introduce a model called PRediction Transformer (PReTR) that extracts features from the multi-agent scenes by employing a factorized spatio-temporal attention module. It shows less computational needs than previously studied models with empirically better results. Besides, previous works in motion prediction suffer from the exposure bias problem caused by generating future sequences conditioned on model prediction samples rather than ground-truth samples. In order to go beyond the proposed solutions, we leverage encoder-decoder Transformer networks for parallel decoding a set of learned object queries. This non-autoregressive solution avoids the need for iterative conditioning and arguably decreases training and testing computational time. We evaluate our model on the ETH/UCY datasets, a publicly available benchmark for pedestrian trajectory prediction. Finally, we justify our usage of the parallel decoding technique by showing that the trajectory prediction task can be better solved as a non-autoregressive task.",
      "published_utc": "2022-03-17T12:52:23Z",
      "updated_utc": "2022-03-17T12:52:23Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG",
        "cs.MA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2203.09293v1",
      "abs_url": "http://arxiv.org/abs/2203.09293v1"
    },
    "2202.03954v1": {
      "arxiv_id": "2202.03954v1",
      "title": "Social-DualCVAE: Multimodal Trajectory Forecasting Based on Social Interactions Pattern Aware and Dual Conditional Variational Auto-Encoder",
      "authors": [
        "Jiashi Gao",
        "Xinming Shi",
        "James J. Q. Yu"
      ],
      "summary": "Pedestrian trajectory forecasting is a fundamental task in multiple utility areas, such as self-driving, autonomous robots, and surveillance systems. The future trajectory forecasting is multi-modal, influenced by physical interaction with scene contexts and intricate social interactions among pedestrians. The mainly existing literature learns representations of social interactions by deep learning networks, while the explicit interaction patterns are not utilized. Different interaction patterns, such as following or collision avoiding, will generate different trends of next movement, thus, the awareness of social interaction patterns is important for trajectory forecasting. Moreover, the social interaction patterns are privacy concerned or lack of labels. To jointly address the above issues, we present a social-dual conditional variational auto-encoder (Social-DualCVAE) for multi-modal trajectory forecasting, which is based on a generative model conditioned not only on the past trajectories but also the unsupervised classification of interaction patterns. After generating the category distribution of the unlabeled social interaction patterns, DualCVAE, conditioned on the past trajectories and social interaction pattern, is proposed for multi-modal trajectory prediction by latent variables estimating. A variational bound is derived as the minimization objective during training. The proposed model is evaluated on widely used trajectory benchmarks and outperforms the prior state-of-the-art methods.",
      "published_utc": "2022-02-08T16:04:47Z",
      "updated_utc": "2022-02-08T16:04:47Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2202.03954v1",
      "abs_url": "http://arxiv.org/abs/2202.03954v1"
    },
    "2112.06624v1": {
      "arxiv_id": "2112.06624v1",
      "title": "Pedestrian Trajectory Prediction via Spatial Interaction Transformer Network",
      "authors": [
        "Tong Su",
        "Yu Meng",
        "Yan Xu"
      ],
      "summary": "As a core technology of the autonomous driving system, pedestrian trajectory prediction can significantly enhance the function of active vehicle safety and reduce road traffic injuries. In traffic scenes, when encountering with oncoming people, pedestrians may make sudden turns or stop immediately, which often leads to complicated trajectories. To predict such unpredictable trajectories, we can gain insights into the interaction between pedestrians. In this paper, we present a novel generative method named Spatial Interaction Transformer (SIT), which learns the spatio-temporal correlation of pedestrian trajectories through attention mechanisms. Furthermore, we introduce the conditional variational autoencoder (CVAE) framework to model the future latent motion states of pedestrians. In particular, the experiments based on large-scale trafc dataset nuScenes [2] show that SIT has an outstanding performance than state-of-the-art (SOTA) methods. Experimental evaluation on the challenging ETH and UCY datasets conrms the robustness of our proposed model",
      "published_utc": "2021-12-13T13:08:04Z",
      "updated_utc": "2021-12-13T13:08:04Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2112.06624v1",
      "abs_url": "http://arxiv.org/abs/2112.06624v1"
    },
    "2111.03822v1": {
      "arxiv_id": "2111.03822v1",
      "title": "Prediction of Pedestrian Spatiotemporal Risk Levels for Intelligent Vehicles: A Data-driven Approach",
      "authors": [
        "Zheyu Zhang",
        "Boyang Wang",
        "Chao Lu",
        "Jinghang Li",
        "Cheng Gong",
        "Jianwei Gong"
      ],
      "summary": "In recent years, road safety has attracted significant attention from researchers and practitioners in the intelligent transport systems domain. As one of the most common and vulnerable groups of road users, pedestrians cause great concerns due to their unpredictable behavior and movement, as subtle misunderstandings in vehicle-pedestrian interaction can easily lead to risky situations or collisions. Existing methods use either predefined collision-based models or human-labeling approaches to estimate the pedestrians' risks. These approaches are usually limited by their poor generalization ability and lack of consideration of interactions between the ego vehicle and a pedestrian. This work tackles the listed problems by proposing a Pedestrian Risk Level Prediction system. The system consists of three modules. Firstly, vehicle-perspective pedestrian data are collected. Since the data contains information regarding the movement of both the ego vehicle and pedestrian, it can simplify the prediction of spatiotemporal features in an interaction-aware fashion. Using the long short-term memory model, the pedestrian trajectory prediction module predicts their spatiotemporal features in the subsequent five frames. As the predicted trajectory follows certain interaction and risk patterns, a hybrid clustering and classification method is adopted to explore the risk patterns in the spatiotemporal features and train a risk level classifier using the learned patterns. Upon predicting the spatiotemporal features of pedestrians and identifying the corresponding risk level, the risk patterns between the ego vehicle and pedestrians are determined. Experimental results verified the capability of the PRLP system to predict the risk level of pedestrians, thus supporting the collision risk assessment of intelligent vehicles and providing safety warnings to both vehicles and pedestrians.",
      "published_utc": "2021-11-06T07:32:05Z",
      "updated_utc": "2021-11-06T07:32:05Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2111.03822v1",
      "abs_url": "http://arxiv.org/abs/2111.03822v1"
    },
    "2108.10879v2": {
      "arxiv_id": "2108.10879v2",
      "title": "Are socially-aware trajectory prediction models really socially-aware?",
      "authors": [
        "Saeed Saadatnejad",
        "Mohammadhossein Bahari",
        "Pedram Khorsandi",
        "Mohammad Saneian",
        "Seyed-Mohsen Moosavi-Dezfooli",
        "Alexandre Alahi"
      ],
      "summary": "Our field has recently witnessed an arms race of neural network-based trajectory predictors. While these predictors are at the core of many applications such as autonomous navigation or pedestrian flow simulations, their adversarial robustness has not been carefully studied. In this paper, we introduce a socially-attended attack to assess the social understanding of prediction models in terms of collision avoidance. An attack is a small yet carefully-crafted perturbations to fail predictors. Technically, we define collision as a failure mode of the output, and propose hard- and soft-attention mechanisms to guide our attack. Thanks to our attack, we shed light on the limitations of the current models in terms of their social understanding. We demonstrate the strengths of our method on the recent trajectory prediction models. Finally, we show that our attack can be employed to increase the social understanding of state-of-the-art models. The code is available online: https://s-attack.github.io/",
      "published_utc": "2021-08-24T17:59:09Z",
      "updated_utc": "2022-02-11T17:21:56Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2108.10879v2",
      "abs_url": "http://arxiv.org/abs/2108.10879v2"
    },
    "2108.08236v3": {
      "arxiv_id": "2108.08236v3",
      "title": "LOKI: Long Term and Key Intentions for Trajectory Prediction",
      "authors": [
        "Harshayu Girase",
        "Haiming Gang",
        "Srikanth Malla",
        "Jiachen Li",
        "Akira Kanehara",
        "Karttikeya Mangalam",
        "Chiho Choi"
      ],
      "summary": "Recent advances in trajectory prediction have shown that explicit reasoning about agents' intent is important to accurately forecast their motion. However, the current research activities are not directly applicable to intelligent and safety critical systems. This is mainly because very few public datasets are available, and they only consider pedestrian-specific intents for a short temporal horizon from a restricted egocentric view. To this end, we propose LOKI (LOng term and Key Intentions), a novel large-scale dataset that is designed to tackle joint trajectory and intention prediction for heterogeneous traffic agents (pedestrians and vehicles) in an autonomous driving setting. The LOKI dataset is created to discover several factors that may affect intention, including i) agent's own will, ii) social interactions, iii) environmental constraints, and iv) contextual information. We also propose a model that jointly performs trajectory and intention prediction, showing that recurrently reasoning about intention can assist with trajectory prediction. We show our method outperforms state-of-the-art trajectory prediction methods by upto $27\\%$ and also provide a baseline for frame-wise intention estimation.",
      "published_utc": "2021-08-18T16:57:03Z",
      "updated_utc": "2021-09-17T16:38:31Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MA",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2108.08236v3",
      "abs_url": "http://arxiv.org/abs/2108.08236v3"
    },
    "2107.11637v5": {
      "arxiv_id": "2107.11637v5",
      "title": "Group-based Motion Prediction for Navigation in Crowded Environments",
      "authors": [
        "Allan Wang",
        "Christoforos Mavrogiannis",
        "Aaron Steinfeld"
      ],
      "summary": "We focus on the problem of planning the motion of a robot in a dynamic multiagent environment such as a pedestrian scene. Enabling the robot to navigate safely and in a socially compliant fashion in such scenes requires a representation that accounts for the unfolding multiagent dynamics. Existing approaches to this problem tend to employ microscopic models of motion prediction that reason about the individual behavior of other agents. While such models may achieve high tracking accuracy in trajectory prediction benchmarks, they often lack an understanding of the group structures unfolding in crowded scenes. Inspired by the Gestalt theory from psychology, we build a Model Predictive Control framework (G-MPC) that leverages group-based prediction for robot motion planning. We conduct an extensive simulation study involving a series of challenging navigation tasks in scenes extracted from two real-world pedestrian datasets. We illustrate that G-MPC enables a robot to achieve statistically significantly higher safety and lower number of group intrusions than a series of baselines featuring individual pedestrian motion prediction models. Finally, we show that G-MPC can handle noisy lidar-scan estimates without significant performance losses.",
      "published_utc": "2021-07-24T15:51:43Z",
      "updated_utc": "2022-03-16T04:57:13Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2107.11637v5",
      "abs_url": "http://arxiv.org/abs/2107.11637v5"
    },
    "2106.12442v1": {
      "arxiv_id": "2106.12442v1",
      "title": "Euro-PVI: Pedestrian Vehicle Interactions in Dense Urban Centers",
      "authors": [
        "Apratim Bhattacharyya",
        "Daniel Olmeda Reino",
        "Mario Fritz",
        "Bernt Schiele"
      ],
      "summary": "Accurate prediction of pedestrian and bicyclist paths is integral to the development of reliable autonomous vehicles in dense urban environments. The interactions between vehicle and pedestrian or bicyclist have a significant impact on the trajectories of traffic participants e.g. stopping or turning to avoid collisions. Although recent datasets and trajectory prediction approaches have fostered the development of autonomous vehicles yet the amount of vehicle-pedestrian (bicyclist) interactions modeled are sparse. In this work, we propose Euro-PVI, a dataset of pedestrian and bicyclist trajectories. In particular, our dataset caters more diverse and complex interactions in dense urban scenarios compared to the existing datasets. To address the challenges in predicting future trajectories with dense interactions, we develop a joint inference model that learns an expressive multi-modal shared latent space across agents in the urban scene. This enables our Joint-$β$-cVAE approach to better model the distribution of future trajectories. We achieve state of the art results on the nuScenes and Euro-PVI datasets demonstrating the importance of capturing interactions between ego-vehicle and pedestrians (bicyclists) for accurate predictions.",
      "published_utc": "2021-06-22T15:40:21Z",
      "updated_utc": "2021-06-22T15:40:21Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2106.12442v1",
      "abs_url": "http://arxiv.org/abs/2106.12442v1"
    },
    "2011.10670v3": {
      "arxiv_id": "2011.10670v3",
      "title": "From Recognition to Prediction: Analysis of Human Action and Trajectory Prediction in Video",
      "authors": [
        "Junwei Liang"
      ],
      "summary": "With the advancement in computer vision deep learning, systems now are able to analyze an unprecedented amount of rich visual information from videos to enable applications such as autonomous driving, socially-aware robot assistant and public safety monitoring. Deciphering human behaviors to predict their future paths/trajectories and what they would do from videos is important in these applications. However, human trajectory prediction still remains a challenging task, as scene semantics and human intent are difficult to model. Many systems do not provide high-level semantic attributes to reason about pedestrian future. This design hinders prediction performance in video data from diverse domains and unseen scenarios. To enable optimal future human behavioral forecasting, it is crucial for the system to be able to detect and analyze human activities as well as scene semantics, passing informative features to the subsequent prediction module for context understanding.",
      "published_utc": "2020-11-20T22:23:34Z",
      "updated_utc": "2021-07-16T13:45:43Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2011.10670v3",
      "abs_url": "http://arxiv.org/abs/2011.10670v3"
    },
    "2010.12007v2": {
      "arxiv_id": "2010.12007v2",
      "title": "PRANK: motion Prediction based on RANKing",
      "authors": [
        "Yuriy Biktairov",
        "Maxim Stebelev",
        "Irina Rudenko",
        "Oleh Shliazhko",
        "Boris Yangel"
      ],
      "summary": "Predicting the motion of agents such as pedestrians or human-driven vehicles is one of the most critical problems in the autonomous driving domain. The overall safety of driving and the comfort of a passenger directly depend on its successful solution. The motion prediction problem also remains one of the most challenging problems in autonomous driving engineering, mainly due to high variance of the possible agent's future behavior given a situation. The two phenomena responsible for the said variance are the multimodality caused by the uncertainty of the agent's intent (e.g., turn right or move forward) and uncertainty in the realization of a given intent (e.g., which lane to turn into). To be useful within a real-time autonomous driving pipeline, a motion prediction system must provide efficient ways to describe and quantify this uncertainty, such as computing posterior modes and their probabilities or estimating density at the point corresponding to a given trajectory. It also should not put substantial density on physically impossible trajectories, as they can confuse the system processing the predictions. In this paper, we introduce the PRANK method, which satisfies these requirements. PRANK takes rasterized bird-eye images of agent's surroundings as an input and extracts features of the scene with a convolutional neural network. It then produces the conditional distribution of agent's trajectories plausible in the given scene. The key contribution of PRANK is a way to represent that distribution using nearest-neighbor methods in latent trajectory space, which allows for efficient inference in real time. We evaluate PRANK on the in-house and Argoverse datasets, where it shows competitive results.",
      "published_utc": "2020-10-22T19:58:02Z",
      "updated_utc": "2021-06-15T09:39:33Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2010.12007v2",
      "abs_url": "http://arxiv.org/abs/2010.12007v2"
    },
    "2009.10468v2": {
      "arxiv_id": "2009.10468v2",
      "title": "Spatial-Temporal Block and LSTM Network for Pedestrian Trajectories Prediction",
      "authors": [
        "Xiong Dan"
      ],
      "summary": "Pedestrian trajectory prediction is a critical to avoid autonomous driving collision. But this prediction is a challenging problem due to social forces and cluttered scenes. Such human-human and human-space interactions lead to many socially plausible trajectories. In this paper, we propose a novel LSTM-based algorithm. We tackle the problem by considering the static scene and pedestrian which combine the Graph Convolutional Networks and Temporal Convolutional Networks to extract features from pedestrians. Each pedestrian in the scene is regarded as a node, and we can obtain the relationship between each node and its neighborhoods by graph embedding. It is LSTM that encode the relationship so that our model predicts nodes trajectories in crowd scenarios simultaneously. To effectively predict multiple possible future trajectories, we further introduce Spatio-Temporal Convolutional Block to make the network flexible. Experimental results on two public datasets, i.e. ETH and UCY, demonstrate the effectiveness of our proposed ST-Block and we achieve state-of-the-art approaches in human trajectory prediction.",
      "published_utc": "2020-09-22T11:43:40Z",
      "updated_utc": "2020-09-23T07:51:39Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2009.10468v2",
      "abs_url": "http://arxiv.org/abs/2009.10468v2"
    },
    "2007.14558v2": {
      "arxiv_id": "2007.14558v2",
      "title": "BiTraP: Bi-directional Pedestrian Trajectory Prediction with Multi-modal Goal Estimation",
      "authors": [
        "Yu Yao",
        "Ella Atkins",
        "Matthew Johnson-Roberson",
        "Ram Vasudevan",
        "Xiaoxiao Du"
      ],
      "summary": "Pedestrian trajectory prediction is an essential task in robotic applications such as autonomous driving and robot navigation. State-of-the-art trajectory predictors use a conditional variational autoencoder (CVAE) with recurrent neural networks (RNNs) to encode observed trajectories and decode multi-modal future trajectories. This process can suffer from accumulated errors over long prediction horizons (>=2 seconds). This paper presents BiTraP, a goal-conditioned bi-directional multi-modal trajectory prediction method based on the CVAE. BiTraP estimates the goal (end-point) of trajectories and introduces a novel bi-directional decoder to improve longer-term trajectory prediction accuracy. Extensive experiments show that BiTraP generalizes to both first-person view (FPV) and bird's-eye view (BEV) scenarios and outperforms state-of-the-art results by ~10-50%. We also show that different choices of non-parametric versus parametric target models in the CVAE directly influence the predicted multi-modal trajectory distributions. These results provide guidance on trajectory predictor design for robotic applications such as collision avoidance and navigation systems.",
      "published_utc": "2020-07-29T02:40:17Z",
      "updated_utc": "2020-11-16T17:30:24Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2007.14558v2",
      "abs_url": "http://arxiv.org/abs/2007.14558v2"
    },
    "2003.06594v1": {
      "arxiv_id": "2003.06594v1",
      "title": "Collaborative Motion Prediction via Neural Motion Message Passing",
      "authors": [
        "Yue Hu",
        "Siheng Chen",
        "Ya Zhang",
        "Xiao Gu"
      ],
      "summary": "Motion prediction is essential and challenging for autonomous vehicles and social robots. One challenge of motion prediction is to model the interaction among traffic actors, which could cooperate with each other to avoid collisions or form groups. To address this challenge, we propose neural motion message passing (NMMP) to explicitly model the interaction and learn representations for directed interactions between actors. Based on the proposed NMMP, we design the motion prediction systems for two settings: the pedestrian setting and the joint pedestrian and vehicle setting. Both systems share a common pattern: we use an individual branch to model the behavior of a single actor and an interactive branch to model the interaction between actors, while with different wrappers to handle the varied input formats and characteristics. The experimental results show that both systems outperform the previous state-of-the-art methods on several existing benchmarks. Besides, we provide interpretability for interaction learning.",
      "published_utc": "2020-03-14T10:12:54Z",
      "updated_utc": "2020-03-14T10:12:54Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2003.06594v1",
      "abs_url": "http://arxiv.org/abs/2003.06594v1"
    },
    "2002.03038v1": {
      "arxiv_id": "2002.03038v1",
      "title": "DenseCAvoid: Real-time Navigation in Dense Crowds using Anticipatory Behaviors",
      "authors": [
        "Adarsh Jagan Sathyamoorthy",
        "Jing Liang",
        "Utsav Patel",
        "Tianrui Guan",
        "Rohan Chandra",
        "Dinesh Manocha"
      ],
      "summary": "We present DenseCAvoid, a novel navigation algorithm for navigating a robot through dense crowds and avoiding collisions by anticipating pedestrian behaviors. Our formulation uses visual sensors and a pedestrian trajectory prediction algorithm to track pedestrians in a set of input frames and provide bounding boxes that extrapolate the pedestrian positions in a future time. Our hybrid approach combines this trajectory prediction with a Deep Reinforcement Learning-based collision avoidance method to train a policy to generate smoother, safer, and more robust trajectories during run-time. We train our policy in realistic 3-D simulations of static and dynamic scenarios with multiple pedestrians. In practice, our hybrid approach generalizes well to unseen, real-world scenarios and can navigate a robot through dense crowds (~1-2 humans per square meter) in indoor scenarios, including narrow corridors and lobbies. As compared to cases where prediction was not used, we observe that our method reduces the occurrence of the robot freezing in a crowd by up to 48%, and performs comparably with respect to trajectory lengths and mean arrival times to goal.",
      "published_utc": "2020-02-07T22:46:21Z",
      "updated_utc": "2020-02-07T22:46:21Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2002.03038v1",
      "abs_url": "http://arxiv.org/abs/2002.03038v1"
    },
    "2001.11597v1": {
      "arxiv_id": "2001.11597v1",
      "title": "Path Planning in Dynamic Environments using Generative RNNs and Monte Carlo Tree Search",
      "authors": [
        "Stuart Eiffert",
        "He Kong",
        "Navid Pirmarzdashti",
        "Salah Sukkarieh"
      ],
      "summary": "State of the art methods for robotic path planning in dynamic environments, such as crowds or traffic, rely on hand crafted motion models for agents. These models often do not reflect interactions of agents in real world scenarios. To overcome this limitation, this paper proposes an integrated path planning framework using generative Recurrent Neural Networks within a Monte Carlo Tree Search (MCTS). This approach uses a learnt model of social response to predict crowd dynamics during planning across the action space. This extends our recent work using generative RNNs to learn the relationship between planned robotic actions and the likely response of a crowd. We show that the proposed framework can considerably improve motion prediction accuracy during interactions, allowing more effective path planning. The performance of our method is compared in simulation with existing methods for collision avoidance in a crowd of pedestrians, demonstrating the ability to control future states of nearby individuals. We also conduct preliminary real world tests to validate the effectiveness of our method.",
      "published_utc": "2020-01-30T22:46:37Z",
      "updated_utc": "2020-01-30T22:46:37Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2001.11597v1",
      "abs_url": "http://arxiv.org/abs/2001.11597v1"
    },
    "1910.06673v1": {
      "arxiv_id": "1910.06673v1",
      "title": "SafeCritic: Collision-Aware Trajectory Prediction",
      "authors": [
        "Tessa van der Heiden",
        "Naveen Shankar Nagaraja",
        "Christian Weiss",
        "Efstratios Gavves"
      ],
      "summary": "Navigating complex urban environments safely is a key to realize fully autonomous systems. Predicting future locations of vulnerable road users, such as pedestrians and cyclists, thus, has received a lot of attention in the recent years. While previous works have addressed modeling interactions with the static (obstacles) and dynamic (humans) environment agents, we address an important gap in trajectory prediction. We propose SafeCritic, a model that synergizes generative adversarial networks for generating multiple \"real\" trajectories with reinforcement learning to generate \"safe\" trajectories. The Discriminator evaluates the generated candidates on whether they are consistent with the observed inputs. The Critic network is environmentally aware to prune trajectories that are in collision or are in violation with the environment. The auto-encoding loss stabilizes training and prevents mode-collapse. We demonstrate results on two large scale data sets with a considerable improvement over state-of-the-art. We also show that the Critic is able to classify the safety of trajectories.",
      "published_utc": "2019-10-15T12:15:19Z",
      "updated_utc": "2019-10-15T12:15:19Z",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CV",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/1910.06673v1",
      "abs_url": "http://arxiv.org/abs/1910.06673v1"
    },
    "1907.01577v2": {
      "arxiv_id": "1907.01577v2",
      "title": "SVM Enhanced Frenet Frame Planner For Safe Navigation Amidst Moving Agents",
      "authors": [
        "Unni Krishnan R Nair",
        "Nivedita Rufus",
        "Vashist Madiraju",
        "K Madhava Krishna"
      ],
      "summary": "This paper proposes an SVM Enhanced Trajectory Planner for dynamic scenes, typically those encountered in on road settings. Frenet frame based trajectory generation is popular in the context of autonomous driving both in research and industry. We incorporate a safety based maximal margin criteria using a SVM layer that generates control points that are maximally separated from all dynamic obstacles in the scene. A kinematically consistent trajectory generator then computes a path through these waypoints. We showcase through simulations as well as real world experiments on a self driving car that the SVM enhanced planner provides for a larger offset with dynamic obstacles than the regular Frenet frame based trajectory generation. Thereby, the authors argue that such a formulation is inherently suited for navigation amongst pedestrians. We assume the availability of an intent or trajectory prediction module that predicts the future trajectories of all dynamic actors in the scene.",
      "published_utc": "2019-07-02T18:22:53Z",
      "updated_utc": "2020-09-11T06:42:00Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/1907.01577v2",
      "abs_url": "http://arxiv.org/abs/1907.01577v2"
    },
    "1906.08469v2": {
      "arxiv_id": "1906.08469v2",
      "title": "Predicting Motion of Vulnerable Road Users using High-Definition Maps and Efficient ConvNets",
      "authors": [
        "Fang-Chieh Chou",
        "Tsung-Han Lin",
        "Henggang Cui",
        "Vladan Radosavljevic",
        "Thi Nguyen",
        "Tzu-Kuo Huang",
        "Matthew Niedoba",
        "Jeff Schneider",
        "Nemanja Djuric"
      ],
      "summary": "Following detection and tracking of traffic actors, prediction of their future motion is the next critical component of a self-driving vehicle (SDV) technology, allowing the SDV to operate safely and efficiently in its environment. This is particularly important when it comes to vulnerable road users (VRUs), such as pedestrians and bicyclists. These actors need to be handled with special care due to an increased risk of injury, as well as the fact that their behavior is less predictable than that of motorized actors. To address this issue, in the current study we present a deep learning-based method for predicting VRU movement, where we rasterize high-definition maps and actor's surroundings into a bird's-eye view image used as an input to deep convolutional networks. In addition, we propose a fast architecture suitable for real-time inference, and perform an ablation study of various rasterization approaches to find the optimal choice for accurate prediction. The results strongly indicate benefits of using the proposed approach for motion prediction of VRUs, both in terms of accuracy and latency.",
      "published_utc": "2019-06-20T07:16:16Z",
      "updated_utc": "2020-06-11T06:54:12Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.LG",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/1906.08469v2",
      "abs_url": "http://arxiv.org/abs/1906.08469v2"
    },
    "1903.01860v1": {
      "arxiv_id": "1903.01860v1",
      "title": "Stochastic Sampling Simulation for Pedestrian Trajectory Prediction",
      "authors": [
        "Cyrus Anderson",
        "Xiaoxiao Du",
        "Ram Vasudevan",
        "Matthew Johnson-Roberson"
      ],
      "summary": "Urban environments pose a significant challenge for autonomous vehicles (AVs) as they must safely navigate while in close proximity to many pedestrians. It is crucial for the AV to correctly understand and predict the future trajectories of pedestrians to avoid collision and plan a safe path. Deep neural networks (DNNs) have shown promising results in accurately predicting pedestrian trajectories, relying on large amounts of annotated real-world data to learn pedestrian behavior. However, collecting and annotating these large real-world pedestrian datasets is costly in both time and labor. This paper describes a novel method using a stochastic sampling-based simulation to train DNNs for pedestrian trajectory prediction with social interaction. Our novel simulation method can generate vast amounts of automatically-annotated, realistic, and naturalistic synthetic pedestrian trajectories based on small amounts of real annotation. We then use such synthetic trajectories to train an off-the-shelf state-of-the-art deep learning approach Social GAN (Generative Adversarial Network) to perform pedestrian trajectory prediction. Our proposed architecture, trained only using synthetic trajectories, achieves better prediction results compared to those trained on human-annotated real-world data using the same network. Our work demonstrates the effectiveness and potential of using simulation as a substitution for human annotation efforts to train high-performing prediction algorithms such as the DNNs.",
      "published_utc": "2019-03-05T14:39:04Z",
      "updated_utc": "2019-03-05T14:39:04Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/1903.01860v1",
      "abs_url": "http://arxiv.org/abs/1903.01860v1"
    },
    "1902.05437v1": {
      "arxiv_id": "1902.05437v1",
      "title": "Situation-Aware Pedestrian Trajectory Prediction with Spatio-Temporal Attention Model",
      "authors": [
        "Sirin Haddad",
        "Meiqing Wu",
        "He Wei",
        "Siew Kei Lam"
      ],
      "summary": "Pedestrian trajectory prediction is essential for collision avoidance in autonomous driving and robot navigation. However, predicting a pedestrian's trajectory in crowded environments is non-trivial as it is influenced by other pedestrians' motion and static structures that are present in the scene. Such human-human and human-space interactions lead to non-linearities in the trajectories. In this paper, we present a new spatio-temporal graph based Long Short-Term Memory (LSTM) network for predicting pedestrian trajectory in crowded environments, which takes into account the interaction with static (physical objects) and dynamic (other pedestrians) elements in the scene. Our results are based on two widely-used datasets to demonstrate that the proposed method outperforms the state-of-the-art approaches in human trajectory prediction. In particular, our method leads to a reduction in Average Displacement Error (ADE) and Final Displacement Error (FDE) of up to 55% and 61% respectively over state-of-the-art approaches.",
      "published_utc": "2019-02-13T17:57:50Z",
      "updated_utc": "2019-02-13T17:57:50Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/1902.05437v1",
      "abs_url": "http://arxiv.org/abs/1902.05437v1"
    },
    "1706.05904v2": {
      "arxiv_id": "1706.05904v2",
      "title": "Pedestrian Prediction by Planning using Deep Neural Networks",
      "authors": [
        "Eike Rehder",
        "Florian Wirth",
        "Martin Lauer",
        "Christoph Stiller"
      ],
      "summary": "Accurate traffic participant prediction is the prerequisite for collision avoidance of autonomous vehicles. In this work, we predict pedestrians by emulating their own motion planning. From online observations, we infer a mixture density function for possible destinations. We use this result as the goal states of a planning stage that performs motion prediction based on common behavior patterns. The entire system is modeled as one monolithic neural network and trained via inverse reinforcement learning. Experimental validation on real world data shows the system's ability to predict both, destinations and trajectories accurately.",
      "published_utc": "2017-06-19T12:40:30Z",
      "updated_utc": "2017-06-20T07:25:49Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/1706.05904v2",
      "abs_url": "http://arxiv.org/abs/1706.05904v2"
    },
    "2601.10554v1": {
      "arxiv_id": "2601.10554v1",
      "title": "DeepUrban: Interaction-Aware Trajectory Prediction and Planning for Automated Driving by Aerial Imagery",
      "authors": [
        "Constantin Selzer",
        "Fabian B. Flohr"
      ],
      "summary": "The efficacy of autonomous driving systems hinges critically on robust prediction and planning capabilities. However, current benchmarks are impeded by a notable scarcity of scenarios featuring dense traffic, which is essential for understanding and modeling complex interactions among road users. To address this gap, we collaborated with our industrial partner, DeepScenario, to develop DeepUrban-a new drone dataset designed to enhance trajectory prediction and planning benchmarks focusing on dense urban settings. DeepUrban provides a rich collection of 3D traffic objects, extracted from high-resolution images captured over urban intersections at approximately 100 meters altitude. The dataset is further enriched with comprehensive map and scene information to support advanced modeling and simulation tasks. We evaluate state-of-the-art (SOTA) prediction and planning methods, and conducted experiments on generalization capabilities. Our findings demonstrate that adding DeepUrban to nuScenes can boost the accuracy of vehicle predictions and planning, achieving improvements up to 44.1 % / 44.3% on the ADE / FDE metrics. Website: https://iv.ee.hm.edu/deepurban",
      "published_utc": "2026-01-15T16:18:42Z",
      "updated_utc": "2026-01-15T16:18:42Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2601.10554v1",
      "abs_url": "http://arxiv.org/abs/2601.10554v1"
    },
    "2601.09856v1": {
      "arxiv_id": "2601.09856v1",
      "title": "How Human Motion Prediction Quality Shapes Social Robot Navigation Performance in Constrained Spaces",
      "authors": [
        "Andrew Stratton",
        "Phani Teja Singamaneni",
        "Pranav Goyal",
        "Rachid Alami",
        "Christoforos Mavrogiannis"
      ],
      "summary": "Motivated by the vision of integrating mobile robots closer to humans in warehouses, hospitals, manufacturing plants, and the home, we focus on robot navigation in dynamic and spatially constrained environments. Ensuring human safety, comfort, and efficiency in such settings requires that robots are endowed with a model of how humans move around them. Human motion prediction around robots is especially challenging due to the stochasticity of human behavior, differences in user preferences, and data scarcity. In this work, we perform a methodical investigation of the effects of human motion prediction quality on robot navigation performance, as well as human productivity and impressions. We design a scenario involving robot navigation among two human subjects in a constrained workspace and instantiate it in a user study ($N=80$) involving two different robot platforms, conducted across two sites from different world regions. Key findings include evidence that: 1) the widely adopted average displacement error is not a reliable predictor of robot navigation performance and human impressions; 2) the common assumption of human cooperation breaks down in constrained environments, with users often not reciprocating robot cooperation, and causing performance degradations; 3) more efficient robot navigation often comes at the expense of human efficiency and comfort.",
      "published_utc": "2026-01-14T20:34:34Z",
      "updated_utc": "2026-01-14T20:34:34Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2601.09856v1",
      "abs_url": "http://arxiv.org/abs/2601.09856v1"
    },
    "2601.09377v1": {
      "arxiv_id": "2601.09377v1",
      "title": "ReflexDiffusion: Reflection-Enhanced Trajectory Planning for High-lateral-acceleration Scenarios in Autonomous Driving",
      "authors": [
        "Xuemei Yao",
        "Xiao Yang",
        "Jianbin Sun",
        "Liuwei Xie",
        "Xuebin Shao",
        "Xiyu Fang",
        "Hang Su",
        "Kewei Yang"
      ],
      "summary": "Generating safe and reliable trajectories for autonomous vehicles in long-tail scenarios remains a significant challenge, particularly for high-lateral-acceleration maneuvers such as sharp turns, which represent critical safety situations. Existing trajectory planners exhibit systematic failures in these scenarios due to data imbalance. This results in insufficient modelling of vehicle dynamics, road geometry, and environmental constraints in high-risk situations, leading to suboptimal or unsafe trajectory prediction when vehicles operate near their physical limits. In this paper, we introduce ReflexDiffusion, a novel inference-stage framework that enhances diffusion-based trajectory planners through reflective adjustment. Our method introduces a gradient-based adjustment mechanism during the iterative denoising process: after each standard trajectory update, we compute the gradient between the conditional and unconditional noise predictions to explicitly amplify critical conditioning signals, including road curvature and lateral vehicle dynamics. This amplification enforces strict adherence to physical constraints, particularly improving stability during high-lateral-acceleration maneuvers where precise vehicle-road interaction is paramount. Evaluated on the nuPlan Test14-hard benchmark, ReflexDiffusion achieves a 14.1% improvement in driving score for high-lateral-acceleration scenarios over the state-of-the-art (SOTA) methods. This demonstrates that inference-time trajectory optimization can effectively compensate for training data sparsity by dynamically reinforcing safety-critical constraints near handling limits. The framework's architecture-agnostic design enables direct deployment to existing diffusion-based planners, offering a practical solution for improving autonomous vehicle safety in challenging driving conditions.",
      "published_utc": "2026-01-14T11:03:29Z",
      "updated_utc": "2026-01-14T11:03:29Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2601.09377v1",
      "abs_url": "http://arxiv.org/abs/2601.09377v1"
    },
    "2601.10521v1": {
      "arxiv_id": "2601.10521v1",
      "title": "BikeActions: An Open Platform and Benchmark for Cyclist-Centric VRU Action Recognition",
      "authors": [
        "Max A. Buettner",
        "Kanak Mazumder",
        "Luca Koecher",
        "Mario Finkbeiner",
        "Sebastian Niebler",
        "Fabian B. Flohr"
      ],
      "summary": "Anticipating the intentions of Vulnerable Road Users (VRUs) is a critical challenge for safe autonomous driving (AD) and mobile robotics. While current research predominantly focuses on pedestrian crossing behaviors from a vehicle's perspective, interactions within dense shared spaces remain underexplored. To bridge this gap, we introduce FUSE-Bike, the first fully open perception platform of its kind. Equipped with two LiDARs, a camera, and GNSS, it facilitates high-fidelity, close-range data capture directly from a cyclist's viewpoint. Leveraging this platform, we present BikeActions, a novel multi-modal dataset comprising 852 annotated samples across 5 distinct action classes, specifically tailored to improve VRU behavior modeling. We establish a rigorous benchmark by evaluating state-of-the-art graph convolution and transformer-based models on our publicly released data splits, establishing the first performance baselines for this challenging task. We release the full dataset together with data curation tools, the open hardware design, and the benchmark code to foster future research in VRU action understanding under https://iv.ee.hm.edu/bikeactions/.",
      "published_utc": "2026-01-15T15:47:46Z",
      "updated_utc": "2026-01-15T15:47:46Z",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2601.10521v1",
      "abs_url": "http://arxiv.org/abs/2601.10521v1"
    },
    "2601.10233v1": {
      "arxiv_id": "2601.10233v1",
      "title": "Proactive Local-Minima-Free Robot Navigation: Blending Motion Prediction with Safe Control",
      "authors": [
        "Yifan Xue",
        "Ze Zhang",
        "Knut Åkesson",
        "Nadia Figueroa"
      ],
      "summary": "This work addresses the challenge of safe and efficient mobile robot navigation in complex dynamic environments with concave moving obstacles. Reactive safe controllers like Control Barrier Functions (CBFs) design obstacle avoidance strategies based only on the current states of the obstacles, risking future collisions. To alleviate this problem, we use Gaussian processes to learn barrier functions online from multimodal motion predictions of obstacles generated by neural networks trained with energy-based learning. The learned barrier functions are then fed into quadratic programs using modulated CBFs (MCBFs), a local-minimum-free version of CBFs, to achieve safe and efficient navigation. The proposed framework makes two key contributions. First, it develops a prediction-to-barrier function online learning pipeline. Second, it introduces an autonomous parameter tuning algorithm that adapts MCBFs to deforming, prediction-based barrier functions. The framework is evaluated in both simulations and real-world experiments, consistently outperforming baselines and demonstrating superior safety and efficiency in crowded dynamic environments.",
      "published_utc": "2026-01-15T09:46:03Z",
      "updated_utc": "2026-01-15T09:46:03Z",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2601.10233v1",
      "abs_url": "http://arxiv.org/abs/2601.10233v1"
    }
  },
  "topics": {
    "pedestrian_trajectory_prediction": {
      "name": "Pedestrian Trajectory Prediction",
      "papers": [
        "2505.00586v2",
        "2504.17371v3",
        "2405.16439v3",
        "2403.00353v1",
        "2306.01075v1",
        "2210.10254v2",
        "2209.02297v1",
        "2106.00559v2",
        "2008.08294v2",
        "2003.09996v1",
        "1911.09476v1",
        "1910.03088v1",
        "1906.00486v4",
        "1806.09453v1",
        "1806.09444v1",
        "1804.00495v2",
        "1803.03577v1"
      ],
      "latest_update": "2026-01-16"
    },
    "cyclist_micromobility_prediction": {
      "name": "Cyclist & Micromobility Prediction",
      "papers": [
        "1803.03577v1"
      ],
      "latest_update": "2026-01-16"
    },
    "interaction_aware_models": {
      "name": "Interaction-aware & Social Models",
      "papers": [
        "2512.02777v1",
        "2506.12474v1",
        "2505.00586v2",
        "2504.17371v3",
        "2504.15541v1",
        "2504.13111v3",
        "2501.13461v2",
        "2405.16439v3",
        "2405.02145v1",
        "2404.11946v1",
        "2404.11181v2",
        "2403.00353v1",
        "2312.05144v1",
        "2309.13893v3",
        "2306.01075v1",
        "2211.10226v1",
        "2207.10398v1",
        "2202.05140v2",
        "2111.00788v3",
        "2106.00559v2",
        "2011.12406v2",
        "2010.16267v3",
        "2008.08294v2",
        "2005.08307v2",
        "2003.09996v1",
        "2002.01965v1",
        "1911.03801v1",
        "1910.03088v1",
        "1909.00792v1",
        "1906.00486v4",
        "1808.06887v5",
        "1807.09995v1",
        "1803.03577v1",
        "1705.02445v1",
        "2601.10554v1",
        "2601.09856v1",
        "2601.09377v1"
      ],
      "latest_update": "2026-01-16"
    },
    "intention_crossing_behavior": {
      "name": "Intention & Crossing Behavior",
      "papers": [
        "2505.09935v1",
        "2502.15824v1",
        "2405.16439v3",
        "2311.16091v1",
        "2201.04742v1",
        "2010.05115v1",
        "2008.08294v2",
        "2003.09998v1",
        "2003.09996v1",
        "1809.03705v3",
        "1806.09444v1",
        "1804.00495v2",
        "1803.03577v1",
        "1803.02242v1",
        "2601.10521v1"
      ],
      "latest_update": "2026-01-16"
    },
    "risk_safety_collision": {
      "name": "Risk-aware / Safety / Collision Prediction",
      "papers": [
        "2511.09735v1",
        "2510.04365v1",
        "2510.03314v1",
        "2509.15219v1",
        "2509.10570v1",
        "2509.00624v1",
        "2508.14523v1",
        "2508.07079v1",
        "2507.22742v1",
        "2506.22111v1",
        "2506.14144v1",
        "2504.17371v3",
        "2503.08016v1",
        "2501.02530v1",
        "2412.03689v1",
        "2409.20324v1",
        "2409.15224v1",
        "2406.18050v1",
        "2406.02436v3",
        "2404.15557v2",
        "2404.02227v1",
        "2403.16485v1",
        "2402.08698v2",
        "2402.03893v2",
        "2312.15881v2",
        "2312.03296v1",
        "2311.15193v2",
        "2311.14922v3",
        "2312.10041v1",
        "2311.04383v1",
        "2310.14570v1",
        "2308.06654v1",
        "2307.05288v2",
        "2305.15942v1",
        "2305.02859v2",
        "2303.04320v2",
        "2303.01424v1",
        "2302.07583v1",
        "2210.10254v2",
        "2209.02297v1",
        "2207.02281v1",
        "2207.02279v1",
        "2206.13387v1",
        "2203.09293v1",
        "2202.03954v1",
        "2112.06624v1",
        "2111.03822v1",
        "2108.10879v2",
        "2108.08236v3",
        "2107.11637v5",
        "2106.12442v1",
        "2011.10670v3",
        "2010.12007v2",
        "2009.10468v2",
        "2007.14558v2",
        "2003.06594v1",
        "2002.03038v1",
        "2001.11597v1",
        "1910.06673v1",
        "1910.03088v1",
        "1907.01577v2",
        "1906.08469v2",
        "1903.01860v1",
        "1902.05437v1",
        "1803.03577v1",
        "1706.05904v2",
        "2601.10233v1",
        "2601.09856v1",
        "2601.09377v1"
      ],
      "latest_update": "2026-01-16"
    }
  }
}